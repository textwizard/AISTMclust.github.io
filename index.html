<!DOCTYPE html><meta charset=UTF-8>
  <style>.node rect{cursor:pointer;fill:#fff;fill-opacity:.5;stroke:#3182bd;stroke-width:1.5px;}
  .node text{font:10px sans-serif;pointer-events:none;}
  path.link{fill:none;stroke:#9ecae1;stroke-width:1.5px;}
  .control.glyphicon{position:static;color:#4A4C4F;font-family:"Oxygen", sans-serif;cursor:pointer;}
  .scrollbox{height: 120px;border: 1px solid #e5e5e5;overflow: scroll;}
  .bar{fill: steelblue;}.axis{font: 10px sans-serif;}.axis path,.axis line{fill: none; stroke: #000;
  shape-rendering: crispEdges;}.x.axis path{display: none;}</style>
  <link href="https://fonts.googleapis.com/css?family=Oxygen" rel=stylesheet type="text/css">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css" rel=stylesheet>
  <title>STM Visualization</title><body>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.min.js"></script>
  <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script>var margin={top:30,right:20,bottom:30,left:20},width=window.innerWidth/1.5-margin.left-margin.right,height=window.innerHeight-margin.top-margin.bottom,barHeight=20,barWidth=width*.8;
  var i=0,duration=400,root;var tree=d3.layout.tree().nodeSize([0,20]);var diagonal=d3.svg.diagonal().projection(function(d){return[d.y,d.x];});
  var svg=d3.select("body").append("svg").attr("width",width+margin.left+margin.right).attr("height",height+margin.top+margin.bottom).append("g").attr("transform","translate("+margin.left+","+margin.top+")");
  root= {"children":[{"name":["artificial_intelligence, process, data, knowledge, develop, decision, technology"],"children":[{"name":["technology, data, artificial_intelligence, management, work, service, knowledge"],"children":[{"name":["social, organization, effect, adoption, influence, trust, relationship"],"children":[{"name":["trust, social, perception, perceive, influence, intention, effect"],"size":[1800],"topic_no":[1],"thought_1":["Trust plays an important role in sharing transactions on short-term rental platforms. However, the impact of host self-description on trust perception and whether trust perception can influence purchase behavior remain under-studied. Therefore, a text analytics framework was proposed to research the relationships among host self-description, trust perception and purchase behavior on Airbnb. Specifically, a deep-learning-based method was designed to automatically code trust perception of host self-descriptions. And the linguistic and semantic features of description texts were extracted with text mining methods. The estimated order quantity was used to quantify purchase behavior. Then, the influence of linguistic and semantic features on trust perception was identified, and the relationship between trust perception and purchase behavior was also verified. The empirical analysis derives the following findings: i. The readability of self-description is positively associated with trust perception; ii. Perspective taking expressed in self-description is also helpful; iii. Excessive positive sentiment expression can raise barriers to trust building; iv. Paying more attention to family relationship, openness, service and travel experience in self-description would be helpful; v. Trust perception can promote purchases. These findings can help hosts write better self-description, which contributes to trust building and purchases on short-term rental platforms. "],"thought_2":["While the technology acceptance model (TAM) is generally robust it does not always adequately explain user behavior. Recent studies argue that including individual characteristics in TAM can improve our understanding of those conditions under which TAM is not adequate for explaining acceptance behavior. Using this argument, we examine the effects of positive mood, one individual characteristic that significantly affects an individual's cognition and behavior, on acceptance of a DSS that supports uncertain tasks. Our results show that positive mood has a significant influence on DSS acceptance and that its influence on users' behavior is not due to a halo effect. "],"prob":["social, influence, effect, trust, relationship, perceive, perception"],"frex":["trust, social, perception, perceive, influence, intention, effect"],"lift":["trust, perception, intention, perceive, acceptance, social, participant"],"score":["trust, social, influence, perceive, intention, perception, effect"],"proportion":["0.01"]},{"name":["employee, organization, adoption, organizational, outcome, expectation, survey"],"size":[1800],"topic_no":[15],"thought_1":["This research examines the adoption of an interorganizational system standard and its benefits by using RosettaNet as a case study. A comprehensive research framework derived from institutional theory and a Technology- Organization-Environment model was developed for this research. Data were collected from a sample of 212 Malaysian manufacturing firms. A multi-state analytic approach was proposed whereby the research model was tested using structural equation modeling (SEM), and the results from SEM were used as inputs for a neural network model for predicting RosettaNet adoption. The results showed that factors related to the environment, the Interorganizational Relationship (IOR) and an information sharing culture have a positive influence on the adoption of RosettaNet. In terms of organizational factors, top management support was found to have a positive and significant relationship with RosettaNet adoption. The results also showed that RosettaNet adoption has a significant and positive relationship with organizational performance. The research findings can also assist managerial decision making for those organizations planning to adopt RosettaNet. This research reduces the previous research gap by advancing understanding on the relationship of adoption factors and RosettaNet adoption, and extends previous approaches on RosettaNet adoption by investigating the relationships between RosettaNet adoption and organizational performance. Improved existing technology adoption methodology was achieved by integrating both SEM and neural network for examining the adoptions of RosettaNet. "],"thought_2":["Purpose: The purpose of this study is to explore the behavioral intention of the employees to adopt artificial intelligence (AI) integrated customer relationship management (CRM) system in Indian organizations. Design/methodology/approach: To identify the factors impacting the behavioral intention of the employees to adopt AI integrated CRM system in Indian organizations helps of literature review and theories have been taken. Thereafter, some hypotheses have been formulated followed by the development of a theoretical model conceptually. The model has been tested statistically for validation using a survey by considering 308 usable respondents. Findings: The results of this study show that perceived usefulness and perceived ease of use directly impact the behavioral intention of the employees to adopt an AI integrated CRM system in organizations. Also, these two exogenous factors impact the behavioral intention of the employees to adopt an AI integrated CRM system mediating through two intermediate variables such as utilitarian attitude (UTA) and hedonic attitude (HEA). The proposed model has achieved predictive power of 67%. Research limitations/implications: By the help of the technology acceptance model and motivational theory, the predictors of behavioral intention to adopt AI integrated CRM systems in organizations were identified. The effectiveness of the model was strengthened by the consideration of two employee-centric attitudinal attributes such as UTA and HEA, which is claimed to have provided contributions to the extant literature. The proposed theoretical model claims a special theoretical contribution as no extant literature considered the effects of leadership support as a moderator for the adoption of an AI integrated CRM system in Indian organizations. Practical implications: The model implies that the employees using AI integrated CRM system in organizations must be made aware of the usefulness of the system and the employees must not face any complexity to use the system. For this, the managers of the concerned organizations must create a conducive atmosphere congenial for the employees to use the AI integrated CRM system in the organizations. Originality/value: Studies covering exploration of the adoption of AI integrated CRM systems in Indian organizations are found to be in a rudimentary stage and in that respect, this study claims to have possessed its uniqueness. "],"prob":["organization, employee, adoption, organizational, impact, outcome, survey"],"frex":["employee, organization, adoption, organizational, outcome, expectation, survey"],"lift":["employee, organization, organizational, adoption, expectation, questionnaire, interview"],"score":["employee, organization, organizational, adoption, questionnaire, interview, survey"],"proportion":["0.01"]}],"topic_no":[1,15]},{"name":["human, robot, intelligence, robotic, automation, autonomous, people"],"size":[1800],"topic_no":[4],"thought_1":["Many ethicists writing about automated systems (e.g. self-driving cars and autonomous weapons systems) attribute agency to these systems. Not only that; they seemingly attribute an autonomous or independent form of agency to these machines. This leads some ethicists to worry about responsibility-gaps and retribution-gaps in cases where automated systems harm or kill human beings. In this paper, I consider what sorts of agency it makes sense to attribute to most current forms of automated systems, in particular automated cars and military robots. I argue that whereas it indeed makes sense to attribute different forms of fairly sophisticated agency to these machines, we ought not to regard them as acting on their own, independently of any human beings. Rather, the right way to understand the agency exercised by these machines is in terms of human–robot collaborations, where the humans involved initiate, supervise, and manage the agency of their robotic collaborators. This means, I argue, that there is much less room for justified worries about responsibility-gaps and retribution-gaps than many ethicists think. "],"thought_2":["We need an account of corporate agency that is temporally robust—one that will help future people to cope with challenges posed by corporate groups in a range of credible futures. In particular, we need to bequeath moral resources that enable future people to avoid futures dominated by corporate groups that have no regard for human beings. This paper asks how future philosophers living in broken or digital futures might re-imagine contemporary debates about corporate agency. It argues that the only temporally robust account is moralised extreme collectivism, where full moral personhood is accorded (only) to those corporate groups that are reliably disposed to respond appropriately to moral reasons. "],"prob":["human, robot, intelligence, future, robotic, interaction, automation"],"frex":["human, robot, intelligence, robotic, automation, autonomous, people"],"lift":["robot, human, intelligence, robotic, replace, autonomous, live"],"score":["robot, human, robotic, intelligence, autonomous, automation, future"],"proportion":["0.01"]},{"name":["automate, public, principle, concept, law, standard, explain"],"size":[1800],"topic_no":[42],"thought_1":["The aim of this paper is to analyse the very recently approved national Member States’ laws that have implemented the GDPR in the field of automated decision-making (prohibition, exceptions, safeguards): all national legislations have been analysed and in particular 9 Member States Law address the case of automated decision making providing specific exemptions and relevant safeguards, as requested by Article 22(2)(b) of the GDPR (Belgium, The Netherlands, France, Germany, Hungary, Slovenia, Austria, the United Kingdom, Ireland). The approaches are very diverse: the scope of the provision can be narrow (just automated decisions producing legal or similarly detrimental effects) or wide (any decision with a significant impact) and even specific safeguards proposed are very diverse. After this overview, this article will also address the following questions: are Member States free to broaden the scope of automated decision-making regulation? Are ‘positive decisions’ allowed under Article 22, GDPR, as some Member States seem to affirm? Which safeguards can better guarantee rights and freedoms of the data subject? In particular, while most Member States refers just to the three safeguards mentioned at Article 22(3) (i.e. subject's right to express one's point of view; right to obtain human intervention; right to contest the decision), three approaches seem very innovative: a) some States guarantee a right to legibility/explanation about the algorithmic decisions (France and Hungary); b) other States (Ireland and United Kingdom) regulate human intervention on algorithmic decisions through an effective accountability mechanism (e.g. notification, explanation of why such contestation has not been accepted, etc.); c) another State (Slovenia) require an innovative form of human rights impact assessments on automated decision-making. "],"thought_2":["This article presents an initial appraisal of the emerging Australian approach to applying privacy and data protection laws to automated technologies. These laws and the general context in which they operate will be explained, with appropriate comparisons made to the European Union frameworks. In order to examine their specific application vis-à-vis automated technologies, three case studies – automated facial recognition technologies (AFRT), unmanned aerial vehicles (UAVs – better known as ‘drones’) and autonomous vehicles (or ‘driverless cars’) – are selected to examine the extent to which existing privacy and data protection laws, and their application, can be considered adequate to address privacy and data protection risks that these technologies bring. These case studies evidence existing deficiencies with privacy protection in Australia and the inadequacy of recent reform processes, demonstrating that Australian data privacy laws are not well placed to protect individuals' rights vis-à-vis automated technologies. "],"prob":["automate, public, concept, potential, principle, data, standard"],"frex":["automate, public, principle, concept, law, standard, explain"],"lift":["law, principle, automate, public, argue, explain, european"],"score":["law, automate, public, principle, policy, concept, argue"],"proportion":["0.02"]},{"name":["innovation, digital, development, emerge, artificial_intelligence, technology, technological"],"children":[{"name":["innovation, technology, technological, emerge, conceptual, opportunity, development"],"size":[1800],"topic_no":[24],"thought_1":["This paper makes a distinction between three theoretical frameworks that have been highly influential in the discourse on innovation, competitiveness and sustainability: sectoral systems of innovation (SSI), technological innovation systems (TIS) and socio-technical systems (ST-Systems). These frameworks share a common systems approach to innovation but are often positioned as different bodies of literature that correspond to different epistemic communities. This paper is explorative and conceptual in nature. It presents a systematic comparative review of SSI, TIS and ST-Systems based on the following analytical dimensions: (1) system boundaries, (2) actors and networks, (3) institutions, (4) knowledge, (5) dynamics and (6) policy approach. In the concluding section commonalities and differences, of the three approaches, are presented and suggestions for complimentarily are made. "],"thought_2":["The 'Technological Innovation System' (TIS) framework and its system functions have become a popular analytical tool for the study of clean-tech innovation. There is increasing attention for the role of emerging economies in global clean-tech innovation, but the applicability of TIS to emerging economies cases is not entirely straightforward. A key issue is the limited geographical considerations, in particular transnational dimensions in TIS, whereas earlier perspectives on innovation in emerging economies have stressed the role of such transnational dimensions. This paper elaborates transnational TIS actor-networks and institutions, categorizes these in relation to TIS functions, and describes their potential to induce or block TIS development in emerging economies. We draw on insights from the perspectives of National Learning Systems, International Technology Transfer, and Global Production Networks for this purpose. We conclude that the potential effects of these transnational dimensions may be accurately grasped by the existing list of system functions, lending credence to its further application of the TIS framework on emerging economy case studies. Policy makers in emerging economies should recognize these transnational dimensions and seek to optimize their potential effect on domestic TIS development, taking in to consideration a realistic assessment of its role in the global TIS. "],"prob":["technology, innovation, technological, emerge, development, conceptual, opportunity"],"frex":["innovation, technology, technological, emerge, conceptual, opportunity, development"],"lift":["innovation, technological, technology, conceptual, emerge, opportunity, collaboration"],"score":["innovation, technology, technological, conceptual, emerge, development, opportunity"],"proportion":["0.02"]},{"name":["digital, artificial_intelligence, skill, transformation, bring, automation, society"],"size":[1800],"topic_no":[51],"thought_1":["Purpose: The purpose of this paper is to examine new directions for diversity scholarship in the context of future of work or advanced technological shifts that are impacting organizations and society. It proposes that both new opportunities and challenges are likely to emerge for individuals and offers considerations around ethics, inequalities and global dimensions as relevant conversations within this domain. Design/methodology/approach: The paper provides an overview of new technological advances in the domains of artificial intelligence, automation and the gig economy. It then layers considerations related to diversity within this context, focusing on issues of relevance to mainstream, critical and transnational traditions within diversity scholarship. Findings: It is likely that technological shifts will impact several domains of diversity scholarship including how we define “diversity,” and the value and appropriateness of using advanced technologies to replace certain jobs that are predominantly held by underrepresented groups. Furthermore, the paper outlines ways in which bias, ethical considerations and emergent digital inequalities will become important conversations within diversity research in the context of future of work. Originality/value: This paper brings together diversity scholarship and future of work conversations in assessing the ways such research and trends will intersect and provides insights about future directions that diversity-focused research should take to address and understand the consequences of rapid technological advances for inclusion. "],"thought_2":["Public relations’ (PR) professional habitus is defined by a relentless focus on optimism and futurity. This professional habitus renders PR indispensable to the corporate world after crisis, when new, potentially controversial, growth strategies must be sold-in to stakeholders. This article argues that PR’s professional habitus is heavily influenced by neoliberalism, an ideology which ‘confidently identifies itself with the future’. The discussion is timely, as 21st-century neoliberal capitalism becomes redefined by artificial intelligence (AI). The article combines PR theory, communications theory and political economy to consider the changing shape of neoliberal capitalism, as AI becomes naturalised as ‘common sense’ and a ‘public good’. The article explores how PR supports AI discourses, including promoting AI in national competitiveness and promoting ‘friendly’ AI to consumers, while promoting Internet inequalities. The article concludes that the PR profession’s myopia regarding the implications of promoting AI and neoliberalism is shaped by poor levels of diversity in the PR profession. "],"prob":["artificial_intelligence, digital, change, skill, business, future, world"],"frex":["digital, artificial_intelligence, skill, transformation, bring, automation, society"],"lift":["skill, digital, artificial_intelligence, bring, transformation, society, automation"],"score":["skill, artificial_intelligence, digital, automation, business, change, society"],"proportion":["0.02"]}],"topic_no":[24,51]},{"name":["construction, cost, assessment, safety, develop, risk, project"],"children":[{"name":["project, construction, success, productivity, cost, phase, stage"],"size":[1800],"topic_no":[29],"thought_1":["The accurate prediction of the duration of a construction project represents a critical factor for the feasibility study of this project. Employers are in an urgent need for reliable information about the construction duration in this early stage of the project. Such information can materially help project managers create a cash and material flow plan in a pre-set time. This paper aims to develop an artificial neural network (ANN) model for predicting the expected construction duration of building projects in its early stage, where no detailed planning is available. The MATLAB program was used as a suitable environment for developing the proposed model. The required field data was collected from 130 building projects in Egypt, which fall within the appropriate sample size. Testing the validity of the model clearly showed that it has a good prediction capability with a maximum error of 14%. Copyright "],"thought_2":["Purpose The purpose of this paper is to develop models to forecast final budget and duration of a highway construction project during construction stage. Design/methodology/approach Highway construction project data are collected and analyzed to find out factors affecting project final budget and duration before developing the forecasting models, research for which is based on the principle of Artificial Neural Network (ANN). The forecasting results obtained from the proposed method are compared with those obtained from the current method based on earned value. Findings Factors affecting final budget and duration are presented. The forecasting results obtained from the proposed method based on ANN application are more accurate and stable than those obtained from the current method based on earned value. Research limitations/implications Factors affecting final budget and duration may differ if applied in other countries, since the project data were collected in the Kingdom of Thailand. The forecasting models, therefore, must be reconsidered for better outcomes. Practical implications The study presents a useful tool for the highway construction project manager to predict project final budget and duration. The results can potentially provide early warning of over-budget and schedule delay. Originality/value The ANN models to forecast final budget and duration of highway construction projects during the construction stage, developed by using project data reflecting continual and seasonal cycle data, can provide better predicting results. "],"prob":["project, construction, cost, develop, success, productivity, build"],"frex":["project, construction, success, productivity, cost, phase, stage"],"lift":["project, construction, productivity, success, phase, progress, delay"],"score":["project, construction, cost, productivity, success, phase, factor"],"proportion":["0.01"]},{"name":["risk, assessment, safety, assess, mitigate, bayesian, quantify"],"size":[1800],"topic_no":[44],"thought_1":["Purpose - Globally expanding supply chains (SCs) have grown in complexity increasing the nature and magnitude of risks companies are exposed to. Effective methods to identify, model and analyze these risks are needed. Risk events often influence each other and rarely act independently. The SC risk management practices currently used are mostly qualitative in nature and are unable to fully capture this interdependent influence of risks. The purpose of this paper is to present a methodology and tool developed for multi-tier SC risk modeling and analysis. Design/methodology/approach - SC risk taxonomy is developed to identify and document all potential risks in SCs and a risk network map that captures the interdependencies between risks is presented. A Bayesian Theory-based approach, that is capable of analyzing the conditional relationships between events, is used to develop the methodology to assess the influence of risks on SC performance Findings - Application of the methodology to an industry case study for validation reveals the usefulness of the Bayesian Theory-based approach and the tool developed. Back propagation to identify root causes and sensitivity of risk events in multi-tier SCs is discussed. Practical implications - SC risk management has grown in significance over the past decade. However, the methods used to model and analyze these risks by practitioners is still limited to basic qualitative approaches that cannot account for the interdependent effect of risk events. The method presented in this paper and the tool developed demonstrates the potential of using Bayesian Belief Networks to comprehensively model and study the effects or SC risks. The taxonomy presented will also be very useful for managers as a reference guide to begin risk identification. Originality/value - The taxonomy developed presents a comprehensive compilation of SC risks at organizational, industry, and external levels. A generic, customizable software tool developed to apply the Bayesian approach permits capturing risks and the influence of their interdependence to quantitatively model and analyze SC risks, which is lacking. Copyright "],"thought_2":["Reliable and efficient risk assessments are essential to deal effectively with potential risks in international construction projects. However, most conventional risk modeling methods are based on the hypothesis that risk factors are independent, which does not account adequately for the causal relationships among risk factors. In this study, a risk assessment model for international construction projects was developed to improve the efficacy of risk management by integrating fault tree analysis and fuzzy set theory with a Bayesian belief network. The risk rating of each risk factor, expressed as the product of risk occurrence probability and impact, was incorporated into the risk assessment model to evaluate degrees of risk. Therefore, risk factors were categorized into different risk levels taking into account their inherent causal relationships, which allowed the identification of critical risk factors. The applicability of the fuzzy Bayesian belief network-based risk assessment model was verified using a case study through a comparative analysis with the results from a fuzzy synthetic evaluation method. The comparison shows that the proposed risk assessment model is able to provide guidelines for an effective risk management process and ultimately to increase project performance in a complex environment such as international construction projects. "],"prob":["risk, assessment, safety, assess, develop, identify, level"],"frex":["risk, assessment, safety, assess, mitigate, bayesian, quantify"],"lift":["risk, safety, assessment, mitigate, assess, quantify, prevent"],"score":["risk, assessment, safety, assess, bayesian, probability, mitigate"],"proportion":["0.01"]}],"topic_no":[29,44]},{"name":["firm, practice, management, information, company, business, performance"],"children":[{"name":["practice, management, healthcare, practitioner, organisation, manage, gap"],"size":[1800],"topic_no":[55],"thought_1":["COVID-19’s rapid spread has caused a global pandemic. Consequently, it is imperative that healthcare organisations conduct crisis management (CM) to cope with this calamity. This study presents a set of operational guidelines for healthcare organisations to launch effective countermeasures against such crises by means of effective knowledge management (KM) practices. Additionally, information-technology (IT) applications can significantly improve organisations’ CM and KM capabilities by enhancing organisational responsiveness and flexibility. This study thus aims to articulate how the use of innovative IT-enabled mechanisms (e.g., non-contact monitoring devices, intelligent robots, and telemedicine) can reduce the risk of exposure and leverage an artificial intelligence-based epidemic intelligence dashboard to support appropriate decision-making by taking the operation of healthcare organisations in Taiwan during COVID-19 crisis as an example. The research results demonstrate the effectiveness of the employment of IT-enabled KM practices in CM settings in terms of preventing or minimising undesirable crisis consequences. "],"thought_2":["Abstract: Despite significant advances in Clinical Decision Support Systems, they have not been extensively used in nursing practice to date. One key problem is the failure of these systems to fully support actionable nursing practices that guide nurse decision-making. In addition, current workflow-related systems have failed to consider the specific workflow challenges associated with acute-care nursing. In response to these challenges, we describe a novel three-stage approach that builds and evaluates a meta-model that addresses key requirements of multi-level guideline-based clinical nursing-specific decision support. This research-in-progress presents the first two stages of this approach, highlighting the importance of meta-modelling as a tool to identify the essential system-centric information that underpins acute-care nursing practice. "],"prob":["management, practice, information, practitioner, identify, healthcare, manage"],"frex":["practice, management, healthcare, practitioner, organisation, manage, gap"],"lift":["healthcare, practice, management, organisation, practitioner, guideline, gap"],"score":["healthcare, management, practice, organisation, analytics, practitioner, manage"],"proportion":["0.02"]},{"name":["firm, company, corporate, performance, internal, external, strategic"],"size":[1800],"topic_no":[66],"thought_1":["Research Summary: We introduce to the upper echelons literature a novel, linguistic measure of CEOs' Big Five personality traits that we specifically developed and validated using a sample of CEOs. We then provide a predictive test of the measure by applying it to a sample of more than 3,000 CEOs of S&P 1500 firms to explore the direct and interactive effects of CEOs' Big Five personality traits and firm performance on strategic change. Our validated, unobtrusive measure of CEOs' Big Five traits provides a strong foundation for future theory development on the firm-level effects of CEOs' personality traits. Our specific findings also extend our understanding of how CEO personality influences firm-level change and how both person and situation-based factors interact to jointly influence firm strategy. Managerial Summary: This paper introduces a language-based tool we developed to measure the Big Five personality traits (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism) of more than 3,000 CEOs of S&P 1500 firms. After describing our process to develop and validate the tool, we test it by examining how CEOs' Big Five traits influence strategic change, both in isolation and in combination with recent firm performance. Our results suggest that CEOs' personality traits have a meaningful impact on strategic change, but that the nature of these effects differs based on their firms' recent performance. Our tool also provides a strong basis for scholars seeking to measure the personality traits of large samples of public-company executives. "],"thought_2":["Afrequent claim in the entrepreneurship literature is that employees learn to become entrepreneurs during paid employment. We revisit this mechanism in the context of the well-established finding that smaller firms generate higher rates of entrepreneurship. We propose a novel mechanism responsible for higher rates of entrepreneurship emanating from smaller firms: large firms might have a advantage over small firms in providing internal opportunities to retain entrepreneurial talent. We test this claim in a setting where firm dissolution extinguishes internal opportunities, using a new hand-collected data set of career histories in the automatic speech recognition (ASR) industry. For nondefunct firms, we replicate the \"small-firm effect.\" However, the small-firm effect no longer holds within the subsample of defunct firms: entrepreneurship rates among individuals present at firm dissolution are in fact higher for larger firms. Additional analyses indicate that this effect is unlikely to be driven by the early departure of higher-skilled workers who anticipate the firm's demise. Finally, we find preliminary evidence consistent with the notion that large organizations may not only retain but also \"mold\" workers into entrepreneurs. More broadly, the study emphasizes the need to consider a novel mechanism responsible for transition into entrepreneurship-the role of opportunities available to employees in incumbent firms. "],"prob":["performance, company, firm, business, corporate, strategic, external"],"frex":["firm, company, corporate, performance, internal, external, strategic"],"lift":["firm, corporate, internal, company, managerial, external, strategic"],"score":["firm, company, performance, corporate, business, strategic, managerial"],"proportion":["0.01"]}],"topic_no":[55,66]},{"name":["sale, retail, configuration, volume, store, historical, real"],"size":[1800],"topic_no":[30],"thought_1":["Despite being of increasing strategic importance for firms employing B2B salespeople, the concept of sales enablement, and the initiatives it inspires, has heretofore been unexplored by academic sales researchers. This omission is all the more surprising when considering that recent statistics suggest that 61% of firms employing B2B salespeople engage in sales enablement initiatives. The purpose of this agenda setting piece is to introduce the concept of sales enablement to a broader academic audience, and to outline a research agenda for sales researchers. Based on semi-structured, qualitative interviews with practitioners, we propose a framework that suggests that sales enablement can be best understood as a firm-wide strategic initiative that incorporates the 3 Ps of People, Process, and Performance- to deliver value to seller firms and customers alike. Then, we conclude the manuscript by proposing a set of research questions to encourage further academic research into sales enablement. "],"thought_2":["Purpose - Product configurator is a sales and production-planning tool that helps to transform customer requirements into bills-of-materials, lists of features and cost estimations. The purpose of this paper is to introduce a method of how to analyse sales configuration models by using a design structure matrix (DSM) tool. By applying the DSM techniques, the sales configuration managers may sequence the product configuration questions and organize the connection to production. Design/methodology/approach - First, the paper explains a sales configuration system structure from published academic and non-academic works. These sources employ both theoretical and practical views on the topic of computer-based sales expert systems. Second, the paper demonstrates an application of using DSM for configuration modelling. Findings - The current sales configuration approaches include constraint-based, rules-based, and object-oriented approaches. Product description methods vary, but the general problem remains the same: the configuration process should be designed in such a way that customer selections do not affect the previous selections. From the user point of view, answering the questions should be smooth and fast. In turn this will lead to the growing importance of building more effective product configuration models. DSM offers a systematic way to organise customer interface in sales configuration systems. Research limitations/implications - This paper analyses how DSM could help in planning product configuration modelling. Comparison of different sequences is presented. The examples used are hypothetical, but illustrate the suitability of DSM analysis. Companies are trying to establish easily configured product models, which are fast, flexible and cost-effective for adjustments and modifications. Use of DSM may help in the roll-out of sales configuration projects. DSM may also be used as a quick view to represent the complexity of product configurability. The future needs for configuration tools will be focused towards product model management from the technical limitations of different data storage approaches. Practical implications - Configurator software creates product variants, which are logical descriptions of physical products. Variants have parameters which describe the customer-made selections. The parameter selections may have interconnections between the choices. Some selections may affect further selections and some combinations may not be allowed for incompatibility, cost or safety reasons. There are several commercial software packages available for creating product configurations. Product description methods vary, but the general problem remains the same: the configuration process should be designed in such a way that customer selections do not affect the previous selections. Answering the questions should be smooth and fast. Configuration of complex products, for instance, airplanes, may include several sub-systems and have various loops within the quotation process. The use of DSM may help in the roll-out of sales configuration projects. DSM may also be used as a quick view to represent the complexity of product configurability. Originality/value - The paper helps both researchers and practitioners to obtain a clearer view on the development of sales configuration systems and the potential of systematic DSM-based product model analysis. "],"prob":["sale, retail, volume, configuration, store, data, real"],"frex":["sale, retail, configuration, volume, store, historical, real"],"lift":["sale, configuration, retail, volume, store, historical, million"],"score":["sale, retail, volume, configuration, store, historical, data"],"proportion":["0.01"]},{"name":["consumer, satisfaction, online, product, content, service, custom"],"children":[{"name":["custom, service, satisfaction, provider, delivery, experience, encounter"],"size":[1800],"topic_no":[47],"thought_1":["Purpose: The purpose of this paper is to propose an automated machine learning (AutoML) and multi-agent system approach to improve overall product delivery satisfaction under limited resources. Design/methodology/approach: An AutoML method is purposed to model delivery satisfaction of individual customer, and a heuristic method and multi-agent system are proposed to improve overall satisfaction under limited processing capability. A series of simulation experiments have been conducted to illustrate the effectiveness of the proposed methodology. Findings: The simulated results show that the proposed method can effectively improve overall delivery satisfaction, especially when the demand of customer orders is highly fluctuating and when the customer satisfaction models are highly diversified. Practical implications: The proposed framework provides a more dynamic and continuously improving way to model delivery satisfaction of individual customer, thereby supports companies to provide personalized services and develop scalable and flexible business at a lower cost, and ultimately improves the overall quality, efficiency and effectiveness of delivery services. Originality/value: The proposed methodology utilizes AutoML and multi-agent system to model individual customer delivery satisfaction and improve the overall satisfaction. It can cooperate with the existing delivery resource planning methods to further improve customer delivery satisfaction. The authors propose an AutoML approach to model individual customer delivery satisfaction, which enables continuous update and improvements. The authors propose multi-agent system and a heuristic method to improve overall delivery satisfaction. The numerical results show that the proposed method can improve overall delivery satisfaction with limited processing capability. "],"thought_2":["Purpose: The customer contact approach to service has been at the core of service theory since the 1970s. It suggests that the potential operating efficiency of a service is inversely related to the extent of customer contact with the provider's operations and that various service design issues are dictated by the presence or absence of customer contact. The purpose of this article is to reevaluate the customer contact approach in light of advanced digital technologies. Design/methodology/approach: The authors review the origins and history of the customer contact approach and show ways it has been refined in research literature. From that they demonstrate how the refined approach can be applied to contemporary conditions. Findings: Recent advances in digital technologies have indeed required us to revise our conceptualization of customer contact. There is now a blurring between front-office and back-office operations. Emerging technologies are allowing customers to have high-contact experiences with low-contact efficiencies. Research limitations/implications: Going forward, conceptualizations of customer contact are becoming increasingly complex and requiring increasingly complex models. Armed with self-service technologies, customers are able to permeate the “buffered core” of service businesses. Artificial intelligence and anthropomorphic devices have further blurred the distinction between front-office and back-office operations. Research will need to consider new forms of technology-enabled customer contact. Practical implications: Customer contact is no longer limited to interpersonal interactions and the relationships between service providers and customers are increasingly complex. Customers may interact with automated service providers, or service providers may interact with customer technologies. New forms of customer contact may not involve humans at all, but instead involve technologies interacting with technologies. Originality/value: The customer contact approach to service was one of the original models of service design. By revisiting and revising the model we bring it in-line with the realities of the contemporary service economy. "],"prob":["service, custom, satisfaction, provider, experience, improve, delivery"],"frex":["custom, service, satisfaction, provider, delivery, experience, encounter"],"lift":["custom, service, satisfaction, provider, delivery, encounter, satisfy"],"score":["custom, service, satisfaction, provider, delivery, experience, business"],"proportion":["0.01"]},{"name":["consumer, online, product, purchase, content, shop, platform"],"size":[1800],"topic_no":[57],"thought_1":["Understanding the process of consumer decision making is important for many decision support systems. Consumers evaluate different alternatives and then come to a decision. Prior research suggests that consumer evaluations leading to choice are comparative in nature and can be affected by other alternatives or reference products. This study proposes a mixtures-of-experts model framework to examine the role of different reference products in consumer choice of multi-attribute products. While multiple external and internal reference points have been proposed, previous studies have very rarely investigated more than one reference point in the same model. Using data from a choice-based conjoint experiment, our empirical model enables us to identify which product consumers tend to use as the reference product by incorporating four different reference products and includes consumer characteristics to examine how consumers differ in their utilization of different reference products. The results show that our model outperforms other reference-dependent models in prior literature. In our empirical context of smartphone choices, the most commonly used reference product is the most preferred product in the choice set, while the least preferred product and the average product are rarely used. We also examine the role of consumer characteristics such as gender, product familiarity, and product interest in utilizing reference products. This paper provides insights into the unobserved comparison process in consumer choice, which can be applied to decision support systems such as recommendation engines. "],"thought_2":["The purpose of this article is to offer an argument for a wider acceptance and adoption of online auto-ethnography—or auto-netnography as an alternative social media research method to online ethnography—or netnography—when undertaking consumer research. As an online research method, netnographies have attracted increasing attention from researchers in various inter-disciplinary studies during recent years, but the method is still not considered mainstream. While the proliferation of online communities using various social media platforms is increasingly supporting consumers when making product/service choices, the adoption of netnographies appears to leave room for an extension toward the consideration by consumer researchers of how auto-netnography could highlight these researchers’ own personal experiences in online communities. Auto-netnography allows the researcher to capture their own online experiences as a consumer would through social observation, reflexive note-taking, and other forms of data. Contemporary technology can also provide a more innovative approach with artificial intelligence offering an alternative dimension. We contend there is a need for consumer researchers—both academic and practitioner—to further reflect on and discuss the deployment of auto-netnography to contribute to further exploration of online communities through the qualitative lens. "],"prob":["product, online, consumer, content, purchase, platform, shop"],"frex":["consumer, online, product, purchase, content, shop, platform"],"lift":["consumer, purchase, online, product, shop, content, platform"],"score":["consumer, product, online, content, purchase, shop, platform"],"proportion":["0.01"]}],"topic_no":[47,57]},{"name":["sentiment, social_media, negative, opinion, positive, post, content"],"size":[1800],"topic_no":[5],"thought_1":["We present a novel approach for analysing the qualitative content of annual reports. Using natural language processing techniques we determine if sentiment expressed in the text matters in fraud detection. We focus on the Management Discussion and Analysis (MD&A) section of annual reports because of the nonfactual content present in this section, unlike other components of the annual reports. We measure the sentiment expressed in the text on the dimensions of polarity, subjectivity, and intensity and investigate in depth whether truthful and fraudulent MD&As differ in terms of sentiment polarity, sentiment subjectivity and sentiment intensity. Our results show that fraudulent MD&As on average contain three times more positive sentiment and four times more negative sentiment compared with truthful MD&As. This suggests that use of both positive and negative sentiment is more pronounced in fraudulent MD&As. We further find that, compared with truthful MD&As, fraudulent MD&As contain a greater proportion of subjective content than objective content. This suggests that the use of subjectivity clues such as presence of too many adjectives and adverbs could be an indicator of fraud. Clear cases of fraud show a higher intensity of sentiment exhibited by more use of adverbs in the “adverb modifying adjective” pattern. Based on the results of this study, frequent use of intensifiers, particularly in this pattern, could be another indicator of fraud. Moreover, the dimensions of subjectivity and intensity help in accurately classifying borderline examples of MD&As (that are equal in sentiment polarity) into fraudulent and truthful categories. When taken together, these findings suggest that fraudulent MD&As in contrast to truthful MD&As contain higher sentiment content. Copyright © 2016 John Wiley & Sons, Ltd. Copyright "],"thought_2":["Due to the huge popularity of microblogging services, microblogs have become important sources of customer opinions. Sentiment analysis systems can provide useful knowledge to decision support systems and decision makers by aggregating and summarizing the opinions in massive microblogs automatically. The most important component of sentiment analysis systems is sentiment lexicon. However, the performance of traditional sentiment lexicons on microblog sentiment analysis is far from satisfactory, especially for Chinese. In this paper, we propose a data-driven approach to build a high-quality microblog-specific sentiment lexicon for Chinese microblog sentiment analysis system. The core of our method is a unified framework that incorporates three kinds of sentiment knowledge for sentiment lexicon construction, i.e., the word-sentiment knowledge extracted from microblogs with emoticons, the sentiment similarity knowledge extracted from words’ associations among all the messages, and the prior sentiment knowledge extracted from existing sentiment lexicons. In addition, in order to improve the coverage of our sentiment lexicon, we propose an effective method to detect popular new words in microblogs, which considers not only words’ distributions over texts, but also their distributions over users.The detected new words with strong sentiment are incorporated in our sentiment lexicon.We built a microblog-specific Chinese sentiment lexicon on a large microblog dataset with more than 17 million messages. Experimental results on two microblog sentiment datasets show that our microblog-specific sentiment lexicon can significantly improve the performance of microblog sentiment analysis. "],"prob":["sentiment, social_media, negative, positive, opinion, content, post"],"frex":["sentiment, social_media, negative, opinion, positive, post, content"],"lift":["sentiment, social_media, post, opinion, negative, positive, express"],"score":["sentiment, social_media, opinion, post, negative, positive, content"],"proportion":["0.01"]},{"name":["question, topic, literature, interest, response, application, machine_learn"],"children":[{"name":["question, answer, community, response, primary, raise, reason"],"size":[1800],"topic_no":[33],"thought_1":["The question answering system can answer questions from various fields and forms with deep neural networks, but it still lacks effective ways when facing multiple evidences. We introduce a new model called SRQA, which means Synthetic Reader for Factoid Question Answering. This model enhances the question answering system in the multi-document scenario from three aspects: model structure, optimization goal, and training method, corresponding to Multilayer Attention (MA), Cross Evidence (CE), and Adversarial Training (AT) respectively. First, we propose a multilayer attention network to obtain a better representation of the evidences. The multilayer attention mechanism conducts interaction between the question and the passage within each layer, making the token representation of evidences in each layer takes the requirement of the question into account. Second, we design a cross evidence strategy to choose the answer span within more evidences. We improve the optimization goal, considering all the answers’ locations in multiple evidences as training targets, which leads the model to reason among multiple evidences. Third, adversarial training is employed to high-level variables besides the word embedding in our model. A new normalization method is also proposed for adversarial perturbations so that we can jointly add perturbations to several target variables. As an effective regularization method, adversarial training enhances the model's ability to process noisy data. Combining these three strategies, we enhance the contextual representation and locating ability of our model, which could synthetically extract the answer span from several evidences. We perform SRQA on the WebQA dataset, and experiments show that our model outperforms the state-of-the-art models (the best fuzzy score of our model is up to 78.56%, with an improvement of about 2%). "],"thought_2":["This paper presents a solution for question answering system for Vietnamese language by integrating diacritics restoration and question classification via deep learning approach. It could be said that this will be the first research integrating two phases into Vietnamese question answering system. Question classification has a critical role in the question answering system. However if the question has too many missing diacritics, this will make the classification extremely more difficult. In this paper, both automatic insertion of diacritics and question classification tasks are built to rely on deep learning approach. For diacritics restoration task, we apply the Encoder-Decoder LSTM model. The result of the first step will be the input of question classification. We use pre-train word embeddings in the Bidirectional LSTM model for Vietnamese question classification. The deep learning approach for both tasks is powerful and highly accurate model. By integrating diacritics restoration and question classification into Vietnamese question answering system - ICTbot of Binh Duong Department of Information and Communications Support System - it has produced remarkably positive results; thus proves the practicability of this proposed system. "],"prob":["response, question, community, answer, primary, raise, reason"],"frex":["question, answer, community, response, primary, raise, reason"],"lift":["answer, question, community, raise, primary, response, decide"],"score":["answer, question, community, response, primary, raise, reason"],"proportion":["0.01"]},{"name":["topic, science, machine_learn, interest, idea, year, literature"],"size":[1800],"topic_no":[58],"thought_1":["Purpose: This paper aims to identify the intellectual structure of four leading hospitality journals over 40 years by applying mixed-method approach, using both machine learning and traditional statistical analyses. Design/methodology/approach: Abstracts from all 4,139 articles published in four top hospitality journals were analyzed using the structured topic modeling and inferential statistics. Topic correlation and community detection were applied to identify strengths of correlations and sub-groups of topics. Trend visualization and regression analysis were used to quantify the effects of the metadata (i.e. year of publication and journal) on topic proportions. Findings: The authors found 50 topics and eight subgroups in the hospitality journals. Different evolutionary patterns in topic popularity were demonstrated, thereby providing the insights for popular research topics over time. The significant differences in topical proportions were found across the four leading hospitality journals, suggesting different foci in research topics in each journal. Research limitations/implications: Combining machine learning techniques with traditional statistics demonstrated potential for discovering valuable insights from big text data in hospitality and tourism research contexts. The findings of this study may serve as a guide to understand the trends in the research field as well as the progress of specific areas or subfields. Originality/value: It is the first attempt to apply topic modeling to academic publications and explore the effects of article metadata with the hospitality literature. "],"thought_2":["Behavioral science and machine learning have rapidly progressed in recent years. As there is growing interest among behavioral scholars to leverage machine learning, we present strategies for how these methods that can be of value to behavioral scientists using examples centered on behavioral research. "],"prob":["machine_learn, topic, application, literature, interest, year, science"],"frex":["topic, science, machine_learn, interest, idea, year, literature"],"lift":["topic, decade, science, scientific, overview, idea, academic"],"score":["topic, machine_learn, science, literature, trend, interest, academic"],"proportion":["0.02"]}],"topic_no":[33,58]},{"name":["activity, process, internet, application, compute, data, big_data"],"children":[{"name":["mobile, activity, internet, access, device, security, source"],"size":[1800],"topic_no":[37],"thought_1":["The new communications technologies and the convergence of the internet and the mobile phone create possibilities as never seen before in the world. Mobile phones have now become an indispensable tool for man. Attacks on mobile networks and devices have grown in number and sophistication. The need for highly secure identification and personal verification technologies is becoming apparent. Biometric authentication is gaining significance, with these systems offering several advantages over traditional authentication methods. This paper proposes the incorporation of a 'low quality image' based face recognition method to prevent the loss of mobile phones. The camera in the mobile phone captures the image of the user and verifies it with the authentication list. The recognised users are allowed to access the mobile. Any mismatch can facilitate the automatic blocking of the SIM card and the permanent locking of the mobile phone to prevent the access to confidential data. Copyright "],"thought_2":["The deployment of Internet based applications calls for adequate users management procedures, being online registration a critical element. In this respect, Email Based Identification and Authentication (EBIA) is an outstanding technique due to its usability. However, it does not handle properly some major issues which make it unsuitable for systems where security is of concern. In this work we modify EBIA to propose a protocol for users registration. Moreover, we assess the security properties of the protocol using the automatic protocol verifier ProVerif. Finally, we show that the modifications applied to EBIA are necessary to ensure security since, if they are removed, attacks on the protocol are enabled. Our proposal keeps the high usability features of EBIA, while reaching a reasonable security level for many applications. Additionally, it only requires minor modifications to current Internet infrastructures. "],"prob":["activity, internet, information, mobile, source, security, data"],"frex":["mobile, activity, internet, access, device, security, source"],"lift":["mobile, device, access, usage, activity, security, internet"],"score":["mobile, activity, internet, device, security, access, usage"],"proportion":["0.01"]},{"name":["compute, smart, cloud, big_data, data, analytics, data-driven"],"size":[1800],"topic_no":[45],"thought_1":["Employing recent research results covering Internet of Things-based real-time production logistics, and building our argument by drawing on data collected from Catapult, Deloitte, MHI, PwC, SME, Software AG, and ZDNet, we performed analyses and made estimates regarding the relationship between industrial artificial intelligence, smart connected sensors, and big data-driven decision-making processes. Structural equation modeling was used to analyze the collected data. "],"thought_2":["Technology advancements in cloud computing, big data systems, No-SQL database, cognitive systems, deep learning, and other artificial intelligence techniques make the integration of traditional ERP transaction data and big data streaming from various social media platforms and Internet of Things (IOTs) into a unified analytics system not only feasible but also inevitable. Two steps are prominent for this integration. The first, coined as forming the big-data ERP, is the integration of traditional ERP transaction data and the big data and the second is to integrate the big-data ERP with business analytics (BA). As ERP implementers and BA users are facing various challenges, managers responsible for this big-data ERP-BA integration are also seriously challenged. To help them deal with these challenges, we develop the SIST model (including Strategic alignment, Intellectual and Social capital integration, and Technology integration) and propose that this integration is an evolving portfolio with various maturity levels for different business functions, likely leading to sustainable competitive advantages. "],"prob":["data, process, compute, big_data, smart, cloud, application"],"frex":["compute, smart, cloud, big_data, data, analytics, data-driven"],"lift":["cloud, smart, compute, big_data, data-driven, thing, execution"],"score":["cloud, data, smart, big_data, compute, process, analytics"],"proportion":["0.02"]}],"topic_no":[37,45]},{"name":["software, work, machine, job, time, requirement, task"],"children":[{"name":["work, task, job, worker, machine, time, fit"],"size":[1800],"topic_no":[39],"thought_1":["The capability of AI is currently expanding beyond mechanical and repetitive to analytical and thinking. A “Feeling Economy” is emerging, in which AI performs many of the analytical and thinking tasks, and human workers gravitate more toward interpersonal and empathetic tasks. Although these people-focused tasks have always been important to jobs, they are now becoming more important to an unprecedented degree. To manage more effectively in the Feeling Economy, managers must adapt the nature of jobs to compensate for the fact that many of the analytical and thinking tasks are increasingly being performed by AI, and, thus, human workers must place increased emphasis on the empathetic and emotional dimensions of their work. "],"thought_2":["Much Workload Control research has focussed on the order release stage but failed to address practical considerations that impact practical application. Order release mechanisms have been developed through simulations that neglect job size variation effects while empirical evidence suggests groups of small/large jobs are often found in practice. When job sizes vary, it is difficult to release all jobs effectivelysmall jobs favour a short period between releases and a tight workload bounding while large jobs require a longer period between releases and a slacker workload bounding. This paper represents a return from a case study setting to theory building. Through simulation, the impact of job sizes on overall performance is explored using all three aggregate load approaches. Options tested include: using distinct load capacities for small/large jobs and prioritising based on job size or routing length. Results suggest the best solution is assigning priority based on routing length; this improved performance, especially for large jobs, and allowed a short release period to be applied, as favoured by small jobs. These ideas have also been applied to a second practical problem: how to handle rush orders. Again, prioritisation, given to rush orders, leads to the best overall shop performance. "],"prob":["work, task, time, machine, job, worker, numb"],"frex":["work, task, job, worker, machine, time, fit"],"lift":["job, worker, work, task, machine, fit, time"],"score":["job, work, task, machine, worker, time, shop"],"proportion":["0.01"]},{"name":["software, maintenance, requirement, failure, reliability, engineer, equipment"],"size":[1800],"topic_no":[46],"thought_1":["Purpose: Opportunistic maintenance (OM) policy is a prospective maintenance approach that instigates for a more effective and optimized system. The purpose of this paper is to provide the steps and methods used in model development processes for the application of the OM policy. Design/methodology/approach: Dubbed as opportunistic principle toward optimal maintenance system (OPTOMS) for OM policy toward optimal maintenance system, the model is devised as a decision support system model and contains five phases. The motivation and focus of the model resolve around the need for a practical framework or model of maintenance policy for the application in an industry. In this paper, the OPTOMS model was verified and validated to ensure that the model is applicable in the industry and robust as a support system in decision making for the optimal maintenance system. Findings: From the verification steps conducted in a case study company, it was found that the developed model incorporated simple but practical tools like check sheet, failure mode and effect analysis (FMEA), control chart that has been commonly used in the industry. Practical implications: This paper provides the general explanations of the developed model and tools used for each phase in implementing OM to achieve an optimal maintenance system. Based on a case study conducted in a semiconductor company, the OPTOMS model can align and prepare the company in increasing machine reliability by reducing machine downtime. Originality/value: The novelty of this paper is based on the in-depth discussion of all phases and steps in the model that emphasize on how the model will become practical theories in conducting an OM policy in a company. The proposed methods and tools for data collection and analysis are practical and commonly used in the industry. The framework is designed for practical application in the industry. The users would be from the Maintenance and Production Department. "],"thought_2":["Software requirements engineering is a critical discipline in the software development life cycle. The major problem in software development is the selection and prioritization of the requirements in order to develop a system of high quality. This research analyzes the issues associated with existing software requirement prioritization techniques. One of the major issues in software requirement prioritization is that the existing techniques handle only toy projects or software projects with very few requirements. The current techniques are not suitable for the prioritization of a large number of requirements in projects where requirements may grow to the hundreds or even thousands. The research paper proposes an expert system, called the Priority Handler (PHandler), for requirement prioritization. PHandler is based on the value-based intelligent requirement prioritization technique, neural network and analytical hierarchical process in order to make the requirement prioritization process scalable. The back-propagation neural network is used to predict the value of a requirement in order to reduce the extent of expert biases and make the PHandler efficient. Moreover, the analytical hierarchy process is applied on prioritized groups of requirements in order to enhance the scalability of the requirement prioritization process. "],"prob":["software, requirement, maintenance, failure, development, tool, engineer"],"frex":["software, maintenance, requirement, failure, reliability, engineer, equipment"],"lift":["maintenance, software, equipment, reliability, failure, requirement, module"],"score":["maintenance, software, failure, requirement, equipment, reliability, module"],"proportion":["0.01"]}],"topic_no":[39,46]},{"name":["dynamic, event, behavior, situation, behaviour, dynamics, game"],"size":[1800],"topic_no":[27],"thought_1":["The paper explores the implications of melioration learning-an empirically significant variant of reinforcement learning-for game theory. We show that in games with invariable pay-offs melioration learning converges to Nash equilibria in a way similar to the replicator dynamics. Since melioration learning is known to deviate from optimizing behavior when an action's rewards decrease with increasing relative frequency of that action, we also investigate an example of a game with frequency-dependent pay-offs. Interactive melioration learning is then still appropriately described by the replicator dynamics, but it indeed deviates from rational choice behavior in such a game. "],"thought_2":["Games technology has undergone tremendous development. In this article, the authors report the rapid advancement that has been observed in the way games software is being developed, as well as in the development of games content using game engines. One area that has gained special attention is modeling the game environment such as terrain and buildings. This article presents the continuous level of detail terrain modeling techniques that can help generate and render realistic terrain in real time. Deployment of characters in the environment is increasingly common. This requires strategies to map scalable behavior characteristics for characters as well. The authors present two important aspects of crowd simulation: the realism of the crowd behavior and the computational overhead involved. A good simulation of crowd behavior requires delicate balance between these aspects. The focus in this article is on human behavior representation for crowd simulation. To enhance the player experience, the authors present the concept of player adaptive entertainment computing, which provides a personalized experience for each individual when interacting with the game. The current state of game development involves using very small percentage (typically 4% to 12%) of CPU time for game artificial intelligence (AI). Future game AI requires developing computational strategies that have little involvement of CPU for online play, while using CPU's idle capacity when the game is not being played, thereby emphasizing the construction of complex game AI models offline. A framework of such nonconventional game AI models is introduced. "],"prob":["dynamic, behavior, event, complex, situation, play, choice"],"frex":["dynamic, event, behavior, situation, behaviour, dynamics, game"],"lift":["game, dynamics, dynamic, behaviour, event, behavior, situation"],"score":["game, behavior, dynamic, event, dynamics, team, behaviour"],"proportion":["0.02"]},{"name":["agent, intelligent, expert, environment, knowledge, domain, process"],"children":[{"name":["agent, action, intelligent, paradigm, communication, interface, environment"],"size":[1800],"topic_no":[54],"thought_1":["The design and development of the Open Agent Architecture (OAA)3 system has focused on providing access to agent-based applications through an intelligent, cooperative, distributed, and multimodal agent-based user interface. Only the primary user interface agents need run on the local computer, thereby simplifying the task of using a range of applications from a variety of platforms, especially low-powered computers. An important consideration in the design of the OAA was to facilitate the reuse of agents in new and unanticipated applications, and to support rapid prototyping. The utility of the agents and tools developed has been demonstrated by their use as infrastructure in unrelated projects. "],"thought_2":["The main aim of this paper is to discuss the design issues and implications that relate to the use of software agents in Training Systems. We have designed and implemented a Radiotherapy Treatment Planning Learning Environment (called RATAPLAN), which incorporates an interface intelligent agent to support training of Simulation Technologists and Radiation Physicists in this knowledge domain. The learning environment consists of an interactive simulation. The interface agent (called Consulta) acts both as demonstrator and assistant to the users. The paper describes in depth the agent's architecture and illustrates the agent-user interaction and communication. With Consulta we promote the collaboration between novice and expert practitioners in Radiotherapy Treatment Planning. "],"prob":["agent, intelligent, environment, action, communication, interaction, architecture"],"frex":["agent, action, intelligent, paradigm, communication, interface, environment"],"lift":["agent, paradigm, action, interface, virtual, interactive, intelligent"],"score":["agent, intelligent, action, communication, environment, architecture, virtual"],"proportion":["0.02"]},{"name":["knowledge, expert, domain, acquisition, knowledge_based, describe, reason"],"size":[1800],"topic_no":[64],"thought_1":["The purpose of the paper is to propose a framework for the development of a decision support system in order to evaluate the knowledge boundaries in agricultural value chain. Knowledge boundaries exist due to differences in the way we work, share our knowledge, expertise, different organisational culture, etc. In this paper, we identify the most common knowledge boundaries that are reported in the literature, and propose a general framework for a preparation of a decision support system to evaluate the existing knowledge boundaries. In particular, we are interested in identifying the knowledge boundaries in agricultural value chain, evaluating them and providing possible solutions of crossing them. It is a two-step method: firstly, a semi-automatic ontology is generated using the freely available tool OntoGen, which we use to define the most commonly reported concepts in crossing knowledge boundaries, and then, based on the obtained ontology, we propose a decision support system for evaluation of the level to which the boundaries exist. © 2018, "],"thought_2":["Much of today's organizational knowledge still exists outside of formal information repositories and often only in people's heads. While organizations are eager to capture this knowledge, existing acquisition methods are not up to the task. Neither traditional artificial intelligence-based approaches nor more recent, less-structured knowledge management techniques have overcome the knowledge acquisition challenges. This article investigates knowledge acquisition bottlenecks and proposes the use of collaborative, conversational knowledge management to remove them. The article demonstrates the opportunity for more effective knowledge acquisition through the application of the principles of Bazaar style, open-source development. The article introduces wikis as software that enables this type of knowledge acquisition. It empirically analyzes the Wikipedia to produce evidence for the feasibility and effectiveness of the proposed approach. Copyright "],"prob":["knowledge, expert, domain, process, describe, reason, acquisition"],"frex":["knowledge, expert, domain, acquisition, knowledge_based, describe, reason"],"lift":["acquisition, knowledge_based, knowledge, expert, discovery, domain, acquire"],"score":["acquisition, knowledge, expert, domain, knowledge_based, process, reason"],"proportion":["0.02"]}],"topic_no":[54,64]}],"topic_no":[1,15,4,42,24,51,29,44,55,66,30,47,57,5,33,58,37,45,39,46,27,54,64]},{"name":["decision_support, selection, decision_make, evaluation, process, decision, select"],"children":[{"name":["decision_support, decision, decision_support_system, decision_make, maker, support, aid"],"size":[1800],"topic_no":[9],"thought_1":["Understanding the culture of decisions in an organizational context is imperative for the successful design and delivery of decision support systems. The cultural paradigm is especially important in the context of strategic decisions. This paper develops a framework for emphasizing the cultural distinctions around decision processes that semantically articulates those distinctions through four culturally-embedded decision modes: making a decision, taking a decision, baking a decision, and faking a decision. The paper then articulates the multidimensionality of decisions around the types of actors, the reach and range of the decision, and the decision's central metaphor. Using those dimensions, the paper shows the implications of those four decision modes for the design and delivery of decision support systems. This is illustrated by an example of an enterprise using DSS/EISfor strategic decision making. The paper concludes with the observation that the critical leverage points for the successful design and delivery of DSS may be different depending on the dominant decision culture. "],"thought_2":["Decision problems at the strategic level tend to have multiple criteria and outcomes that are uncertain. Many of the current decision-making tools are too simplistic to incorporate the important features. This paper considers a multicriteria decision-making scenario in which the outcomes of the decisions, evaluated on different criteria, are uncertain. The main contribution of this paper is the presentation of a tool that enables decision makers to visualize the expected payoff and likelihood that the payoff of a decision does not fall short of a preset target value. Furthermore, it presents decision makers with a tool that shows the tradeoff between expected payoff and downside risk. A variety of solution techniques are suggested that build upon this visualization. "],"prob":["decision, decision_support, decision_make, process, decision_support_system, support, maker"],"frex":["decision_support, decision, decision_support_system, decision_make, maker, support, aid"],"lift":["decision_support_system, decision_support, decision, decision_make, maker, aid, assist"],"score":["decision_support_system, decision, decision_support, decision_make, maker, process, aid"],"proportion":["0.02"]},{"name":["selection, evaluation, criterion, supplier, select, choose, quantitative"],"size":[1800],"topic_no":[22],"thought_1":["Multiple Criteria Decision Making (MCDM) methods generally require the decision maker to evaluate alternatives with respect to decision criteria and also to assign importance weightings to the criteria. Then, based on the assigned weightings, the best alternative can be selected. However, after a decision is made it often happens that the decision maker becomes doubtful whether the right weightings have been assigned to the criteria given that a variety of eventualities may occur in the near future. The main aim of this paper is to address this concern and improve the application of MCDM methods by addressing possible fluctuations in the criteria weightings. The recently proposed concept of stratification (CST) is used in conjunction with MCDM methods to stratify the decision environment. The method is then applied to a supplier selection problem. The stratified MCDM (SMCDM) approach is in its early stages only and requires further research to reach its maturity. "],"thought_2":["This paper proposes a multi-attribute comprehensive evaluation method of individual research output (IRO). It highlights the fact that a single index can never give more than a rough approximation to IRO, and the evaluation of IRO is a multi-attribute complex problem. Firstly, an evaluation index system is established by determining evaluation attributes and choosing the appropriate bibliometric indicators. To address the multiple authorship problem, this paper develops an improved number-of-papers-published indicator. Following this, TOPSIS method is used to conduct a comprehensive IRO evaluation. Then this paper uses a case study to test the feasibility of the methodology. Finally, this paper discusses the effectiveness of the proposed method. Compared with traditional single-indicator evaluation approaches, the proposed multi-attribute evaluation takes more aspects into consideration, therefore it is able to effectively overcome the one-sidedness of a single indicator. The proposed method also has significant advantages compared with other comprehensive IRO evaluation methods. "],"prob":["evaluation, selection, select, criterion, evaluate, supplier, choose"],"frex":["selection, evaluation, criterion, supplier, select, choose, quantitative"],"lift":["supplier, criterion, selection, evaluation, hierarchy, select, quantitative"],"score":["supplier, selection, criterion, evaluation, select, hierarchy, qualitative"],"proportion":["0.02"]}],"topic_no":[9,22]},{"name":["measure, fuzzy, attribute, set, alternative, rank, rule"],"children":[{"name":["attribute, preference, alternative, measure, conflict, rank, association"],"size":[1800],"topic_no":[40],"thought_1":["Measuring consistency of preferences is very important in decision-making. This paper addresses this key issue for interval-valued reciprocal preference relations. Existing studies implement one of two different measures: the “classical” consistency measure, and the “boundary” consistency measure. The classical consistency degree of an interval-valued reciprocal preference relation is determined by its associated reciprocal preference relation with highest consistency degree, while the boundary consistency degree is determined by its two associated boundary reciprocal preference relations. However, the consistency index of an interval-valued reciprocal preference relation should be determined by taking into account all its associated reciprocal preference relations. Motivated by this, a new consistency measure for interval-valued reciprocal preference relations, the average-case consistency measure, is suggested and introduced. The new average-case consistency measure of an interval-valued reciprocal preference relation is determined as the average consistency degree of all reciprocal preference relations associated to the interval-valued reciprocal preference relation. Furthermore, the analysis and comparison of the different consistency measure internal mechanisms is used to justify the validity of the average-case consistency measure. Finally, an average-case consistency improving method which aims to obtain a modified interval-valued reciprocal preference relation with a required average consistency degree is developed. "],"thought_2":["Purpose - This paper explores the relationships between body type and fit preferences with body cathexis, clothing benefits sought by consumers, and demographic profiles of consumers. Design/methodology/approach - The survey instrument consisted of a questionnaire with scales assessing fit preference, body type, body cathexis, clothing benefits sought and consumer demographics. Findings - Significant associations were found between body cathexis (satisfaction with head/upper body, lower body, height, weight and torso) and body shape. The degree of satisfaction with different body parts depended on the body type of the individual. The level of satisfaction with head/upper body, height and torso did not vary by body type. No significant differences were found between fit preferences and body type for lower body garments. Research limitations/implications - The majority of respondents were between the ages 18 and 28, affluent Caucasian Americans, with an hourglass body type, who had a family income of $85,000 or more and shopped in department or boutique/specialty stores. Originality/value - Understanding the fit preferences of female consumers could help apparel companies to produce and meet demands for comfortable and well fitting clothes for women. The results of this research may be used as a first step to develop an expert system to correlate body shape and fit preferences of consumers. "],"prob":["measure, attribute, alternative, rank, preference, set, conflict"],"frex":["attribute, preference, alternative, measure, conflict, rank, association"],"lift":["preference, attribute, conflict, alternative, association, measure, rank"],"score":["preference, measure, attribute, rank, alternative, conflict, association"],"proportion":["0.01"]},{"name":["fuzzy, rule, inference, fuzzy_logic, uncertainty, rule-based, set"],"size":[1800],"topic_no":[52],"thought_1":["A new approach to the rule-base evidential reasoning based on the synthesis of fuzzy logic, Atannasov's intuitionistic fuzzy sets theory and the Dempster-Shafer theory of evidence is proposed. It is shown that the use of intuitionistic fuzzy values and the classical operations on them directly may provide counter-intuitive results. Therefore, an interpretation of intuitionistic fuzzy values in the framework of Dempster-Shafer theory is proposed and used in the evidential reasoning. The merits of the proposed approach are illustrated with the use of developed expert systems for diagnostics of type 2 diabetes. Using the real-world examples, it is shown that such an approach provides reasonable and intuitively obvious results when the classical method of rule-base evidential reasoning cannot produce any reasonable results. "],"thought_2":["Hesitant fuzzy sets are very useful to deal with group decision making problems when experts have a hesitation among several possible memberships for an element to a set. During the evaluating process in practice, however, these possible memberships may be not only crisp values in [0, 1], but also interval values. In this study, we extend hesitant fuzzy sets by intuitionistic fuzzy sets and refer to them as generalized hesitant fuzzy sets. Zadeh's fuzzy sets, intuitionistic fuzzy sets and hesitant fuzzy sets are special cases of the new fuzzy sets. We redefine some basic operations of generalized hesitant fuzzy sets, which are consistent with those of hesitant fuzzy sets. Some arithmetic operations and relationships among them are discussed as well. We further introduce the comparison law to distinguish two generalized hesitant fuzzy sets according to score function and consistency function. Besides, the proposed extension principle enables decision makers to employ aggregation operators of intuitionistic fuzzy sets to aggregate a set of generalized hesitant fuzzy sets for decision making. The rationality of applying the proposed techniques is clarified by a practical example. At last, the proposed techniques are devoted to a decision support system. "],"prob":["fuzzy, rule, set, uncertainty, inference, fuzzy_logic, rule-based"],"frex":["fuzzy, rule, inference, fuzzy_logic, uncertainty, rule-based, set"],"lift":["fuzzy, rule, fuzzy_logic, rule-based, inference, uncertainty, uncertain"],"score":["fuzzy, rule, inference, uncertainty, fuzzy_logic, rule-based, set"],"proportion":["0.01"]}],"topic_no":[40,52]},{"name":["strength, quality, property, high, correlation, air, test"],"children":[{"name":["property, air, quality, correlation, metric, improvement, relationship"],"size":[1800],"topic_no":[12],"thought_1":["Air transportation direct share is the ratio of direct passengers to total passengers on a directional origin and destination (O&D) pair. Direct share is an essential factor of passenger flow distribution and shows passengers' general preference for direct flight services on a certain O&D. A better understanding and a more accurate forecast of direct share can benefit air transportation planners, airlines, and airports in multiple ways. In most of the previous research and applications, it is commonly assumed that direct share is a fixed ratio, which contradicts the air transportation practice. In the Federal Aviation Administration (FAA) Terminal Area Forecast (TAF), the O&D direct share is forecasted as a constant based on the latest observation of direct share on the O&D. To find factors which have significant impacts on O&D direct share and to build an accurate model for O&D direct share forecasting, both parametric and nonparametric machine learning models are investigated in this research. We propose a novel category-based learning method which can provide better forecasting performance compared to employing the single modeling method for O&D direct share forecasting. Based on the comparison, the developed category-based learning model is a promising replacement for the model used for O&D direct share forecasting by the FAA TAF. "],"thought_2":["Air permeability is one of the most important utility properties of textile materials as it in-fuences air fow through textile material. Air permeability plays a significant role in textiles for clothing due to their influence on physiological comfort. Air permeability is also very important in technical textiles, especially for filtration, automotive airbags, parachutes, etc. The air permeability of textile materials depends on their porosity. There are a lot of structural properties of textile materials influencing air permeability and there are also statistically significant interactions between the main factors influencing the air permeability of fabrics. It justifies the application of artificial neural networks (ANNs) to predict the air permeability of textile materials on the basis of their structural parameters. Within the framework of the work presented ANNs were applied to predict the air permeability of cotton woven fabrics. "],"prob":["quality, property, air, correlation, metric, relationship, high"],"frex":["property, air, quality, correlation, metric, improvement, relationship"],"lift":["air, property, quality, correlation, metric, direct, measurement"],"score":["air, quality, property, correlation, metric, monitor, relationship"],"proportion":["0.01"]},{"name":["strength, test, carry, high, successfully, increase, mix"],"size":[1800],"topic_no":[67],"thought_1":["High strength concrete (HSC) (50–100 MPa) and ultra-high strength concrete (UHSC) (>100 MPa) have been increasingly used in the construction industry due to its inherent performance characteristics. However, these concrete mixes have a higher carbon footprint and it is vital to consider the embodied carbon of the HSC and UHSC due to the massive consumption throughout the world. In this study, embodied carbon analysis, using machine learning algorithms has been carried out to minimize the carbon footprint of concrete without jeopardizing the mechanical properties of the concrete. Machine learning models are developed using experimental results in the literature and used to predict the compressive strength of concrete using the constituent materials. Using the experimental data and machine-learned models for mix designs, embodied carbon emissions were calculated. It is shown that there can be many mix compositions which have the same compressive strength while having significantly different embodied carbon values. Based on experimental and machine learned mix designs, an equation to predict the average embodied carbon value for concrete mixes is proposed. The study suggested proposed intervals for the benchmark function in order to propose a region where the embodied carbon value of a concrete mix should lie while achieving the desired compressive strength. Finally, it is shown that machine learning can be used successfully to identify the high strength concrete mixes while minimizing the embodied carbon value of that mix composition. Finally, guidelines are presented to produce a concrete mix within proposed benchmark limits while achieving the desirable strength grade. "],"thought_2":["Aluminum and steel are widely used in automotive and aerospace industries. As a new type of solid-phase welding, ultrasonic spot welding is an effective way to achieve joints of high strength. In this paper, ultrasonic welding was carried out on aluminum-steel dissimilar alloys to investigate the influences of welding parameters on joint strength. Designed and conducted a 3-factor, 3-level comprehensive test. The analyses of test results show that there are 3 kinds of fractures on the welding joint with different welding parameters. The highest strength can reach 3910 N. Clamping force and vibration amplitude not significantly impact the tensile strength. Vibration time significantly impact the tensile strength although its significance level is close to the threshold. The interaction between welding parameters all can significantly impact the tensile strength. The artificial neural network optimized by Genetic Algorithm was used to establish an analytical model. The supplemental experiment and residual analysis were conducted to verify the accuracy of the analytical model. The analytical model show that with the increase of clamping force, the changes of optimal and minimum strength are limited, but the range of welding parameters to obtain a higher strength change significantly; the optimal welding parameters from lower vibration amplitude and higher vibration time shifts towards to higher vibration amplitude and shorter vibration time gradually; for 0.3 Mpa clamping force, the influences of vibration amplitude and vibration time on tensile strength are not significant. "],"prob":["test, high, strength, carry, increase, develop, time"],"frex":["strength, test, carry, high, successfully, increase, mix"],"lift":["strength, carry, test, successfully, mix, investigation, high"],"score":["strength, test, high, carry, increase, mix, time"],"proportion":["0.01"]}],"topic_no":[12,67]},{"name":["patient, health, medical, care, disease, record, treatment"],"size":[1800],"topic_no":[23],"thought_1":["Cancer is a worldwide health problem with extremely high morbidity and mortality. Pancreatic cancer specifically is the fourth leading cause of death by cancer in the United States and is a leading cause of cancer deaths worldwide. The optimal treatment for pancreatic cancer is resection surgery, but even with surgery many patients suffer high morbidity and mortality, leading to regret in physicians over whether or not the optimal course of treatment with regard to the patient's quality of life was made. Patients also suffer regret concerning the morbidity associated with treatment. An artificial neural network is developed to predict 7-month survival of pancreatic cancer patients that achieves over a 91% sensitivity and an overall accuracy above 70%. The artificial neural network outcome predictions may be used as an additional source of information to assist physicians and patients in selecting the treatment that provides the best quality of life for the patient and reduces treatment decision regret. "],"thought_2":["Patients at risk for hepatocellular carcinoma or liver cancer should undergo semiannual screening tests to facilitate early detection, effective treatment options at lower cost, better recovery prognosis, and higher life expectancy. Health care institutions invest in direct-to-patient outreach marketing to encourage regular screening. They ask the following questions: (1) Does the effectiveness of outreach vary among patients and over time?; (2) What is the return on outreach?; and (3) Can patient-level targeted outreach increase the return? The authors use a multiperiod, randomized field experiment involving 1,800 patients. Overall, relative to the usual-care condition, outreach alone (outreach with patient navigation) increases screening completion rates by 10–20 (13–24) percentage points. Causal forests demonstrate that patient-level treatment effects vary substantially across periods and by patients’ demographics, health status, visit history, health system accessibility, and neighborhood socioeconomic status, thereby facilitating the implementation of the targeted outreach program. A simulation shows that the targeted outreach program improves the return on the randomized outreach program by 74%–96% or $1.6 million to $2 million. Thus, outreach marketing provides a substantial positive payoff to the health care system. "],"prob":["health, patient, medical, record, disease, treatment, care"],"frex":["patient, health, medical, care, disease, record, treatment"],"lift":["care, patient, health, disease, medical, record, treatment"],"score":["care, patient, health, disease, medical, treatment, record"],"proportion":["0.01"]},{"name":["resource, water, scale, spatial, location, map, area"],"children":[{"name":["resource, water, area, plant, site, storage, gain"],"size":[1800],"topic_no":[18],"thought_1":["The objective of this research study was to evaluate the consequences of climate change on shifts in distributions of plant species and the vulnerability of the species in Peninsular Thailand. A sub-scene of the predicted climate in the year 2100, under the B2a scenario of the Hadley Centre Coupled Model, version 3 (HadCM3), was extracted and calibrated with topographic variables. A machine learning algorithm based on the maximum entropy theory (Maxent) was employed to generate ecological niche models of 66 forest plant species from 22 families. The results of the study showed that altitude was a significant factor for calibrating all 19 bioclimatic variables. According to the global climate data, the temperature in Peninsular Thailand will increase from 26.6 °C in 2008 to 28.7 °C in 2100, while the annual precipitation will decrease from 2253 mm to 2075 mm during the same period. Currently, nine species have suitable distribution ranges in more than 15% of the region, 20 species have suitable ecological niches in less than 10% while the ecological niches of many Dipterocarpus species cover less than 1% of the region. The number of trees gaining or losing climatically suitable areas is quite similar. However, 10 species have a turnover rate greater than 30% of the current distribution range and the status of several species will in 2100 be listed as threatened. Species hotspots are mainly located in large, intact protected forest complexes. However, several landscape indices indicated that the integrity of species hotspots in 2100 will deteriorate significantly due to the predicted climate change. "],"thought_2":["Huangshui River Valley (HRV) is a typical valley area located in the northeast of the Qinghai-Tibetan plateau. Its long and narrow space, large vertical gradient and diverse land use types bring great challenges to the land use zoning in this region. The paper aims to propose a gridding-self-organizing feature maps (SOFM) coupled method based on grid cell, which will be more reliable and refined, if compared to the method of commonly used administrative regions as the basic analytic units. Firstly the focused study area is divided into 500 m × 500 m grid cells. Six indexes, including grassland, cultivated land, forest land, construction land, population density and per capita income are both spatialized and quantified in every grid. Meanwhile, that SOFM neural network model is at the usage of classifying the land use into six zoning areas can be certain. And urban area, urban-rural transition area, agriculture-forestry transition area, typical agricultural area, agriculture-pastoral transition area, and typical pastoral area are sure to belong to them. Its results can vividly indicate the actual situation where land use pattern is in constant changes in Huangshui River Valley (HRV). The research will provide a specific method for the substantial utilization of land resources. "],"prob":["resource, area, water, plant, site, storage, potential"],"frex":["resource, water, area, plant, site, storage, gain"],"lift":["water, plant, resource, site, area, storage, utilization"],"score":["water, resource, plant, area, site, storage, treatment"],"proportion":["0.01"]},{"name":["map, location, scale, road, city, spatial, urban"],"size":[1800],"topic_no":[65],"thought_1":["Background: The rapid and often uncontrolled rural-urban migration in Sub-Saharan Africa is transforming urban landscapes expected to provide shelter for more than 50% of Africa's population by 2030. Consequently, the burden of malaria is increasingly affecting the urban population, while socio-economic inequalities within the urban settings are intensified. Few studies, relying mostly on moderate to high resolution datasets and standard predictive variables such as building and vegetation density, have tackled the topic of modeling intra-urban malaria at the city extent. In this research, we investigate the contribution of very-high-resolution satellite-derived land-use, land-cover and population information for modeling the spatial distribution of urban malaria prevalence across large spatial extents. As case studies, we apply our methods to two Sub-Saharan African cities, Kampala and Dar es Salaam. Methods: Openly accessible land-cover, land-use, population and OpenStreetMap data were employed to spatially model Plasmodium falciparum parasite rate standardized to the age group 2-10 years (PfPR2-10) in the two cities through the use of a Random Forest (RF) regressor. The RF models integrated physical and socio-economic information to predict PfPR2-10 across the urban landscape. Intra-urban population distribution maps were used to adjust the estimates according to the underlying population. Results: The results suggest that the spatial distribution of PfPR2-10 in both cities is diverse and highly variable across the urban fabric. Dense informal settlements exhibit a positive relationship with PfPR2-10 and hotspots of malaria prevalence were found near suitable vector breeding sites such as wetlands, marshes and riparian vegetation. In both cities, there is a clear separation of higher risk in informal settlements and lower risk in the more affluent neighborhoods. Additionally, areas associated with urban agriculture exhibit higher malaria prevalence values. Conclusions: The outcome of this research highlights that populations living in informal settlements show higher malaria prevalence compared to those in planned residential neighborhoods. This is due to (i) increased human exposure to vectors, (ii) increased vector density and (iii) a reduced capacity to cope with malaria burden. Since informal settlements are rapidly expanding every year and often house large parts of the urban population, this emphasizes the need for systematic and consistent malaria surveys in such areas. Finally, this study demonstrates the importance of remote sensing as an epidemiological tool for mapping urban malaria variations at large spatial extents, and for promoting evidence-based policy making and control efforts. "],"thought_2":["Many studies have explored the relationship between population density and obesity, but there is no consensus, particularly in dense Chinese cities. This study applied gradient boosting decision trees to 2014 national survey data to examine the non-linear or threshold effects of population density at both local and regional levels on waist-hip ratio (WHR), controlling for other built environment elements and socio-demographics. Built environment elements collectively have a stronger predictive power than socio-demographics (56.6% vs. 43.4%). Within a certain range, regional population density is negatively associated with WHR, but its marginal effect diminishes beyond the upper threshold. Local population density has a U-shaped relationship with WHR. These results suggest that urban planners can alleviate the risk of obesity through population densification, but over-densification tends to be inefficient, and sometimes counterproductive. "],"prob":["map, spatial, scale, location, city, urban, road"],"frex":["map, location, scale, road, city, spatial, urban"],"lift":["road, map, location, city, spatial, scale, urban"],"score":["road, map, spatial, urban, city, location, scale"],"proportion":["0.01"]}],"topic_no":[18,65]},{"name":["investment, portfolio, asset, allocation, capital, return, investor"],"size":[1800],"topic_no":[19],"thought_1":["This paper presents an incentive scheme to encourage investment in the improvement and expansion of the transmission in the competitive electricity market environment. To create these incentives, a decentralized transmission asset investment model is proposed, where the new assets are built by the investors. The incentives are based on the value added to the social welfare through each asset investment. By viewing each potential investor as a player in a cooperative game the Shapley value is used to reward investors according to the added value that they create. The proposed methodology is applied to the Garver 6-bus system and the IEEE 24-bus Reliability Test System to illustrate the capability and flexibility of the decision support system presented. "],"thought_2":["The Hierarchical risk parity (HRP) approach of portfolio allocation, introduced by Lopez de Prado (2016), applies graph theory and machine learning to build a diversified portfolio. Like the traditional risk-based allocation methods, HRP is also a function of the estimate of the covariance matrix, however, it does not require its invertibility. In this paper, we first study the impact of covariance misspecification on the performance of the different allocation methods. Next, we study under an appropriate covariance forecast model whether the machine learning based HRP outperforms the traditional risk-based portfolios. For our analysis, we use the test for superior predictive ability on out-of-sample portfolio performance, to determine whether the observed excess performance is significant or if it occurred by chance. We find that when the covariance estimates are crude, inverse volatility weighted portfolios are more robust, followed by the machine learning-based portfolios. Minimum variance and maximum diversification are most sensitive to covariance misspecification. HRP follows the middle ground; it is less sensitive to covariance misspecification when compared with minimum variance or maximum diversification portfolio, while it is not as robust as the inverse volatility weighed portfolio. We also study the impact of the different rebalancing horizon and how the portfolios compare against a market-capitalization weighted portfolio. "],"prob":["investment, return, portfolio, allocation, asset, capital, investor"],"frex":["investment, portfolio, asset, allocation, capital, return, investor"],"lift":["portfolio, asset, investment, capital, allocation, return, maximize"],"score":["portfolio, investment, return, asset, investor, capital, allocation"],"proportion":["0.01"]},{"name":["index, price, stock, exchange, strategy, market, trade"],"children":[{"name":["stock, exchange, index, indices, movement, investor, daily"],"size":[1800],"topic_no":[32],"thought_1":["Predicting intraday stock jumps is a significant but challenging problem in finance. Due to the instantaneity and imperceptibility characteristics of intraday stock jumps, relevant studies on their predictability remain limited. This paper proposes a data-driven approach to predict intraday stock jumps using the information embedded in liquidity measures and technical indicators. Specifically, a trading day is divided into a series of 5-min intervals, and at the end of each interval, the candidate attributes defined by liquidity measures and technical indicators are input into machine learning algorithms to predict the arrival of a stock jump as well as its direction in the following 5-min interval. An empirical study is conducted using level-2 high-frequency data of 1271 stocks on the Shenzhen Stock Exchange of China to validate our approach. The results provide initial evidence of the predictability of jump arrivals and jump directions using level-2 stock data as well as the effectiveness of using a combination of liquidity measures and technical indicators for such prediction. We also reveal the superiority of using random forest compared with other machine learning algorithms in building prediction models. Importantly, our study provides a portable data-driven approach that exploits liquidity and technical information from level-2 stock data to predict intraday price jumps of individual stocks. "],"thought_2":["The world has become data driven, which highly accentuated the utilization of information technology. The movements of stock markets are influenced, by both the micro as well as macro economic variables including the legal framework and taxation policies of the respective economies. The crux of the issue lies in exactly forecasting the future stock price movements of individual firms and stock indices, based on historical past prices. The accuracy, in forecasting the market trend, has become difficult due to the prevalence of stochastic behaviour and volatility in the stock prices and index movements. This paper analyses the nonlinear movement pattern of the most volatile, top three stocks in terms of market capitalization, listed in the Bombay Stock Exchange (BSE) in India, namely Reliance Industries Limited (RIL), Tata Consultancy Services (TCS) Limited and HDFC Bank Limited, using the Artificial Neural Network (ANN) for the study period from 2008 to 2017. The findings of the study would help the investors, to make rational, well informed investment decisions, to optimize the stock returns by investing in the most valuable stocks. "],"prob":["stock, index, exchange, indices, predict, price, market"],"frex":["stock, exchange, index, indices, movement, investor, daily"],"lift":["stock, index, exchange, indices, movement, investor, daily"],"score":["stock, index, exchange, indices, price, investor, market"],"proportion":["0.01"]},{"name":["strategy, market, trade, price, option, profit, transaction"],"size":[1800],"topic_no":[56],"thought_1":["Automated traders operate market shares without human intervention. We propose a Trading Team based on atomic traders with opportunity detectors and simple effectors. The detectors signalize trading opportunities. For each trading signal, the effectors follow deterministic rules on when and what to trade in the market. The detectors are based on Partial Least Squares. We perform some trading experiments with twelve BM&FBovespa stocks. The empirical findings indicate that the proposed trading strategy reaches a 77.26 % annualized profit, outperforming by 380.07 % the chosen baseline strategy with a 16.07 % profit. We also investigate Multistock Resolution Strategy (MSR) performance subject to brokerage commissions and income tax. Whenever the initial investment is at least US$ 50, 000, the MSR strategy provides a profit of at least 38.63 %. "],"thought_2":["Futures markets have seen a phenomenal success since their inception both in developed and developing countries during the last four decades. This success is attributable to the tremendous leverage the futures provide to market participants. This study contributes to the literature by analyzing a trading strategy which benefits from this leverage by using the Capital Asset Pricing Model (CAPM) and cost-of-carry relationship. We apply the technical trading rules developed from spot market prices, on futures market prices using a CAPM based hedge ratio. Historical daily prices of twenty stocks from each of the ten markets (five developed markets and five emerging markets) are used for the analysis. Popular technical indicators, along with artificial intelligence techniques like Neural Networks and Genetic Algorithms, are used to generate buy and sell signals for each stock and for portfolios of stocks. The performance of the trading strategies is then calculated and compared. The results show that, although equal amounts invested in both spot and futures markets, the profit from the strategies applied on futures is considerably higher than that from the spot market in both developed and emerging markets. Moreover, the overall performance of the artificial intelligence strategies is far better than the traditional ones. "],"prob":["market, strategy, price, trade, option, profit, change"],"frex":["strategy, market, trade, price, option, profit, transaction"],"lift":["trade, strategy, market, price, option, profit, compete"],"score":["trade, market, price, strategy, option, profit, transaction"],"proportion":["0.01"]}],"topic_no":[32,56]},{"name":["growth, emission, factor, china, consumption, gas, affect"],"size":[1800],"topic_no":[21],"thought_1":["Carbon emissions in China have attracted increasing world attention with rapid urbanization of this country. It is critical for the government to identify the key factors causing these emissions and take controlling measures. Consistent results have not been achieved yet although some research has been conducted on the factors leading to emissions. Meanwhile, there is still considerable room to improve the methods of previous research. Index decomposition analysis (IDA) is the main method for quantifying the impact of different factors on carbon emissions. At present, the widely used forms of IDA are primarily the Laspeyres and the Divisia index methods. Compared with the Laspeyres and the majority of the Divisia index methods, the generalized Fisher index (GFI) decomposition method can eliminate the residuals and has better factor decomposition characteristics. This paper chooses Beijing as a typical example and analyzes the factors causing carbon emissions. Based on the extended Kaya identity, we built a multivariate generalized Fisher index decomposition model to measure the impacts of economic growth, population size, energy intensity and energy structure on energy-related carbon emissions from 1995 to 2012 in Beijing. The results show that the sustained growth of economic output in Beijing was the leading factor in carbon emissions. Population size had a stimulating effect on the growth of carbon emissions during this period; the pulling effect increased after 2003 and then decreased slightly after 2011 with a cumulative effect of 165.4%. Energy intensity was the primary factor restraining carbon emissions, and the inhibition effect increased yearly. The continuous optimization of the energy structure had no obvious inhibitory effect on carbon emissions. To control carbon emissions, Beijing should continue to adjust the mode of economic development and appropriately control the population size while improving energy efficiency. "],"thought_2":["With the development of China's economy, the use of fossil energy has become more and more, resulting in increasing carbon emissions. CO2 emissions have caused global warming, threatening humans and creatures on Earth. In order to effectively suppress the growth of carbon emissions, it is necessary to analyze the influencing factors of carbon emissions and apply them to predict carbon emissions. This paper presents sixteen potential influencing factors and uses grey relational analysis to identify the factors that have a strong correlation with carbon emissions. The principal component analysis (PCA) is used to extract the four principal components, which reduce the redundancy of the input data. The long short-term memory (LSTM) method is established to predict carbon emissions in China. We use back propagation neural network (BPNN) and Gaussian process regression (GPR) to compare LSTM method. The simulation results show that the prediction accuracy of carbon emissions based on LSTM is better than that of BPNN and GPR, indicating the effectiveness of PCA and LSTM in prediction of carbon emissions. Finally, this paper provides the theoretical basis for China to reduce carbon emissions by studying prediction of carbon emissions. "],"prob":["factor, growth, emission, consumption, china, influence, affect"],"frex":["growth, emission, factor, china, consumption, gas, affect"],"lift":["emission, growth, china, factor, consumption, gas, percentage"],"score":["emission, factor, growth, consumption, china, gas, influence"],"proportion":["0.01"]},{"name":["country, impact, economic, industry, development, environmental, sector"],"children":[{"name":["country, industry, tourism, sector, tourist, analyse, government"],"size":[1800],"topic_no":[17],"thought_1":["The aim of this paper is to analyse the state of the art of the Spanish rural tourism sector, as well as performing forecasts for this strategically important sector of Spanish economy. Section 1 of the paper describes rural tourism in Spain, while in Section 2 three time series belonging to this sector are analysed, and then forecasts are calculated by applying Box-Jenkins and Artificial Neural Nets methodologies. Finally, the paper summarises major conclusions and implications for policy makers and managers involved in rural tourism in Spain. "],"thought_2":["As an industry, tourism tends to be extremely responsive and vulnerable to political instabilities. Recently, a political conflict occurred in Spain, a leader in international tourism. In October 2017, the regional parliament of Catalonia asserted its independence from Spain, engendering a negative impact on the tourism sector of Catalonia. The main goal of our study is to assess the economic impact of the Catalan separatist challenge on the region’s tourism sector during the last quarter of 2017. To this end, we conducted a counterfactual analysis, based on forecasts generated by a seasonal autoregressive moving average model and an artificial neural network. The forecasts allowed us to calculate the projected number of international and domestic tourist visitors that would have travelled to Catalonia, had the separatist challenge not occurred. According to our results, the Catalan tourist sector effectively forfeited close to €200 million in revenue from the international tourism market, and around €27 million in revenue from the domestic market. These amounts differ from the economic gains attained by the other Spanish Mediterranean regions that compete with Catalonia to attract tourists. "],"prob":["industry, country, sector, tourism, analyse, develop, tourist"],"frex":["country, industry, tourism, sector, tourist, analyse, government"],"lift":["tourism, tourist, country, industry, sector, government, analyse"],"score":["tourism, industry, country, tourist, sector, government, travel"],"proportion":["0.01"]},{"name":["enterprise, environmental, economic, sustainability, life, indicator, sustainable"],"size":[1800],"topic_no":[48],"thought_1":["Life Cycle Assessment (LCA) methodology was used to estimate the environmental impacts and identify the most critical stages (hotspots) of cultivation of three cereal crops typically used for animal feed purposes – barley, rye and sorghum – in the Lombardy region, the most productive crop and livestock area in Northern Italy. The crop variety (out of 3 and 4 varieties of barley and rye, respectively) and cultivation regime (single vs. double cropping of sorghum) with the lowest impacts per kg of crude protein (mass-based functional unit) were identified. Environmental impact categories reported by ReCiPe method were used. According to the results, both Reni and Dank Nowe were the varieties with the lowest environmental impacts for barley and rye varieties, respectively; single cropping of sorghum had lower impacts than double cropping. Impact hotspots included field emissions, agricultural activities and agrochemical (fertilisers and herbicides) production regardless the cropping system considered. Moreover, among the cereals studies, rye was identified as the best environmental alternative. Use of land-based and economic functional units did not change the ranking of systems according to their impacts. "],"thought_2":["Sustainability of food consumption requires the understanding of multi-dimensional environmental, economic and social impacts using a holistic and integrated sustainability assessment and modeling framework. This article presents a novel method on the assessment and modeling of sustainability impacts of food consumption. First, sustainability impacts of food consumption categories are quantified using high sector resolution input-output tables of U.S. economy. Later, an integrated sustainability modeling framework based on two supervised machine-learning techniques such as k-means clustering and logistics regression is presented. The proposed framework involves five steps: (1) economic input-output life cycle sustainability assessment, (2) non-dimensional normalization, (3) sustainability performance evaluation, (4) centroid-based clustering analysis, and (5) sustainability impact modeling. The findings show that the supply chains of food production sectors are accounted for major environmental impacts with higher than 80% of portions for total carbon footprints. Animal slaughtering, rendering, and processing is found as the most dominant sector in most of the environmental impact categories. The logistic model results revealed an overall model accuracy equal to 91.67%. Furthermore, among all the environmental sustainability indicators, it has found that CO and SO2 are the most significant contributors. The results also show that 13.7% of the food and beverage sectors are clustered as high, in which the bread and bakery product manufacturing is the central sector. The large value of the variance (5.24) is attributed to the large total weighted impact value of the animal (except poultry) slaughtering, rendering, and processing cluster. "],"prob":["economic, environmental, impact, enterprise, indicator, development, life"],"frex":["enterprise, environmental, economic, sustainability, life, indicator, sustainable"],"lift":["sustainability, enterprise, cycle, sustainable, environmental, life, indicator"],"score":["sustainability, environmental, economic, enterprise, sustainable, indicator, cycle"],"proportion":["0.01"]}],"topic_no":[17,48]}],"topic_no":[1,15,4,42,24,51,29,44,55,66,30,47,57,5,33,58,37,45,39,46,27,54,64,9,22,40,52,12,67,23,18,65,19,32,56,21,17,48]},{"name":["problem, learn, algorithm, forecast, data, neural_network, feature"],"children":[{"name":["information, data, feature, network, classification, train, learn"],"children":[{"name":["learn, network, reinforcement, hide, node, lie, transfer"],"children":[{"name":["learn, reinforcement, learn_algorithm, transfer, extreme, active, generalization"],"size":[1800],"topic_no":[14],"thought_1":["Active learning is an effective methodology to relieve the tedious and expensive work of manual annotation for many supervised learning applications. The active learning framework with good performance usually contains powerful learning models and delicate active learning strategies. Gaussian process (GP)-based active learning was proposed to be one of the most effective methods. However, the single GP suffers from the limitation of not modeling multimodal data well enough, and thus existing active learning strategies based on GPs only make use of limited information from data. In this paper, we propose three novel active learning methods, in which the existing mixture of GP model (MGP) is adjusted as the learning model and three active learning strategies are designed based on the adjusted MGP. Through experiments on multiple data sets, we analyze the performance and characteristics of the three proposed active learning methods, and further compare with popular GP-based methods and some other state-of-the-art methods. "],"thought_2":["This paper provides an improved parallel data processing in Big Data mining using ClowdFlows platform. The big data processing involves an improvement in Proportional Integral Derivative (PID) controller using Reinforcement Adaptive Learning (RAL). The Reinforcement Adaptive Learning involves the use of Actor-critic State-action-reward-state-action (SARSA) learning that suits well the stream mining module of ClowdFlows platform. The study concentrates on batch mode processing in Big Data mining model with the use of proposed PID-SARSA-RAL. The experimental evaluation with the conventional ClowdFlows platform proved the effectiveness of the proposed method over continuous parallel workflow execution. "],"prob":["learn, transfer, reinforcement, learn_algorithm, extreme, active, experiment"],"frex":["learn, reinforcement, learn_algorithm, transfer, extreme, active, generalization"],"lift":["reinforcement, learn, learn_algorithm, extreme, transfer, active, generalization"],"score":["reinforcement, learn, learn_algorithm, transfer, extreme, generalization, active"],"proportion":["0.01"]},{"name":["network, node, lie, hide, neuron, lay, connection"],"size":[1800],"topic_no":[34],"thought_1":["Pulse coupled neural networks (PCNN, for short) are models abstracting the synchronization behavior observed experimentally for the cortical neurons in the visual cortex of a cat's brain, and the intersecting cortical model is a simplified version of the PCNN model. Membrane computing (MC) is a kind computation paradigm abstracted from the structure and functioning of biological cells that provide models working in cell-like mode, neural-like mode and tissue-like mode. Inspired from intersecting cortical model, this paper proposes a new kind of neural-like P systems, called dynamic threshold neural P systems (for short, DTNP systems). DTNP systems can be represented as a directed graph, where nodes are dynamic threshold neurons while arcs denote synaptic connections of these neurons. DTNP systems provide a kind of parallel computing models, they have two data units (feeding input unit and dynamic threshold unit) and the neuron firing mechanism is implemented by using a dynamic threshold mechanism. The Turing universality of DTNP systems as number accepting/generating devices is established. In addition, an universal DTNP system having 109 neurons for computing functions is constructed. "],"thought_2":["The purpose of this paper is to design an efficient recurrent neural network (RNN)-based speech recognition system using software with long short-term memory (LSTM). The design process involves speech acquisition, pre-processing, feature extraction, training and pattern recognition tasks for a spoken sentence recognition system using LSTM-RNN. There are five layers namely, an input layer, a fully connected layer, a hidden LSTM layer, SoftMax layer and a sequential output layer. A vocabulary of 80 words which constitute 20 sentences is used. The depth of the layer is chosen as 20, 42 and 60 and the accuracy of each system is determined. The results reveal that the maximum accuracy of 89% is achieved when the depth of the hidden layer is 42. Since the depth of the hidden layer is fixed for a task, increased performance can be achieved by increasing the number of hidden layers. Copyright "],"prob":["network, lie, node, hide, lay, neuron, link"],"frex":["network, node, lie, hide, neuron, lay, connection"],"lift":["node, network, hide, lie, neuron, connection, lay"],"score":["node, network, lie, hide, neuron, lay, architecture"],"proportion":["0.01"]}],"topic_no":[14,34]},{"name":["complexity, data_set, function, distance, probability, cluster, distribution"],"children":[{"name":["cluster, distance, data_set, matrix, unsupervised, phase, point"],"size":[1800],"topic_no":[20],"thought_1":["Clustering by identifying cluster centers is important for detecting patterns in a data set. However, many center-based clustering algorithms cannot process data sets containing non-spherical clusters. In this paper, we propose a novel clustering algorithm called NaNLORE based on natural neighbor and local representatives. Natural neighbor is a new neighbor concept and introduced to compute local density and find local representatives which are points with local maximum density. We first find local representatives and then select cluster centers from the local representatives. The density-adaptive distance is introduced to measure the distance between local representatives, which helps to solve the problem of clustering data sets with complex manifold structure. Cluster centers are characterized by higher density than their neighbors and a relatively large density-adaptive distance from any local representatives with higher density. In experiments, we compare the proposed algorithm NaNLORE with existing algorithms on synthetic and real data sets. Results show that NaNLORE performs better than existing algorithm, especially on clustering non-spherical data and manifold data. "],"thought_2":["Affinity propagation (AP) is introduced as an unsupervised learning algorithm for exemplar-based clustering. A few methods are stated to extend the AP model to account for semi-supervised clustering. In this paper, constraint (cannot-link and must-link) projections are illustrated for semi-supervised AP (CPSSAP), a hierarchical semi-supervised clustering algorithm. It is flexible for the relaxation of some constraints during the learning stage. First, the data points of instance-level constraints and other data points are together projected in a lower dimensional space guided by the constraints. Then, AP is performed on the new data points in the lower dimensional space. Finally, a few datasets are chosen for experimentation from the UCI machine learning repository. The results show that CPSSAP performs better than some existing algorithms. Furthermore, visualizations of the original data and data after the projections show that the data points overlap less after the constraint projections of the datasets. "],"prob":["cluster, data_set, distance, matrix, point, phase, algorithm"],"frex":["cluster, distance, data_set, matrix, unsupervised, phase, point"],"lift":["cluster, unsupervised, distance, matrix, center, data_set, segment"],"score":["cluster, unsupervised, distance, data_set, matrix, segment, algorithm"],"proportion":["0.01"]},{"name":["function, utility, probability, complexity, distribution, approximate, mine"],"size":[1800],"topic_no":[60],"thought_1":["High-utility itemset mining is used to obtain high utility itemsets by taking into account both the quantity as well as the utility of each item, which have not been considered in frequent itemset mining. Many algorithms compute high utility itemsets by setting a minimum utility threshold in advance. However, determining the minimum utility threshold is not easy. Too high or too low a threshold may result in incorrect high utility itemsets. In this paper, we propose a method based on binary particle swarm optimization to optimize the search for high utility itemsets without setting the minimum utility threshold beforehand. Instead, the application of the minimum utility threshold is performed as a post-processing step. Experiments on five datasets indicate that the proposed method is better than existing methods in finding high utility itemsets, and the time to obtain those itemsets is faster than that with setting the minimum utility threshold first. "],"thought_2":["Top-k high utility itemset mining is the process of discovering the k itemsets having the highest utilities in a transactional database. In recent years, several algorithms have been proposed for this task. However, it remains very expensive both in terms of runtime and memory consumption. The reason is that current algorithms often generate a huge amount of candidate itemsets and are unable to prune the search space effectively. In this paper, we address this issue by proposing a novel algorithm named kHMC to discover the top-k high utility itemsets more efficiently. Unlike several algorithms for top-k high utility itemset mining, kHMC discovers high utility itemsets using a single phase. Furthermore, it employs three strategies named RIU, CUD, and COV to raise its internal minimum utility threshold effectively, and thus reduce the search space. The COV strategy introduces a novel concept of coverage. The concept of coverage can be employed to prune the search space in high utility itemset mining, or to raise the threshold in top-k high utility itemset mining, as proposed in this paper. Furthermore, kHMC relies on a novel co-occurrence pruning technique named EUCPT to avoid performing costly join operations for calculating the utilities of itemsets. Moreover, a novel pruning strategy named TEP is proposed for reducing the search space. To evaluate the performance of the proposed algorithm, extensive experiments have been conducted on six datasets having various characteristics. Results show that the proposed algorithm outperforms the state-of-the-art TKO and REPT algorithms for top-k high utility itemset mining both in terms of memory consumption and runtime. "],"prob":["function, distribution, complexity, probability, utility, mine, bayesian"],"frex":["function, utility, probability, complexity, distribution, approximate, mine"],"lift":["utility, function, probability, approximate, complexity, distribution, bayesian"],"score":["utility, function, distribution, probability, mine, complexity, bayesian"],"proportion":["0.01"]}],"topic_no":[20,60]},{"name":["feature, feature_selection, extract, subset, extraction, reduction, datasets"],"size":[1800],"topic_no":[28],"thought_1":["Feature selection aims at finding a feature subset that has the most discriminative information from the original feature set. In this paper, we firstly present a new scheme for feature relevance, interdependence and redundancy analysis using information theoretic criteria. Then, a dynamic weighting-based feature selection algorithm is proposed, which not only selects the most relevant features and eliminates redundant features, but also tries to retain useful intrinsic groups of interdependent features. The primary characteristic of the method is that the feature is weighted according to its interaction with the selected features. And the weight of features will be dynamically updated after each candidate feature has been selected. To verify the effectiveness of our method, experimental comparisons on six UCI data sets and four gene microarray datasets are carried out using three typical classifiers. The results indicate that our proposed method achieves promising improvement on feature selection and classification accuracy. "],"thought_2":["A central problem in machine learning and pattern recognition is the process of recognizing the most important features. In this paper, we provide a new feature selection method (DRPT) that consists of first removing the irrelevant features and then detecting correlations between the remaining features. Let D=[A∣b] be a dataset, where b is the class label and A is a matrix whose columns are the features. We solve Ax=b using the least squares method and the pseudo-inverse of A. Each component of x can be viewed as an assigned weight to the corresponding column (feature). We define a threshold based on the local maxima of x and remove those features whose weights are smaller than the threshold. To detect the correlations in the reduced matrix, which we still call A, we consider a perturbation Ã of A. We prove that correlations are encoded in Δx=∣x−x̃∣, where x̃ is the least squares solution of Ãx̃=b. We cluster features first based on Δx and then using the entropy of features. Finally, a feature is selected from each sub-cluster based on its weight and entropy. The effectiveness of DRPT has been verified by performing a series of comparisons with seven state-of-the-art feature selection methods over ten genetic datasets ranging up from 9,117 to 267,604 features. The results show that, over all, the performance of DRPT is favorable in several aspects compared to each feature selection algorithm. "],"prob":["feature, extract, feature_selection, extraction, subset, reduction, performance"],"frex":["feature, feature_selection, extract, subset, extraction, reduction, datasets"],"lift":["feature_selection, subset, feature, extraction, extract, reduction, robustness"],"score":["feature_selection, feature, subset, extraction, extract, reduction, datasets"],"proportion":["0.02"]},{"name":["sample, train, data, classification, accuracy, vector, classifier"],"children":[{"name":["classifier, classification, class, vector, support_vector_machine, imbalance, accuracy"],"size":[1800],"topic_no":[10],"thought_1":["Contrast pattern-based classifiers are an important family of both understandable and accurate classifiers. Nevertheless, these classifiers do not achieve good performance on class imbalance problems. In this paper, we introduce a new contrast pattern-based classifier for class imbalance problems. Our proposal for solving the class imbalance problem combines the support of the patterns with the class imbalance level at the classification stage of the classifier. From our experimental results, using highly imbalanced databases, we can conclude that our proposed classifier significantly outperforms the current contrast pattern-based classifiers designed for class imbalance problems. Additionally, we show that our classifier significantly outperforms other state-of-the-art classifiers not directly based on contrast patterns, which are also designed to deal with class imbalance problems. "],"thought_2":["In Machine Learning, a data set is imbalanced when the class proportions are highly skewed. Imbalanced data sets arise routinely in many application domains and pose a challenge to traditional classifiers. We propose a new approach to building ensembles of classifiers for two-class imbalanced data sets, called Random Balance. Each member of the Random Balance ensemble is trained with data sampled from the training set and augmented by artificial instances obtained using SMOTE. The novelty in the approach is that the proportions of the classes for each ensemble member are chosen randomly. The intuition behind the method is that the proposed diversity heuristic will ensure that the ensemble contains classifiers that are specialized for different operating points on the ROC space, thereby leading to larger AUC compared to other ensembles of classifiers. Experiments have been carried out to test the Random Balance approach by itself, and also in combination with standard ensemble methods. As a result, we propose a new ensemble creation method called RB-Boost which combines Random Balance with AdaBoost.M2. This combination involves enforcing random class proportions in addition to instance re-weighting. Experiments with 86 imbalanced data sets from two well known repositories demonstrate the advantage of the Random Balance approach. "],"prob":["classification, classifier, accuracy, vector, class, support, machine"],"frex":["classifier, classification, class, vector, support_vector_machine, imbalance, accuracy"],"lift":["imbalance, class, support_vector_machine, classifier, classification, binary, kernel"],"score":["imbalance, classification, classifier, vector, class, support_vector_machine, ensemble"],"proportion":["0.03"]},{"name":["sample, train, label, target, noise, data, datasets"],"size":[1800],"topic_no":[50],"thought_1":["Semi-supervised self-labeled methods apply unlabeled data to improve the performance of classifiers which are trained by labeled data alone. Nevertheless, applying unlabeled data may deteriorate the prediction accuracy. One of the causes is that there are insufficient labeled data for training an initial classifier in self-labeled methods. However, existing solutions for this problem of lacking sufficient initial labeled data still have technical defects. For example, they fail to deal with non-spherical data and improve insufficient initial labeled data effectively, when initial labeled data are extremely scarce. In this paper, we propose an effective semi-supervised self-labeled framework based on local cores, aiming to solve the problem of lacking adequate initial labeled data in self-labeled methods and overcome existing technical defects above. Main ideas of our framework include two sides: (a) inadequate initial labeled data are improved by adding predicted local cores to them, where local cores are predicted by active labeling or co-labeling; (b) we use any semi-supervised self-labeled method to train a given classifier on improved labeled data and updated unlabeled data. In our framework, local cores roughly reveal the data distribution, which helps the proposed framework work on spherical or non-spherical data sets. In addition, local cores also help our framework improve insufficient initial labeled data effectively, even when initial labeled data are extremely scarce. Experiments show that the proposed framework is compatible with tested self-labeled methods, and can help self-labeled methods train a k nearest neighbor or support vector machine, when initial labeled data are insufficient. "],"thought_2":["Current unsupervised domain adaptation (UDA) methods based on GAN (Generative Adversarial Network) architectures assume that source samples arise from a single distribution. These methods have shown compelling results by finding the transformation between source and target domains to reduce the distribution divergence. However, the one-to-one assumption renders the existing GAN-based UDA methods ineffective in a more realistic scenario that source samples are typically collected from diverse sources. In this paper, we present a novel GAN-enabled framework, which we call Multi-Source Adaptation Network (MSAN), for multiple-source domain adaptation (MDA) to mitigate the domain shifts between multiple source domains and the target domain. The proposed framework consists of multiple GAN architectures to learn bidirectional transformations between the source domains and the target domain efficiently and simultaneously. Technically, we introduce a joint feature space to guide the multi-level consistency constraints across all the transformations, in order to preserve the domain-invariant pattern and endow the discriminative power for the unlabeled target samples simultaneously during the adaptation. Moreover, the proposed model can naturally be used to enlarge the target dataset by utilizing the synthetic target images (with ground-truth labels from different source domains) and the pseudo-labeled target images, thereby allowing constructing the target-specific classifier in an unsupervised manner. Experiments demonstrate that our models exceed state-of-the-art results for MDA tasks on several benchmark datasets. "],"prob":["data, train, sample, label, target, datasets, domain"],"frex":["sample, train, label, target, noise, data, datasets"],"lift":["label, sample, noise, train, target, adaptation, instance"],"score":["label, train, data, sample, noise, datasets, target"],"proportion":["0.02"]}],"topic_no":[10,50]},{"name":["signal, fault, identification, diagnosis, monitor, condition, frequency"],"size":[1800],"topic_no":[6],"thought_1":["Rolling element bearings faults are one of the main causes of breakdown of rotating machines. Aside from this, due to variation of operating condition, domain shift phenomenon results in important detection performance deterioration. Therefore, cross-domain intelligent fault detection and diagnosis of bearings is very critical for the reliable operation. In this paper, a new intelligent fault diagnosis approach based on tensor-aligned invariant subspace learning and two-dimensional convolutional neural networks (TAISL–2DCNN) is proposed for cross-domain intelligent fault diagnosis of bearings. The vibration signals of bearings fault are first formulated as a third-order tensor via trial, condition and channel. For adapting the source domain and the target domain tensor representations directly, without vectorization, the domain adaptation (DA) approach named TAISL is first proposed for tensor representation in bearing intelligent fault diagnosis field. Then the 2DCNN is utilized to recognize different faults. The performance of the presented algorithm has been thoroughly evaluated through extensive cross-domain fault diagnosis experiments. The verification results confirm that the developed approach is able to reliably and accurately identify different fault categories and severities of bearings when testing and training data are drawn from different distribution. "],"thought_2":["Purpose: The purpose of this paper is to study the fault diagnosis of internal combustion (IC) engine gearbox using vibration signals with signal processing and machine learning (ML) techniques. Design/methodology/approach: Vibration signals from the gearbox are acquired for healthy and induced faulty conditions of the gear. In this study, 50% tooth fault and 100% tooth fault are chosen as gear faults in the driver gear. The acquired signals are processed and analyzed using signal processing and ML techniques. Findings: The obtained results show that variation in the amplitude of the crankshaft rotational frequency (CRF) and gear mesh frequency (GMF) for different conditions of the gearbox with various load conditions. ML techniques were also employed in developing the fault diagnosis system using statistical features. J48 decision tree provides better classification accuracy about 85.1852% in identifying gearbox conditions. Practical implications: The proposed approach can be used effectively for fault diagnosis of IC engine gearbox. Spectrum and continuous wavelet transform (CWT) provide better information about gear fault conditions using time–frequency characteristics. Originality/value: In this paper, experiments are conducted on real-time running condition of IC engine gearbox while considering combustion. Eddy current dynamometer is attached to output shaft of the engine for applying load. Spectrum, cepstrum, short-time Fourier transform (STFT) and wavelet analysis are performed. Spectrum, cepstrum and CWT provide better information about gear fault conditions using time–frequency characteristics. ML techniques were used in analyzing classification accuracy of the experimental data to detect the gearbox conditions using various classifiers. Hence, these techniques can be used for detection of faults in the IC engine gearbox and other reciprocating/rotating machineries. "],"prob":["signal, monitor, identification, condition, fault, diagnosis, frequency"],"frex":["signal, fault, identification, diagnosis, monitor, condition, frequency"],"lift":["fault, signal, diagnosis, normal, identification, frequency, monitor"],"score":["fault, signal, diagnosis, monitor, identification, condition, frequency"],"proportion":["0.01"]},{"name":["object, deep_learn, computer, deep, detection, image, detect"],"children":[{"name":["object, detection, track, video, detect, computer, vision"],"size":[1800],"topic_no":[8],"thought_1":["Object and human tracking in streaming videos are one of the most challenging problems in vision computing. In this article, we review some relevant machine learning algorithms and techniques for human identification and tracking in videos. We provide details on metrics and methods used in the computer vision literature for monitoring and propose a state-space representation of the object tracking problem. A proof of concept implementation of the state-space based object tracking using particle filters is presented as well. The proposed approach enables tracking objects/humans in a video, including foreground/background separation for object movement detection. Copyright "],"thought_2":["The rapid expansion of Asian hornets poses a high threat for the honey bee survival, as these invaders pray on them. Furthermore, they also pose a threat to people who are allergic, whose sting can lead to death. This study proposes a Decision Support System that uses Computer Vision techniques to automatically detect signs of Vespa velutina through images from GPS equipped camera. The goal of the system is to provide timely information about the presence of these invaders, allowing park managers and beekeepers to act quickly in removing the Vespidae. The proposed methodology obtained an 85% accuracy in the detection of V. velutina using the Mask RCNN architecture, enabling the system to perform detection at 3 FPS. "],"prob":["detection, detect, object, computer, track, video, vision"],"frex":["object, detection, track, video, detect, computer, vision"],"lift":["video, track, object, vision, detection, detect, computer"],"score":["video, detection, object, detect, vision, track, computer"],"proportion":["0.01"]},{"name":["image, deep_learn, deep, convolutional, convolutional_neural_network, visual, region"],"size":[1800],"topic_no":[13],"thought_1":["We present findings on classifying the class of executable code using convolutional, recurrent neural networks by creating images from only the.text section of executables and dividing them into standard-size windows, using minimal preprocessing. We achieve up to 98.24% testing accuracy on classifying 9 types of malware, and 99.50% testing accuracy on classifying malicious vs. benign code. Then, we find that a recurrent network may not entirely be necessary, opening the door for future neural network architectures. "],"thought_2":["Sign language is one of the languages which are used to communicate with deaf people. By using it, they can communicate and understand each other. In Indonesia, there are two standards of sign language which are SIBI (Sistem Bahasa Isyarat) and BISINDO (Bahasa Isyarat Indonesia). Deep learning is a model that is used to apply to this topic. In this model, there are a lot of methods such as convolutional neural network, recurrent neural network, long-sort term memory, and each model has its characteristics. There are also some issues in deep learning by sign language recognition as the object such as data training, object position, pose, lighting, and the background of objects. This research will describe how to combine background subtraction and gaussian blur pre-processing, forwarding preprocessing background subtraction with CNN by using BISINDO, LSTM, and a combination between CNN and LSTM. In conclusion, this research shows that a combination between CNN and LSTM is the best model by explaining the accuracy and testing with sign language BISINDO as the object. The accuracy showed that for CNN 96%, LSTM 86%, and combination CNN and LSTM 96%, and the loss showed that for CNN 18%, LSTM 41%, and combination CNN and LSTM 17%. "],"prob":["image, deep, deep_learn, neural_network, convolutional, visual, convolutional_neural_network"],"frex":["image, deep_learn, deep, convolutional, convolutional_neural_network, visual, region"],"lift":["image, convolutional_neural_network, convolutional, deep_learn, deep, visual, region"],"score":["image, convolutional, convolutional_neural_network, deep_learn, deep, visual, neural_network"],"proportion":["0.01"]}],"topic_no":[8,13]},{"name":["character, pattern, recognition, face, recognize, correct, difficulty"],"size":[1800],"topic_no":[41],"thought_1":["character(0)"],"thought_2":["character(0)"],"prob":["character, pattern, recognition, face, recognize, correct, difficulty"],"frex":["character, pattern, recognition, face, recognize, correct, difficulty"],"lift":["character, recognition, pattern, face, recognize, correct, difficulty"],"score":["character, recognition, pattern, face, recognize, correct, difficulty"],"proportion":["0.05"]},{"name":["document, information, word, language, representation, text, semantic"],"children":[{"name":["language, document, text, natural, word, textual, corpus"],"size":[1800],"topic_no":[36],"thought_1":["Sentence-based summarization aims at extracting concise summaries of collections of textual documents. Summaries consist of a worthwhile subset of document sentences. The most effective multilingual strategies rely on Latent Semantic Analysis (LSA) and on frequent itemset mining, respectively. LSA-based summarizers pick the document sentences that cover the most important concepts. Concepts are modeled as combinations of single-document terms and are derived from a term-by-sentence matrix by exploiting Singular Value Decomposition (SVD). Itemset-based summarizers pick the sentences that contain the largest number of frequent itemsets, which represent combinations of frequently co-occurring terms. The main drawbacks of existing approaches are (i) the inability of LSA to consider the correlation between combinations of multiple-document terms and the underlying concepts, (ii) the inherent redundancy of frequent itemsets because similar itemsets may be related to the same concept, and (iii) the inability of itemset-based summarizers to correlate itemsets with the underlying document concepts. To overcome the issues of both of the abovementioned algorithms, we propose a new summarization approach that exploits frequent itemsets to describe all of the latent concepts covered by the documents under analysis and LSA to reduce the potentially redundant set of itemsets to a compact set of uncorrelated concepts. The summarizer selects the sentences that cover the latent concepts with minimal redundancy. We tested the summarization algorithm on both multilingual and English-language benchmark document collections. The proposed approach performed significantly better than both itemset- and LSA-based summarizers, and better than most of the other state-of-the-art approaches. "],"thought_2":["Major portion of web pages contains the natural language text and understanding of natural language text from the web pages is a major challenge for machines. Due to this lacking search engines are not able to provide relevant information to the users. This problem is tackled by natural language processing techniques and the development of ontologies from natural language text. With the help of such ontologies search of information can increases manifold. Much research in the field of text processing and automatic ontology building from text has been done to address these challenges. The proposed method in this paper is another effort to build automatic ontology from domain specific text. In this method we first extract concepts from a given domain specific text. We have used a Stanford parser to parse the text and the dictionary of basic concepts is created manually containing all the domain specific concepts and their relationships by recognizing laxico-syntactic patterns in the text corpus. Once concepts and relations among concepts as well as properties of concepts are identified, the extracted information can be represented in the form of graph and OWL. "],"prob":["text, language, document, word, natural, information, semantic"],"frex":["language, document, text, natural, word, textual, corpus"],"lift":["document, language, corpus, text, natural, textual, unstructured"],"score":["document, text, language, word, semantic, natural, retrieval"],"proportion":["0.02"]},{"name":["representation, graph, relation, embed, entity, state-of-the-art, exploit"],"size":[1800],"topic_no":[53],"thought_1":["Entity Disambiguation (ED) aims to automatically resolve mentions of entities in a document to corresponding entries in a given knowledge base. State-of-the-art ED methods typically utilize local contextual information for obtaining mention embeddings which will be compared to candidate entity embeddings and then apply Conditional Random Field (CRF) for collective ED, considering global coherence. An inherent drawback of these methods is that, the global semantic relationships among the candidate entities in the same document are not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the global coherence effect. In this paper, to address the issue, we propose a novel end-to-end graph neural entity disambiguation model which fully exploits the global semantic information. In particular, a heterogeneous entity-word graph is first constructed for each document to model the global semantic relationships among candidate entities in a same document. Then graph convolutional network (GCN) is applied on the entity-word graph to generate enhanced entity embeddings encoding global semantics, which are fed to a CRF for collective ED. Extensive experiments have demonstrated the efficiency and effectiveness of our method over a few state-of-the-art ED methods. "],"thought_2":["Relation extraction is to identify the relationship of two given entities in the text. It is an important step in the task of knowledge extraction. Most conventional methods for the task of relation extraction focus on designing effective handcrafted features or learning a semantic representation of the whole sentence. Sentences with the same relationship always share the similar expressions. Besides, the semantic properties of given entities can also help to distinguish some confusing relations. Based on the above observations, we propose a neural network based framework for relation classification. It can simultaneously learn the relation pattern's information and the semantic properties of given entities. In this framework, we explore two specific models: the CNN-based model and LSTM-based model. We conduct experiments on two public datasets: the SemEval-2010 Task8 dataset and the ACE05 dataset. The proposed method achieves the state-of-the-art result without using any external information. Additionally, the experimental results also show that our approach can represent the semantic relationship of the given entities effectively. "],"prob":["information, representation, relation, graph, state-of-the-art, embed, task"],"frex":["representation, graph, relation, embed, entity, state-of-the-art, exploit"],"lift":["entity, graph, representation, embed, dependency, relation, state-of-the-art"],"score":["entity, representation, graph, semantic, information, relation, state-of-the-art"],"proportion":["0.02"]}],"topic_no":[36,53]},{"name":["match, database, web, search, user, recommendation, item"],"children":[{"name":["web, query, search, match, database, engine, relevance"],"size":[1800],"topic_no":[16],"thought_1":["Web search engines have become an integral part of the daily life of a knowledge worker, who depends on these search engines to retrieve relevant information from the Web or from the company's vast document databases. Current search engines are very fast in terms of their response time to a user query. But their usefulness to the user in terms of retrieval performance leaves a lot to be desired. Typically, the user has to sift through a lot of nonrelevant documents to get only a few relevant ones for the user's information needs. Ranking functions play a very important role in the search engine retrieval performance. In this paper, we describe a methodology using genetic programming to discover new ranking functions for the Web-based information-seeking task. We exploit the content as well as structural information in the Web documents in the discovery process. The discovery process is carried out for both the ad hoc task and the routing task in retrieval. For either of the retrieval tasks, the retrieval performance of these newly discovered ranking functions has been found to be superior to the performance obtained by well-known ranking strategies in the information retrieval literature. "],"thought_2":["The Semantic Web can have great influence on various domains of information. One of them is the domain of World News. Semantic Web technologies aim at providing the means to organize the vast amount of knowledge that is scattered in the Web, in a machine understandable way. Then, searching and data retrieval would be much easier. This would be particularly helpful in the World News domain. There is a big variety of news sources and it would be useful to provide an efficient method to automatically organize them. In this paper, we describe World News Finder, a system which performs semantic search on the World News domain. The system is based on metadata files created for every single World News HTML webpage in an automatic way. According to a user query, the system performs the search on these metadata files rather than keyword search. To achieve the above, we developed the World News Ontology and a large set of domain-specific heuristic rules. "],"prob":["search, database, web, match, query, engine, relevance"],"frex":["web, query, search, match, database, engine, relevance"],"lift":["query, web, engine, match, search, database, relevance"],"score":["query, search, web, engine, database, match, retrieval"],"proportion":["0.01"]},{"name":["recommendation, item, user, feedback, profile, filter, collaborative"],"size":[1800],"topic_no":[49],"thought_1":["As one of the collaborative filtering (CF) techniques, memory-based CF technique which recommends items to users based on rating information of like-minded users (called neighbors) has been widely used and has also proven to be useful in many practices in the age of information overload. However, there is still considerable room for improving the quality of recommendation. Shortly, similarity functions in traditional CF compute a similarity between a target user and the other user without considering a target item. More specifically, they give an equal weight to each of the co-rated items rated by both users. Neighbors of a target user, therefore, are identical for all target items. However, a reasonable assumption is that the similarity between a target item and each of the co-rated items should be considered when finding neighbors of a target user. Additionally, a different set of neighbors should be selected for each different target item. Thus, the objective of this paper is to propose a new similarity function in order to select different neighbors for each different target item. In the new similarity function, the rating of a user on an item is weighted by the item similarity between the item and the target item. Experimental results from MovieLens dataset and Netflix dataset provide evidence that our recommender model considerably outperforms the traditional CF-based recommender model. "],"thought_2":["Acollaborative filtering system recommends to users products that similar users like. Collaborative filtering systems influence purchase decisions and hence have become targets of manipulation by unscrupulous vendors. We demonstrate that nearest neighbors algorithms, which are widely used in commercial systems, are highly susceptible to manipulation and introduce new collaborative filtering algorithms that are relatively robust. "],"prob":["user, recommendation, item, filter, feedback, collaborative, profile"],"frex":["recommendation, item, user, feedback, profile, filter, collaborative"],"lift":["item, recommendation, profile, feedback, recommend, user, collaborative"],"score":["item, user, recommendation, filter, collaborative, feedback, profile"],"proportion":["0.01"]}],"topic_no":[16,49]}],"topic_no":[14,34,20,60,28,10,50,6,8,13,41,36,53,16,49]},{"name":["predict, prediction, algorithm, forecast, problem, neural_network, technique"],"children":[{"name":["level, control, vehicle, cost, demand, drive, supply_chain"],"children":[{"name":["demand, supply_chain, cost, inventory, capacity, operational, total"],"size":[1800],"topic_no":[25],"thought_1":["This paper first proposes the use of metaheuristic, to combine with exponential smoothing methods, in forecasting future demands and in determining the optimal inventory policy values for each node in a supply chain network based on historical demand or order streams without the need of any prior knowledge about the demand distribution or distribution fitting. The effects of five demand forecasting methods, two inventory policies, and three lead times on the total inventory cost of a 3-echelon serial supply chain system are then investigated. The effect of sharing the demand information for planning the inventories is also compared with that of no sharing. For testing, 15 quarterly and 15 monthly time series were taken from the M3 Competition and are considered as the multi-item demand streams to be fulfilled in the supply chain. The results indicate that: (1) the damped Pegel forecasting method is the best in terms of prediction errors because it outperforms others in three of five measures, followed by the simple exponential smoothing that wins one of the remaining two and ties the damped Pegel in one; (2) the supply chain inventory cost increases with increasing lead time and echelon level of the supply chain when the (s, S) policy is used, but not the (r, Q) policy; (3) the (r, Q) inventory policy generally incurs lower supply chain inventory cost than the (s, S) policy; (4) sharing demand information reduces inventory cost and the reduction is higher for (s, S) than for (r, Q); (5) the best demand forecasting method for minimizing inventory cost varies with the inventory policy used and lead time; and (6) the correlation between forecasting errors and inventory costs is either negligible or minimal. "],"thought_2":["An axiomatic approach to universal integrals based on level dependent capacities is presented. Based on a given semicopula, two corresponding extremal universal integrals based on level dependent capacities are given. If the underlying semicopula is even a copula, another class of universal integrals based on level dependent capacities is studied, generalizing, among others, both the Choquet and the Sugeno integral based on level dependent capacities which were introduced recently. "],"prob":["cost, demand, supply_chain, level, inventory, reduce, total"],"frex":["demand, supply_chain, cost, inventory, capacity, operational, total"],"lift":["inventory, supply_chain, demand, cost, supply, capacity, manufacturer"],"score":["inventory, cost, supply_chain, demand, supply, operational, capacity"],"proportion":["0.02"]},{"name":["vehicle, logistic, control, drive, driver, route, transportation"],"size":[1800],"topic_no":[43],"thought_1":["Considering that emergency logistics distribution has the timeliness, weak economy, and the traffic road condition and so on, based on reasonable assumptions, we effectively scheduled vehicles to maximise the demand for emergency logistics within the requested time. This paper proposes an emergency logistics vehicle scheduling model based on internet of things. Firstly, we built the model of emergency logistics distribution vehicle scheduling problem. Our research built a model of emergency logistics distribution vehicle scheduling by using the model of division time, and gave the interrelation of constraint condition. On this basis, this paper combined genetic algorithm and ant colony algorithm for the vehicle scheduling model design and parameter selection, which improved the efficiency of emergency logistics distribution. Simulation results show that the proposed model can effectively increase the transport efficiency and reduce transportation costs, and has strong practical value. It is suited for post-disaster emergency logistics distribution vehicle scheduling. Copyright "],"thought_2":["The modeling of car-following behavior is an attractive research topic in traffic simulation and intelligent transportation. The driver plays an important role in car following but is ignored by most car-following models. This paper presents a novel car-following driver model, which can retain aspects of human driving styles. First, simulated car-following data are generated by using the speed control driver model and the real-world driving behavior data if the real-world car-following data are not available. Then, the car-following driver model is established by imitating human driving maneuver during real-world car following. This is accomplished by using a neural network-based learning control paradigm and car-following data. Finally, the FTP-72 driving cycle is borrowed as the speed profile of the leading vehicle for the model test. The driving style is quantitatively analyzed by AESD. The results show that the proposed car-following driver model is capable of retaining the naturalistic driving styles while well accomplishing the car-following task with the error of relative distance mostly less than 5 meters for every driving styles. "],"prob":["control, vehicle, drive, logistic, transportation, speed, driver"],"frex":["vehicle, logistic, control, drive, driver, route, transportation"],"lift":["vehicle, logistic, route, driver, control, drive, transportation"],"score":["vehicle, control, logistic, transportation, drive, route, driver"],"proportion":["0.01"]}],"topic_no":[25,43]},{"name":["order, part, structure, space, case, operator, weight"],"size":[1800],"topic_no":[59],"thought_1":["The aim of this paper is to investigate an ordered multiplicative modular geometric operator and its relevant properties. The ordered multiplicative modular geometric operator is a generalized form of the ordered weighted geometric operator which has been designed incorporating the advantages of the geometric mean to deal with ratio judgments and the advantages of the ordered weighted averaging (OWA) operator to represent the concept of fuzzy majority in the process of information aggregation. Besides, the ordered multiplicative modular geometric operator can be seen as a symmetrized multiplicative modular aggregation function, characterized by comonotone multiplicative modularity. It is worth pointing that lots of the existing operators (such as the ordered weighted geometric operator, the weighted geometric operator, the ordered weighted maximum, and the Max and Min operators) can be regarded as the special cases of the ordered multiplicative modular geometric operator, which is of value in developing the theory of geometric operators. "],"thought_2":["The focus of this paper are distributivity equations involving the binary aggregation operators on the unit interval [0, 1] with either absorbing or neutral element from the open interval (0, 1), and the Mayor's aggregation operators from [28]. In the second part of this paper, problem is extended to aggregation operators that have neither neutral nor absorbing element. "],"prob":["order, structure, case, part, process, space, weight"],"frex":["order, part, structure, space, case, operator, weight"],"lift":["operator, part, functional, order, element, structure, space"],"score":["operator, structure, order, part, space, weight, case"],"proportion":["0.02"]},{"name":["production, optimization, solve, plan, solution, algorithm, problem"],"children":[{"name":["solve, problem, optimization, solution, algorithm, heuristic, swarm"],"size":[1800],"topic_no":[3],"thought_1":["In this work, an ant colony optimisation-based heuristic is developed to solve the location-routing problem. The proposed heuristic solves the allocation problem and the routing problem simultaneously. This simultaneous approach is the novelty of the solution methodology. The proposed heuristic is tested on a set of well-known benchmark problem instances and the results obtained are compared with that using the other heuristics reported in the literature. For larger size problem instances, the proposed heuristic provides better solutions. "],"thought_2":["Ant colony optimization (ACO) algorithms have been used successfully to solve a wide variety of combinatorial optimization problems. In the recent past many modifications have been proposed in ACO algorithms to solve continuous optimization problems. However, most of the ACO variants to solve continuous optimization problems lack ability of efficient exploration of the search space and suffer from the problem of premature convergence. In this work a new ACO algorithm (ACO–LD) is proposed that incorporates Laplace distribution based interaction scheme among the ants. Also, in order to avoid the problem of stagnation, an additional diversification mechanism is introduced. The proposed ACO–LD is tested on benchmark test functions taken from Congress on Evolutionary Computation 2014 (CEC2014) and the results are compared with four state-of-the-art algorithms reported in CEC2014. ACO–LD is also applied to solve six real life problems and the results are compared with the results of six other algorithms reported in the literature. The analysis of the results shows that the overall performance of ACO–LD is found to be better than the other algorithms included in the present study. "],"prob":["problem, algorithm, solution, optimization, solve, optimal, genetic_algorithm"],"frex":["solve, problem, optimization, solution, algorithm, heuristic, swarm"],"lift":["aco, ant_colony_optimization, swarm, heuristic, solve, particle, solution"],"score":["aco, algorithm, problem, optimization, solve, solution, genetic_algorithm"],"proportion":["0.04"]},{"name":["production, schedule, manufacture, plan, line, programme, constraint"],"size":[1800],"topic_no":[61],"thought_1":["In order to overcome the problems such as low efficiency of flexible production line and delay of production, this paper proposes a scheduling method of green flexible production line based on task priority. The mathematical model of green flexible production line scheduling is established with the constraints of non-interruptibility and machine uniqueness. The dynamic algorithm of task priority based on the number value of remaining operations is applied to the model to determine the priority distribution order. Through ant colony optimisation algorithm to determine the processing path, through the expression of pheromone and the establishment of ant colony solution to achieve green flexible production line scheduling. The experimental results show that the research method can effectively schedule the green flexible production line, and the machine utilisation rate is more than 90% of the processing level, which effectively reduces the production delay of the workpiece. "],"thought_2":["We describe a real world case study that involves the monthly planning and scheduling of the sand-casting department in a metal foundry. The problem can be characterised as a single-level multi-item capacitated lot-sizing model with a variety of additional process-specific constraints. The main objective is to smooth production. We present a hierarchical approach, in which we use a combination of mixed integer linear programming, shortest path algorithms, and iterative local improvement. The quality of the production schedules obtained in this way is by far superior to the quality of the schedules constructed by a very expert production planner with no other tool than a plan board. Furthermore, the planning effort is significantly reduced: the manual method requires about 2-3 days, whereas a typical planning session with a prototype decision support system takes no more than half an hour. "],"prob":["production, plan, manufacture, schedule, process, line, operation"],"frex":["production, schedule, manufacture, plan, line, programme, constraint"],"lift":["schedule, line, manufacture, production, plan, programme, flexible"],"score":["schedule, manufacture, production, plan, line, constraint, sequence"],"proportion":["0.02"]}],"topic_no":[3,61]},{"name":["power, load, generation, day, operate, energy, curve"],"size":[1800],"topic_no":[38],"thought_1":["Biological hydrogen sulphide (H2S) removal from a biogas mimic (pH = ∼7.0) was tested for 189 days in an anoxic biotrickling filter (BTF) inoculated with a pure culture of Paracoccus versutus strain MAL 1HM19. The BTF was packed with polyurethane foam cubes and operated in both fed-batch and continuous modes. The H2S inlet concentration to the BTF was varied between ∼100 and ∼500 ppmv during steady-state tests, and thereafter to ∼1000, ∼2000, ∼3000 and ∼4000 ppmv during shock-load (i.e. transient state) tests. The H2S removal efficiency (RE) ranged between 17 and 100% depending on the operational mode of the BTF and the presence of acetate as a carbon source. The maximum elimination capacity (ECmax) of the BTF reached 113.5 (±6.4) g S/m3 h with 97% RE during H2S shock-load experiments at ∼4000 ppmv which showed the robustness and resilient capacity of BTF for the large fluctuations of H2S concentrations. The results from polymerase chain reaction denaturing gradient gel electrophoresis (PCR–DGGE) revealed that P. versutus remained dominant throughout the 189 days of BTF operation which can imply the crucial role of this bacterium to remove H2S and upgrade to clean biogas. The analysis using artificial neural networks (ANNs) predicted the H2S and NO3 −-N REs and SO4 2− production in the anoxic BTF. Consequently, this study revealed that a BTF can be used to treat H2S contamination of biogas under anoxic conditions. "],"thought_2":["Purpose: Load forecasting is important to any electrical grid, but for the developing and third-world countries with power shortages, load forecasting is essential. When planed load shedding programs are implemented to face power shortage, a noticeable distortion to the load curves will happen, and this will make the load forecasting more difficult. Design/methodology/approach: In this paper, a new load forecasting model is developed that can detect the effect of planned load shedding on the power consumption and estimate the load curve behavior without the shedding and with different shedding programs. A neuro-Fuzzy technique is used for the model, which is trained and tested with real data taken from one of the 11 KV feeders in Najaf city in Iraq to forecast the load for two days ahead for the four seasons. Load, temperature, time of the day and load shedding schedule for one month before are the input parameters for the training, and the load forecasting data for two days are estimated by the model. Findings: To verify the model, the load is forecasted without shedding by the proposed model and compared to real data without shedding and the difference is acceptable. Originality/value: The proposed model provides acceptable forecasting with the load shedding effect available and better than other models. The proposed model provides expected behavior of load with different shedding programs an issue helps to select the appropriate shedding program. The proposed model is useful to estimate the real demands by assuming load shedding hours to be zero and forecast the load. This is important in places suffer from grid problems and cannot supply full loads to calculate the peak demands as the case in Iraq. "],"prob":["power, generation, load, day, energy, operate, compare"],"frex":["power, load, generation, day, operate, energy, curve"],"lift":["load, power, generation, day, curve, storage, operate"],"score":["load, power, generation, day, energy, curve, storage"],"proportion":["0.01"]},{"name":["prediction, traffic, flow, temporal, predict, prediction_accuracy, delay"],"size":[1800],"topic_no":[26],"thought_1":["As an important part of a smart city, intelligent transport can effectively reduce energy consumption and environmental pollution. Traffic flow forecasting provides a reliable traffic dispatch basis for intelligent transport, and most of the existing prediction methods only predict a single saturation or speed and do not use the saturation and speed in a unified way. This paper proposes a new traffic flow prediction method based on RNN-GCN and BRB. First, the belief rule base (BRB) is used for data fusion to obtain new traffic flow data, then the recurrent neural network (RNN) and graph convolution neural network (GCN) model is used to obtain the time correlation of the traffic data, and finally, the traffic flow is predicted by the topology graph. The experimental results show that the method has a better performance than ARIMA, LSTM, and GCN. "],"thought_2":["Travel time of traffic flow is the basis of traffic guidance. To improve the estimation accuracy, a travel time estimation model based on Random Forests is proposed. 7 influence variables are viewed as candidates in this paper. Data obtained from VISSIM simulation are used to verify the model. Different from other machine learning algorithm as black boxes, Random Forests can provide interpretable results through variable importance. The result of variable importance shows that mean travel time of floating car t-f, traffic state parameter X, density of vehicle Kall, and median travel time of floating car tmenf are important variables affecting travel time of traffic flow; meanwhile other variables also have a certain influence on travel time. Compared with the BP (Back Propagation) neural network model and the quadratic polynomial regression model, the proposed Random Forests model is more accurate, and the variables contained in the model are more abundant. "],"prob":["prediction, predict, data, traffic, time, flow, prediction_accuracy"],"frex":["prediction, traffic, flow, temporal, predict, prediction_accuracy, delay"],"lift":["traffic, flow, temporal, prediction, prediction_accuracy, delay, recurrent"],"score":["traffic, prediction, flow, predict, prediction_accuracy, temporal, data"],"proportion":["0.01"]},{"name":["error, efficiency, parameter, neural_network, artificial, artificial_neural_network, input"],"children":[{"name":["parameter, artificial_neural_network, artificial, surface, material, neural_network, optimize"],"size":[1800],"topic_no":[35],"thought_1":["The study aims at design and development of an integrated system to model and optimize the cutting parameters for identifying and controlling the parameters so as to achieve high level performance and quality in the 2.5 D end milling process. Taguchi’s method is used for experimental design to find out the critical parameters in the 2.5 D end milling process. An optimized artificial neural network (ANN) based on feed-forward back propagation was used to establish the model between 2.5 D end milling parameters and responses. Genetic algorithm (GA) was utilized to find the best combination of cutting parameters providing lower temperature rise in the work piece (Al 6061 T6). As fitness function of GA the output of ANN is used in the study. Cutting parameters include speed, feed, depth of cut and step over. Parameters found out by GA were able to lowers the minimum temperature rise from 19.7 to 17.2 °C with almost 13% decrease. Both the modeling and optimization process was found satisfactory. Also the GA has emerged as an effective tool to optimize 2.5 D end milling process parameters. "],"thought_2":["Prediction of cutting parameters as a function of cutting force, surface roughness and cutting temperature is very important in face milling operations. In the present study, the effect of cutting parameters on the mentioned responses were investigated by using artificial neural networks (ANN) which were trained by using experimental results obtained from Taguchis L8 orthogonal design. The experimental results are compared with the results predicted by ANN and the Taguchi method. By training the ANN with the results of experiments which are corresponding with the Taguchi L8 design, with only eight experiments an effective ANN model is trained. By using this network model the other combinations of experiments which did not perform previously, could be predicted with acceptable error. "],"prob":["parameter, artificial_neural_network, artificial, neural_network, process, predict, surface"],"frex":["parameter, artificial_neural_network, artificial, surface, material, neural_network, optimize"],"lift":["surface, parameter, artificial_neural_network, force, fee, temperature, artificial"],"score":["surface, artificial_neural_network, parameter, artificial, neural_network, temperature, process"],"proportion":["0.03"]},{"name":["efficiency, output, energy_consumption, input, coefficient, error, energy"],"size":[1800],"topic_no":[62],"thought_1":["Sensitivity analysis establishes priorities for research and allows to identify and rank the most important factors which lead to great improvements in output factors. The aim of this study is to examine sensitivity analysis of inputs in grape production. We are proposing to perform sensitivity analysis using partial rank correlation coefficient (PRCC) which is the most reliable and efficient method, and we apply this for the first time in crop production. This research investigates the use of energy in the vineyard of a semi-arid zone of Iran. Energy use efficiency, energy productivity, specific energy and net energy were calculated. Various artificial neural network (ANN) models were developed to predict grape yield with respect to input energies. ANN models consist of a multilayer perceptron (MLP) with seven neurons in the input layer, one and two hidden layer(s) with different number of neurons, and an output layer with one neuron. Input energies were labor, machinery, chemicals, farmyard manure (FYM), diesel, electricity and water for irrigation. Sensitivity analysis was performed on over 100 samples of parameter space generated by Latin hypercube sampling method, which was then fed to the ANN model to predict the yield for each sample. The PRCC between the predicted yield and each parameter value (input) was used to calculate the sensitivity of the model to each input. Results of sensitivity analysis showed that machinery had the greatest impact on grape yield followed by diesel fuel and labor. "],"thought_2":["Estimation Solar radiation is the most significant part of the optimization of solar power. This may be achieved if the solar radiation is predicted well in advance. Meteorological stations have radiation measuring devices like pyranometer, pyrheliometer, radiometer, solarimeter, etc. however, it may not be available at the location of interest for researchers. Due to this limitation solar radiation estimation models are devised based on location details like Altitude, Latitude, Longitude, and metrological details like Wind Speed, Ambient Temperature, Relative Humidity, Day Temperature, etc. These radiation models provide Global Solar Radiation (GSR) as output. These models are statistically tested based on error calculation like Mean Bias Error, Mean Absolute Error, Root Mean Square Error, etc. This paper is framed to briefly provide the idea behind different solar radiation estimation models with the methodology used. Soft computing-based models are mainly analyzed here. ANN-based Global Solar Irradiance Estimation Model has been developed using geographical parameters like Elevation, Latitude, Longitude, Longitude, and meteorological parameters like Months of a year, Days of a month, Temperature, Atmospheric Pressure, Relative Humidity, and Wind Speed. The data are downloaded from the National Solar Radiation Database (NSRDB) for 2014 (latest available). From this paper, the reader will come to know about various techniques used in solar radiation estimation. The developed ANN-based model has better results for training, testing, validation, and all with Regression value of 0.94304, 0.9488, 0.94766, 0.94556 respectively. The MSE is found to be 0.0089147 at epoch 0. The obtained values of R and MSE indicates the suitability of the developed model. "],"prob":["input, efficiency, error, energy, output, build, coefficient"],"frex":["efficiency, output, energy_consumption, input, coefficient, error, energy"],"lift":["energy_consumption, coefficient, output, efficiency, energy, root, absolute"],"score":["energy_consumption, energy, error, output, input, efficiency, coefficient"],"proportion":["0.01"]}],"topic_no":[35,62]},{"name":["time_series, regression, linear, compare, forecast, neural_network, performance"],"children":[{"name":["linear, nonlinear, regression, neural_network, hybrid, backpropagation, non-linear"],"size":[1800],"topic_no":[2],"thought_1":["Artificial neural network approach is a well-known method that is a useful tool for time series forecasting. Since real life time series can generally contain both linear and nonlinear components, hybrid approaches which can model both these two components have also been proposed in the literature. The hybrid approaches suggested in the literature generally have two phases. In the first phase, linear component of time series is modeled with a linear model. Then, nonlinear component is modeled by utilizing a nonlinear model in the second phase. In two-phase methods, it is assumed that time series has only a linear structure in the first phase. Also, it is assumed that time series has only a nonlinear structure in the second phase. Therefore, this causes model specification error. In order to overcome this problem, a novel neural network model, which consists of both linear and nonlinear structures, is proposed in this study. The proposed model considers that time series has both linear and nonlinear components. Multiplicative and Mc Culloch-Pitts neuron structures are employed for nonlinear and linear parts of the proposed model, respectively. In addition, the modified particle swarm optimization method is used to train the proposed neural network model. In order to show the performance of the proposed approach, it is applied to three real life time series and obtained results are compared to those obtained from other approaches available in the literature. It is observed that the proposed model gives the best forecasts for these three time series. "],"thought_2":["Artificial neural network modelling has recently attracted much attention as a new technique for estimation and forecasting in economics and finance. The chief advantages of this new approach are that such models can usually find a solution for very complex problems, and that they are free from the assumption of linearity that is often adopted to make the traditional methods tractable. In this paper we compare the performance of Back-Propagation Artificial Neural Network (BPN) models with the traditional econometric approaches to forecasting the inflation rate. Of the traditional econometric models we use a structural reduced-form model, an ARIMA model, a vector autoregressive model, and a Bayesian vector autoregression model. We compare each econometric model with a hybrid BPN model which uses the same set of variables. Dynamic forecasts are compared for three different horizons: one, three and twelve months ahead. Root mean squared errors and mean absolute errors are used to compare quality of forecasts. The results show the hybrid BPN models are able to forecast as well as all the traditional econometric methods, and to outperform them in some cases. Copyright "],"prob":["neural_network, regression, compare, performance, linear, hybrid, nonlinear"],"frex":["linear, nonlinear, regression, neural_network, hybrid, backpropagation, non-linear"],"lift":["autoregressive, nonlinear, non-linear, linear, backpropagation, perceptron, multilayer"],"score":["autoregressive, neural_network, regression, nonlinear, time_series, linear, hybrid"],"proportion":["0.03"]},{"name":["forecast, competition, time_series, horizon, long-term, short-term, accuracy"],"size":[1800],"topic_no":[63],"thought_1":["Forecasting competitions are now so widespread that it is often forgotten how controversial they were when first held, and how influential they have been over the years. I briefly review the history of forecasting competitions, and discuss what we have learned about their design and implementation, and what they can tell us about forecasting. I also provide a few suggestions for potential future competitions, and for research about forecasting based on competitions. "],"thought_2":["The automated Neural Network Autoregressive (NNAR) algorithm from the forecast package in R generates sub-optimal forecasts when faced with seasonal tourism demand data. We propose denoising as a means of improving the accuracy of NNAR forecasts via an application into forecasting monthly tourism demand for ten European countries. Initially, we fit NNAR models on both raw and denoised (with Singular Spectrum Analysis) tourism demand series, generate forecasts and compare the results. Thereafter, the denoised NNAR forecasts are also compared with parametric and nonparametric benchmark forecasting models. Contrary to the deseasonalising hypothesis, we find statistically significant evidence which supports the denoising hypothesis for improving the accuracy of NNAR forecasts. Thus, it is noise and not seasonality which hinders NNAR forecasting capabilities. "],"prob":["forecast, time_series, accuracy, competition, future, combine, accurate"],"frex":["forecast, competition, time_series, horizon, long-term, short-term, accuracy"],"lift":["competition, horizon, forecast, long-term, time_series, short-term, series"],"score":["competition, forecast, time_series, horizon, short-term, accuracy, long-term"],"proportion":["0.01"]}],"topic_no":[2,63]},{"name":["account, estimate, estimation, procedure, claim, individual, stochastic"],"size":[1800],"topic_no":[11],"thought_1":["We present an actuarial claims reserving technique that takes into account both claim counts and claim amounts. Separate (overdispersed) Poisson models for the claim counts and the claim amounts are combined by a joint embedding into a neural network architecture. As starting point of the neural network calibration, we use exactly these two separate (overdispersed) Poisson models. Such a nested model can be interpreted as a boosting machine. It allows us for joint modeling and mutual learning of claim counts and claim amounts beyond the two individual (overdispersed) Poisson models. "],"thought_2":["The aim of this project is to develop a stochastic simulation machine that generates individual claims histories of non-life insurance claims. This simulation machine is based on neural networks to incorporate individual claims feature information. We provide a fully calibrated stochastic scenario generator that is based on real non-life insurance data. This stochastic simulation machine allows everyone to simulate their own synthetic insurance portfolio of individual claims histories and back-test thier preferred claims reserving method. "],"prob":["estimate, account, procedure, estimation, individual, simulation, claim"],"frex":["account, estimate, estimation, procedure, claim, individual, stochastic"],"lift":["claim, account, estimate, estimation, procedure, individual, stochastic"],"score":["claim, estimate, estimation, account, procedure, individual, simulation"],"proportion":["0.01"]},{"name":["technique, machine_learn, financial, predict, predictive, variable, rate"],"children":[{"name":["variable, rate, bank, financial, ratio, predictor, finance"],"size":[1800],"topic_no":[7],"thought_1":["This study determines whether it is possible to distinguish between conventional and Islamic banks in the Gulf Cooperation Council (GCC) region on the basis of financial characteristics alone. Islamic banks operate under different principles, such as risk sharing and the prohibition of interest, yet both types of banks face similar competitive conditions. The combination of effects makes it unclear whether financial ratios will differ significantly between the two categories of banks. We input 26 financial ratios into logit, neural network, and k-means nearest neighbor classification models to determine whether researchers or regulators could use these ratios to distinguish between the two types of banks. Although the means of several ratios are similar between the two categories of banks, non-linear classification techniques (k-means nearest neighbors and neural networks) are able to correctly distinguish Islamic from conventional banks in out-of-sample tests at about a 92% success rate. "],"thought_2":["The banking sector plays a special role in the economy and has critical functions which are essential for economic stability. Hence, systemic banking crises disrupt financial markets and hinder global economic growth. In this study, Extreme Gradient Boosting, a state of the art machine learning method, is applied to identify a set of key leading indicators that may help predict and prevent bank failure in the Eurozone banking sector. The cross-sectional data used in this study consists of 25 annual financial ratio series for commercial banks in the Eurozone. The sample includes Eurozone listed failed and non-failed banks for the period 2006–2016. A number of early warning systems and leading indicator models have been developed to prevent bank failure. Yet the breadth and depth of the recent financial crisis indicates that these methods must improve if they are to serve as a useful tool for regulators and managers of financial institutions. Our goal is to build a classification model to determine which variables should be monitored to anticipate bank financial distress. A set of key variables are identified to anticipate bank defaults. Identifying leading indicators of bank failure is necessary so that regulators and financial institutions' management can take preventive and corrective measures before it is too late. "],"prob":["variable, rate, financial, bank, ratio, predictor, apply"],"frex":["variable, rate, bank, financial, ratio, predictor, finance"],"lift":["bank, variable, rate, financial, ratio, predictor, finance"],"score":["bank, variable, financial, rate, ratio, predictor, finance"],"proportion":["0.01"]},{"name":["predictive, score, credit, technique, data_mine, decision_tree, logistic_regression"],"size":[1800],"topic_no":[31],"thought_1":["This study evaluates four machine learning (ML) techniques (Decision Trees (DT), Random Forests (RF), Neural Networks (NN) and Probabilistic Neural Networks (PNN)) on their ability to accurately predict export credit insurance claims. Additionally, we compare the performance of the ML techniques against a simple benchmark (BM) heuristic. The analysis is based on the utilisation of a dataset provided by the Berne Union, which is the most comprehensive collection of export credit insurance data and has been used in only two scientific studies so far. All ML techniques performed relatively well in predicting whether or not claims would be incurred, and, with limitations, in predicting the order of magnitude of the claims. No satisfactory results were achieved predicting actual claim ratios. RF performed significantly better than DT, NN and PNN against all prediction tasks, and most reliably carried their validation performance forward to test performance. "],"thought_2":["Purpose – This paper aims to distinguish whether the decision-making process of the Islamic financial houses in the UK can be improved through the use of credit scoring modeling techniques as opposed to the currently used judgmental approaches. Subsidiary aims are to identify how scoring models can reclassify accepted applicants who later are considered as having bad credit and how many of the rejected applicants are later considered as having good credit, and highlight significant variables that are crucial in terms of accepting and rejecting applicants, which can further aid the decision-making process. Design/methodology/approach – A real data set of 487 applicants is used consisting of 336 accepted credit applications and 151 rejected credit applications made to an Islamic finance house in the UK. To build the proposed scoring models, the data set is divided into training and hold-out subsets. The training subset is used to build the scoring models, and the hold-out subset is used to test the predictive capabilities of the scoring models. Seventy per cent of the overall applicants will be used for the training subset, and 30 per cent will be used for the testing subset. Three statistical modeling techniques, namely, discriminant analysis, logistic regression (LR) and multilayer perceptron (MP) neural network, are used to build the proposed scoring models. Findings – The findings reveal that the LR model has the highest correct classification (CC) rate in the training subset, whereas MP outperforms other techniques and has the highest CC rate in the hold-out subset. MP also outperforms other techniques in terms of predicting the rejected credit applications and has the lowest misclassification cost above other techniques. In addition, results from MP models show that monthly expenses, age and marital status are identified as the key factors affecting the decision-making process. Originality/value – This contribution is the first to apply credit scoring modeling techniques in Islamic finance. Also in building a scoring model, the authors' application applies a different approach by using accepted and rejected credit applications instead of good and bad credit histories. This identifies opportunity costs of misclassifying credit applications as rejected. © 2014, "],"prob":["technique, machine_learn, predictive, predict, score, good, statistical"],"frex":["predictive, score, credit, technique, data_mine, decision_tree, logistic_regression"],"lift":["credit, logistic_regression, decision_tree, predictive, score, data_mine, boost"],"score":["credit, technique, predictive, score, decision_tree, data_mine, machine_learn"],"proportion":["0.02"]}],"topic_no":[7,31]}],"topic_no":[25,43,59,3,61,38,26,35,62,2,63,11,7,31]}],"topic_no":[14,34,20,60,28,10,50,6,8,13,41,36,53,16,49,25,43,59,3,61,38,26,35,62,2,63,11,7,31]}],"topic_no":[1,15,4,42,24,51,29,44,55,66,30,47,57,5,33,58,37,45,39,46,27,54,64,9,22,40,52,12,67,23,18,65,19,32,56,21,17,48,14,34,20,60,28,10,50,6,8,13,41,36,53,16,49,25,43,59,3,61,38,26,35,62,2,63,11,7,31],"name":["AI STM 2021"],"this_root":[true],"summary":["A topic model with 67 topics, 10036 documents and a 1073 word dictionary."],"proportions":[0.0142,0.0276,0.0424,0.0093,0.0088,0.0105,0.0129,0.0109,0.0202,0.0263,0.0136,0.009,0.0125,0.0138,0.012,0.0093,0.0111,0.008,0.0069,0.0131,0.0114,0.015,0.0088,0.0173,0.0163,0.0143,0.0156,0.016,0.0116,0.0058,0.024,0.0066,0.0074,0.0127,0.0256,0.0176,0.0098,0.0063,0.014,0.0131,0.0518,0.0188,0.0102,0.0091,0.0161,0.0144,0.0104,0.0125,0.0108,0.0236,0.0182,0.0131,0.0193,0.0174,0.0175,0.0119,0.0112,0.0223,0.0245,0.0114,0.0174,0.0139,0.0146,0.0198,0.008,0.0146,0.0058]}
 
 root.x0=0;root.y0=0;update(root);function update(source){var nodes=tree.nodes(root);var height=Math.max(500,nodes.length*barHeight+margin.top+margin.bottom);
  d3.select("svg").transition().duration(duration).attr("height",height);d3.select(self.frameElement).transition().duration(duration).style("height",height+"px");nodes.forEach(function(n,i){n.x=i*barHeight;});
  var node=svg.selectAll("g.node").data(nodes,function(d){return d.id||(d.id=++i);});
  var nodeEnter=node.enter().append("g").attr("class",function(d){if(d.size){return"node leaf"}else{return"node noleaf"}}).attr("transform",function(d){return"translate("+source.y0+","+source.x0+")";}).style("opacity",1e-6);
  nodeEnter.append("rect").data(nodes).attr("y",-barHeight/2).attr("height",barHeight).attr("width",barWidth).style("fill",color).on("click",clickModal);nodeEnter.append("text").attr("dy",3.5).attr("dx",5.5).text(function(d){if (d.children){return d.name;}else{return d.topic_no + ": " + d.name;}});
  d3.selectAll("g.noleaf").append("svg:foreignObject").attr("width",20).attr("height",20).attr("y","-10px").attr("x",barWidth-15).append("xhtml:span").attr("class","control glyphicon glyphicon-minus").attr("width","30").on("click",function(d){click(d);});
  nodeEnter.transition().duration(duration).attr("transform",function(d){return"translate("+d.y+","+d.x+")";}).style("opacity",1);node.transition().duration(duration).attr("transform",function(d){return"translate("+d.y+","+d.x+")";}).style("opacity",1).select("rect").style("fill",color);
  node.exit().transition().duration(duration).attr("transform",function(d){return"translate("+source.y+","+source.x+")";}).style("opacity",1e-6).remove();
  var link=svg.selectAll("path.link").data(tree.links(nodes),function(d){return d.target.id;});link.enter().insert("path","g").attr("class","link").attr("d",function(d){var o={x:source.x0,y:source.y0};return diagonal({source:o,target:o});}).transition().duration(duration).attr("d",diagonal);
  link.transition().duration(duration).attr("d",diagonal);link.exit().transition().duration(duration).attr("d",function(d){var o={x:source.x,y:source.y};return diagonal({source:o,target:o});}).remove();nodes.forEach(function(d){d.x0=d.x;d.y0=d.y;});}
  function click(d){if(d.children){d._children=d.children;d.children=null;}else{d.children=d._children;d._children=null;}update(d);}
  function color(d){return d._children?"#3182bd":d.children?"#c6dbef":"#fd8d3c";}
  function clickModal(d){if(d.size){$("#doc1").text(d.thought_1);$("#doc2").text(d.thought_2);$("#high-prob").text("Highest Probability: "+d.prob);$("#topicModalLabel").text("Topic "+d.topic_no+" Information");$("#frex").text("FREX: "+d.frex);$("#lift").text("Lift: "+d.lift);
  $("#score").text("Score: "+d.score);$("#proportion").text(""+d.proportion);$("#modelBody").hide();$("#clusterBody").hide();$("#topicBody").show();$("#topicModal").modal("show");}else if(d.this_root){$("#topicModalLabel").text("Fitted Model Information");$("#mod1-text").text(d.summary);$("#topicBody").hide();
  $("#clusterBody").hide();$("#modelBody").show();$("#topicModal").modal("show");if ($("#barchartDiv").children().length == 2){proportionChart();}}else{$("#topicModalLabel").text("Cluster Information");$("#clust1-text").text("This cluster comprises topics "+d.topic_no.join(", ")+".");$("#topicBody").hide();$("#modelBody").hide();
  $("#clusterBody").show();$("#topicModal").modal("show");}} function proportionChart(){for(var t=window.innerHeight/3.5,a=window.innerWidth/2.5,r=35,e=35,n=root.proportions.length,o=[.5];o.length<n;)o.push(o[o.length]+1);var i=d3.select("#barchartDiv").append("svg").attr({width:a,height:t,style:"display: block; margin: auto;"}),
  l=d3.scale.linear().domain([0,d3.max(root.proportions)]).range([0,t-r]),s=d3.scale.linear().domain([0,n]).range([0,a-e]),d=d3.scale.linear().domain([0,d3.max(root.proportions)]).range([t-r,0]),p=d3.svg.axis().scale(d).orient("left"),c=d3.svg.axis().scale(s).orient("bottom").tickValues(d3.range(.5,n+.5,1));
  i.selectAll("rect").data(root.proportions).enter().append("rect").attr({x:function(t,r){return r*(a-e)/n+e},y:function(a){return t-l(a)-r},width:(a-e)/n-1,height:l,fill:"orange"}),
  i.append("g").attr({"class":"axis",transform:"translate("+e+","+(t-r)+")"}).call(c),i.append("g").attr({"class":"axis",transform:"translate("+e+")"}).call(p),i.append("text").attr("class","x label").attr("text-anchor","middle").attr("x",a/2).attr("y",t-6).text("Topic")}</script>
  <div class="modal fade" id="topicModal" tabindex="-1" role="dialog" aria-labelledby="topicModalLabel" aria-hidden="true">
  <div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="topicModalLabel">Topic Information</h4></div>
  <div class="modal-body" id="topicBody"><h5>Top Words</h5><ul id="word-list"><li id="high-prob">Highest Probability: </li><li id="frex">FREX: </li><li id="lift">Lift: </li><li id="score">Score: </li></ul><hr><h5>Representative Documents</h5>
  <div id="doc1" class="modal-body scrollbox"></div>
  <br><div id="doc2" class="modal-body scrollbox"></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Close</button></div></div><div class="modal-body" id="modelBody"><h5>Summary</h5><span id="mod1-text"></span><hr><div id="barchartDiv">
  <h5>Topic Proportions in Corpus</h5><br></div><br><div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button></div></div><div class="modal-body" id="clusterBody"><h5>Summary</h5><span id="clust1-text"></span><br><br><div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div></body>
