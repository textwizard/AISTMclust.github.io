<!DOCTYPE html><meta charset=UTF-8>
  <style>.node rect{cursor:pointer;fill:#fff;fill-opacity:.5;stroke:#3182bd;stroke-width:1.5px;}
  .node text{font:10px sans-serif;pointer-events:none;}
  path.link{fill:none;stroke:#9ecae1;stroke-width:1.5px;}
  .control.glyphicon{position:static;color:#4A4C4F;font-family:"Oxygen", sans-serif;cursor:pointer;}
  .scrollbox{height: 120px;border: 1px solid #e5e5e5;overflow: scroll;}
  .bar{fill: steelblue;}.axis{font: 10px sans-serif;}.axis path,.axis line{fill: none; stroke: #000;
  shape-rendering: crispEdges;}.x.axis path{display: none;}</style>
  <link href="https://fonts.googleapis.com/css?family=Oxygen" rel=stylesheet type="text/css">
  <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css" rel=stylesheet>
  <title>STM Visualization</title><body>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.min.js"></script>
  <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
  <script>var margin={top:30,right:20,bottom:30,left:20},width=window.innerWidth/1.5-margin.left-margin.right,height=window.innerHeight-margin.top-margin.bottom,barHeight=20,barWidth=width*.8;
  var i=0,duration=400,root;var tree=d3.layout.tree().nodeSize([0,20]);var diagonal=d3.svg.diagonal().projection(function(d){return[d.y,d.x];});
  var svg=d3.select("body").append("svg").attr("width",width+margin.left+margin.right).attr("height",height+margin.top+margin.bottom).append("g").attr("transform","translate("+margin.left+","+margin.top+")");
  root= {"children":[{"name":["decision, technology, process, knowledge, develop, management, artificial_intelligence"],"children":[{"name":["technology, work, artificial_intelligence, knowledge, process, service, management"],"children":[{"name":["practice, management, analytics, healthcare, practitioner, big_data, organisation"],"size":[1800],"topic_no":[55],"thought_1":["COVID-19’s rapid spread has caused a global pandemic. Consequently, it is imperative that healthcare organisations conduct crisis management (CM) to cope with this calamity. This study presents a set of operational guidelines for healthcare organisations to launch effective countermeasures against such crises by means of effective knowledge management (KM) practices. Additionally, information-technology (IT) applications can significantly improve organisations’ CM and KM capabilities by enhancing organisational responsiveness and flexibility. This study thus aims to articulate how the use of innovative IT-enabled mechanisms (e.g., non-contact monitoring devices, intelligent robots, and telemedicine) can reduce the risk of exposure and leverage an artificial intelligence-based epidemic intelligence dashboard to support appropriate decision-making by taking the operation of healthcare organisations in Taiwan during COVID-19 crisis as an example. The research results demonstrate the effectiveness of the employment of IT-enabled KM practices in CM settings in terms of preventing or minimising undesirable crisis consequences. "],"thought_2":["Purpose: This paper aims to explore the effectiveness of fraud prevention and detection techniques, including data analytics, machine learning and data mining, and to understand how widespread the use of data analytics is across different sectors and to identify and understand the potential barriers to implementing these techniques to detect and prevent fraud. Design/methodology/approach: A survey was administered to 73 Irish businesses to determine to what extent traditional approach, data mining or text mining are being used to prevent or detect fraudulent financial reporting, and to determine the perception level of their effectiveness. Findings: The study suggests that whilst data analytics is widely used by businesses in Ireland there is an under-utilisation of data analytics as an effective tool in the fight against fraud. The study suggests there are barriers that may be preventing companies from implementing advanced data analytics to detect financial statement fraud and identifies how those barriers may be overcome. Originality/value: In contrast to the majority of literature on big data analytics and auditing, which lacks empirical insight into the diffusion, effectiveness and obstacles of data analytics, this explanatory study contributes by providing useful insights from the field on big data analytics. While the extant auditing literature generally addresses the avenues of big data utilisation in auditing domain, our study explores particularly the use big data analytics as a fraud prevention and detection techniques. "],"prob":["management, practice, analytics, information, big_data, practitioner, healthcare"],"frex":["practice, management, analytics, healthcare, practitioner, big_data, organisation"],"lift":["healthcare, practice, management, analytics, organisation, guideline, practitioner"],"score":["healthcare, management, practice, analytics, big_data, organisation, practitioner"],"proportion":["0.02"]},{"name":["innovation, technological, conceptual, opportunity, emerge, collaboration, integration"],"size":[1800],"topic_no":[24],"thought_1":["This paper makes a distinction between three theoretical frameworks that have been highly influential in the discourse on innovation, competitiveness and sustainability: sectoral systems of innovation (SSI), technological innovation systems (TIS) and socio-technical systems (ST-Systems). These frameworks share a common systems approach to innovation but are often positioned as different bodies of literature that correspond to different epistemic communities. This paper is explorative and conceptual in nature. It presents a systematic comparative review of SSI, TIS and ST-Systems based on the following analytical dimensions: (1) system boundaries, (2) actors and networks, (3) institutions, (4) knowledge, (5) dynamics and (6) policy approach. In the concluding section commonalities and differences, of the three approaches, are presented and suggestions for complimentarily are made. "],"thought_2":["The 'Technological Innovation System' (TIS) framework and its system functions have become a popular analytical tool for the study of clean-tech innovation. There is increasing attention for the role of emerging economies in global clean-tech innovation, but the applicability of TIS to emerging economies cases is not entirely straightforward. A key issue is the limited geographical considerations, in particular transnational dimensions in TIS, whereas earlier perspectives on innovation in emerging economies have stressed the role of such transnational dimensions. This paper elaborates transnational TIS actor-networks and institutions, categorizes these in relation to TIS functions, and describes their potential to induce or block TIS development in emerging economies. We draw on insights from the perspectives of National Learning Systems, International Technology Transfer, and Global Production Networks for this purpose. We conclude that the potential effects of these transnational dimensions may be accurately grasped by the existing list of system functions, lending credence to its further application of the TIS framework on emerging economy case studies. Policy makers in emerging economies should recognize these transnational dimensions and seek to optimize their potential effect on domestic TIS development, taking in to consideration a realistic assessment of its role in the global TIS. "],"prob":["innovation, technological, development, opportunity, conceptual, emerge, policy"],"frex":["innovation, technological, conceptual, opportunity, emerge, collaboration, integration"],"lift":["innovation, technological, collaboration, conceptual, opportunity, collaborative, emerge"],"score":["innovation, technological, conceptual, opportunity, collaboration, collaborative, development"],"proportion":["0.01"]},{"name":["digital, advance, technology, industry, change, artificial_intelligence, smart"],"children":[{"name":["artificial_intelligence, digital, skill, society, bring, change, professional"],"size":[1800],"topic_no":[51],"thought_1":["Purpose: The purpose of this paper is to examine new directions for diversity scholarship in the context of future of work or advanced technological shifts that are impacting organizations and society. It proposes that both new opportunities and challenges are likely to emerge for individuals and offers considerations around ethics, inequalities and global dimensions as relevant conversations within this domain. Design/methodology/approach: The paper provides an overview of new technological advances in the domains of artificial intelligence, automation and the gig economy. It then layers considerations related to diversity within this context, focusing on issues of relevance to mainstream, critical and transnational traditions within diversity scholarship. Findings: It is likely that technological shifts will impact several domains of diversity scholarship including how we define “diversity,” and the value and appropriateness of using advanced technologies to replace certain jobs that are predominantly held by underrepresented groups. Furthermore, the paper outlines ways in which bias, ethical considerations and emergent digital inequalities will become important conversations within diversity research in the context of future of work. Originality/value: This paper brings together diversity scholarship and future of work conversations in assessing the ways such research and trends will intersect and provides insights about future directions that diversity-focused research should take to address and understand the consequences of rapid technological advances for inclusion. "],"thought_2":["Public relations’ (PR) professional habitus is defined by a relentless focus on optimism and futurity. This professional habitus renders PR indispensable to the corporate world after crisis, when new, potentially controversial, growth strategies must be sold-in to stakeholders. This article argues that PR’s professional habitus is heavily influenced by neoliberalism, an ideology which ‘confidently identifies itself with the future’. The discussion is timely, as 21st-century neoliberal capitalism becomes redefined by artificial intelligence (AI). The article combines PR theory, communications theory and political economy to consider the changing shape of neoliberal capitalism, as AI becomes naturalised as ‘common sense’ and a ‘public good’. The article explores how PR supports AI discourses, including promoting AI in national competitiveness and promoting ‘friendly’ AI to consumers, while promoting Internet inequalities. The article concludes that the PR profession’s myopia regarding the implications of promoting AI and neoliberalism is shaped by poor levels of diversity in the PR profession. "],"prob":["artificial_intelligence, digital, change, future, skill, society, world"],"frex":["artificial_intelligence, digital, skill, society, bring, change, professional"],"lift":["skill, artificial_intelligence, digital, society, bring, professional, foundation"],"score":["skill, artificial_intelligence, digital, society, change, future, professional"],"proportion":["0.02"]},{"name":["smart, technology, industry, thing, benefit, industrial, advance"],"size":[1800],"topic_no":[68],"thought_1":["We inspect the relevant literature on sustainable smart manufacturing in Industry 4.0, providing both quantitative evidence on trends and numerous in-depth empirical examples. Building our argument by drawing on data collected from BDC, BDO, Deloitte, eMarketer, McKinsey, and PwC, we performed analyses and made estimates regarding companies deploying artificial intelligence at scale (%, by industry) and applications of Industry 4.0 in small and mid-sized businesses (%). Data collected from 4,300 respondents are tested against the research model by using structural equation modeling. "],"thought_2":["We inspect the relevant literature on sensing, smart, and sustainable technologies in Industry 4.0, providing both quantitative evidence on trends and numerous in-depth empirical examples. Building our argument by drawing on data collected from Ad Hoc Research, BDO, DAA, Deloitte, eMarketer, IoT Analytics GmbH, Management Events, and McKinsey, we performed analyses and made estimates regarding challenges of implementing artificial intelligence (%), top barriers encountered by manufacturers in Industry 4.0 (%), the biggest benefits of industrial data analytics for organizations (%), and top five Industry 4.0 applications with progress achieved (%). Data collected from 4,600 respondents are tested against the research model by using structural equation modeling. "],"prob":["technology, industry, smart, data, benefit, advance, thing"],"frex":["smart, technology, industry, thing, benefit, industrial, advance"],"lift":["smart, thing, technology, industry, benefit, modern, industrial"],"score":["smart, technology, industry, thing, internet, industrial, digital"],"proportion":["0.01"]}],"topic_no":[51,68]},{"name":["firm, effect, performance, company, impact, business, organization"],"children":[{"name":["organization, employee, organizational, outcome, effect, expectation, impact"],"size":[1800],"topic_no":[15],"thought_1":["Organizational politics and workplace victimization are social stressors with significant implications on the wellbeing of employees. Applying Job Demand Resources framework, this study examines the impact of favoritism/nepotism, supervisor incivility on employee cynicism, and work withdrawal, and the moderating role of gender. Utilizing a cross-sectional design, data were gathered from frontline employees working in 3-star hotels in Northern Cyprus. Results from structural equation modeling and artificial neural network revealed that: (1) favoritism/nepotism has a positive impact on employee cynicism and work withdrawal; (2) employee cynicism has a positive impact on work withdrawal; (3) employee cynicism mediates the relationship between favoritism/nepotism, and work withdrawal; (4) the impact of employee cynicism on work withdrawal was about 6.7 times stronger for women; (5) the impact of favoritism/nepotism on work withdrawal was about 2.1 times stronger for men. Strategies to reduce this unwanted practices and how to keep employees productive are discussed. "],"thought_2":["Purpose: The purpose of this study is to explore the behavioral intention of the employees to adopt artificial intelligence (AI) integrated customer relationship management (CRM) system in Indian organizations. Design/methodology/approach: To identify the factors impacting the behavioral intention of the employees to adopt AI integrated CRM system in Indian organizations helps of literature review and theories have been taken. Thereafter, some hypotheses have been formulated followed by the development of a theoretical model conceptually. The model has been tested statistically for validation using a survey by considering 308 usable respondents. Findings: The results of this study show that perceived usefulness and perceived ease of use directly impact the behavioral intention of the employees to adopt an AI integrated CRM system in organizations. Also, these two exogenous factors impact the behavioral intention of the employees to adopt an AI integrated CRM system mediating through two intermediate variables such as utilitarian attitude (UTA) and hedonic attitude (HEA). The proposed model has achieved predictive power of 67%. Research limitations/implications: By the help of the technology acceptance model and motivational theory, the predictors of behavioral intention to adopt AI integrated CRM systems in organizations were identified. The effectiveness of the model was strengthened by the consideration of two employee-centric attitudinal attributes such as UTA and HEA, which is claimed to have provided contributions to the extant literature. The proposed theoretical model claims a special theoretical contribution as no extant literature considered the effects of leadership support as a moderator for the adoption of an AI integrated CRM system in Indian organizations. Practical implications: The model implies that the employees using AI integrated CRM system in organizations must be made aware of the usefulness of the system and the employees must not face any complexity to use the system. For this, the managers of the concerned organizations must create a conducive atmosphere congenial for the employees to use the AI integrated CRM system in the organizations. Originality/value: Studies covering exploration of the adoption of AI integrated CRM systems in Indian organizations are found to be in a rudimentary stage and in that respect, this study claims to have possessed its uniqueness. "],"prob":["organization, effect, impact, employee, organizational, outcome, manager"],"frex":["organization, employee, organizational, outcome, effect, expectation, impact"],"lift":["employee, organization, organizational, expectation, outcome, hypothesis, interview"],"score":["employee, organization, organizational, effect, impact, outcome, expectation"],"proportion":["0.01"]},{"name":["firm, company, corporate, business, internal, performance, external"],"size":[1800],"topic_no":[66],"thought_1":["Research Summary: We introduce to the upper echelons literature a novel, linguistic measure of CEOs' Big Five personality traits that we specifically developed and validated using a sample of CEOs. We then provide a predictive test of the measure by applying it to a sample of more than 3,000 CEOs of S&P 1500 firms to explore the direct and interactive effects of CEOs' Big Five personality traits and firm performance on strategic change. Our validated, unobtrusive measure of CEOs' Big Five traits provides a strong foundation for future theory development on the firm-level effects of CEOs' personality traits. Our specific findings also extend our understanding of how CEO personality influences firm-level change and how both person and situation-based factors interact to jointly influence firm strategy. Managerial Summary: This paper introduces a language-based tool we developed to measure the Big Five personality traits (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism) of more than 3,000 CEOs of S&P 1500 firms. After describing our process to develop and validate the tool, we test it by examining how CEOs' Big Five traits influence strategic change, both in isolation and in combination with recent firm performance. Our results suggest that CEOs' personality traits have a meaningful impact on strategic change, but that the nature of these effects differs based on their firms' recent performance. Our tool also provides a strong basis for scholars seeking to measure the personality traits of large samples of public-company executives. "],"thought_2":["Purpose: This paper examines the effects of managerial optimism on corporate cash holdings. Design/methodology/approach: The authors construct a novel measure of managerial optimism based on the linguistic tone of annual reports by applying a Naïve Bayesian Machine Learning algorithm to non-numeric parts of Vietnamese listed firms' reports from 2010 to 2016. The paper employs firm and year fixed effects model and also uses the generalized method of moments estimation as robustness checks. Findings: The authors find that the cash holding of firms managed by optimistic managers is higher than the cash holdings of firms managed by non-optimistic managers. Managerial optimism also influences corporate cash holdings through internal cash flows and the current year’s capital expenditures. Although the authors find no evidence that optimistic managers hold more cash to finance future growth opportunities in general, optimistic managers hold more cash for near future investment opportunities than non-optimistic managers do. Research limitations/implications: The novel measure proposed in this study is expected to provide great potential for future finance studies investigating the relation between managerial traits and corporate policies since it is applicable for any levels of financial market development. In addition, the findings highlight the important role, both direct and indirect, of managerial optimism on cash holdings. Related future research should take this psychological trait into account to gain a better understanding of corporate cash holding. Originality/value: This paper helps to extend the literature on managerial optimism measurement by introducing a new measure of managerial optimism based on the linguistic tone of annual reports. Furthermore, this is among the first studies directly linking annual report linguistic tone to cash holding. The paper also provides new evidence regarding how managerial optimism affects the relationship between the firm's growth opportunities and cash holding, given that mispricing corrections are naturally uncertain. "],"prob":["performance, company, firm, business, corporate, external, managerial"],"frex":["firm, company, corporate, business, internal, performance, external"],"lift":["firm, corporate, internal, company, managerial, external, business"],"score":["firm, company, business, performance, corporate, managerial, strategic"],"proportion":["0.01"]}],"topic_no":[15,66]},{"name":["service, social, satisfaction, relationship, influence, custom, trust"],"children":[{"name":["trust, social, influence, perceive, perception, intention, relationship"],"size":[1800],"topic_no":[1],"thought_1":["Due to the low adoption rate of mHealth apps, the apps designers need to understand the factors behind adoption. But understanding the determinants of mHealth apps adoption remains unclear. Comparatively less attention has been given to the factors affecting the adoption of mHealth apps among the young generation. This study aims to examine the factors influencing behavioral intention and actual usage behavior of mHealth apps among technology prone young generation. The research model has extracted variables from the widely accepted Unified Theory of Acceptance and Use of Technology (UTAUT2) alongside privacy, lifestyles, self-efficacy and trust. Required data were collected from mHealth apps users in Bangladesh. Firstly, this study confirmed that performance expectancy, social influence, hedonic motivation and privacy exerted a positive influence on behavioral intention whereas facilitating conditions, self-efficacy, trust and lifestyle had an influence on both behavioral intention and actual usage behavior. Secondly, the Neural Network Model was employed to rank relatively significant predictors obtained from structural equation modeling (SEM). This study contributes to the growing literature on the use of mHealth apps in trying to elevate the quality of patients' lives. The new methodology and findings from this study will significantly contribute to the extant literature of technology adoption and mHealth apps adoption intention especially. Therefore, for practitioners concerned with fostering mHealth apps adoption, the findings stress the importance of adopting an integrated approach centered on key findings of this study. "],"thought_2":["Drawing on the dual process theory, this study investigates the impacts of systematic and heuristic cues on travelers’ cognitive trust, emotional trust, and adoption intention toward artificial intelligence (AI)–based recommendation systems in travel planning. The moderating effect of perceived risk is also examined. Two studies with both scenario-based surveys and lab experiment approaches are conducted. Findings suggest that while travelers utilize both systematic and heuristic cues, effects of systematic cues on adoption as a decision aid is stronger than the effects of heuristic cues. Emotional trust has a stronger impact on intention to adopt as a delegated agent than cognitive trust. Perceived risk moderates the relationships between systematic and heuristic cues, trust, and adoption intentions. When travelers perceive high risk, they rely more on systematic cues through building cognitive trust. However, when the level of perceived risk is low, travelers depend more on heuristic cues through establishing emotional trust. "],"prob":["influence, social, relationship, trust, perceive, adoption, perception"],"frex":["trust, social, influence, perceive, perception, intention, relationship"],"lift":["trust, perception, intention, perceive, acceptance, social, influence"],"score":["trust, social, influence, perceive, intention, adoption, perception"],"proportion":["0.01"]},{"name":["custom, service, satisfaction, provider, delivery, experience, encounter"],"size":[1800],"topic_no":[47],"thought_1":["Purpose: The purpose of this paper is to propose an automated machine learning (AutoML) and multi-agent system approach to improve overall product delivery satisfaction under limited resources. Design/methodology/approach: An AutoML method is purposed to model delivery satisfaction of individual customer, and a heuristic method and multi-agent system are proposed to improve overall satisfaction under limited processing capability. A series of simulation experiments have been conducted to illustrate the effectiveness of the proposed methodology. Findings: The simulated results show that the proposed method can effectively improve overall delivery satisfaction, especially when the demand of customer orders is highly fluctuating and when the customer satisfaction models are highly diversified. Practical implications: The proposed framework provides a more dynamic and continuously improving way to model delivery satisfaction of individual customer, thereby supports companies to provide personalized services and develop scalable and flexible business at a lower cost, and ultimately improves the overall quality, efficiency and effectiveness of delivery services. Originality/value: The proposed methodology utilizes AutoML and multi-agent system to model individual customer delivery satisfaction and improve the overall satisfaction. It can cooperate with the existing delivery resource planning methods to further improve customer delivery satisfaction. The authors propose an AutoML approach to model individual customer delivery satisfaction, which enables continuous update and improvements. The authors propose multi-agent system and a heuristic method to improve overall delivery satisfaction. The numerical results show that the proposed method can improve overall delivery satisfaction with limited processing capability. "],"thought_2":["Purpose: The customer contact approach to service has been at the core of service theory since the 1970s. It suggests that the potential operating efficiency of a service is inversely related to the extent of customer contact with the provider's operations and that various service design issues are dictated by the presence or absence of customer contact. The purpose of this article is to reevaluate the customer contact approach in light of advanced digital technologies. Design/methodology/approach: The authors review the origins and history of the customer contact approach and show ways it has been refined in research literature. From that they demonstrate how the refined approach can be applied to contemporary conditions. Findings: Recent advances in digital technologies have indeed required us to revise our conceptualization of customer contact. There is now a blurring between front-office and back-office operations. Emerging technologies are allowing customers to have high-contact experiences with low-contact efficiencies. Research limitations/implications: Going forward, conceptualizations of customer contact are becoming increasingly complex and requiring increasingly complex models. Armed with self-service technologies, customers are able to permeate the “buffered core” of service businesses. Artificial intelligence and anthropomorphic devices have further blurred the distinction between front-office and back-office operations. Research will need to consider new forms of technology-enabled customer contact. Practical implications: Customer contact is no longer limited to interpersonal interactions and the relationships between service providers and customers are increasingly complex. Customers may interact with automated service providers, or service providers may interact with customer technologies. New forms of customer contact may not involve humans at all, but instead involve technologies interacting with technologies. Originality/value: The customer contact approach to service was one of the original models of service design. By revisiting and revising the model we bring it in-line with the realities of the contemporary service economy. "],"prob":["service, custom, satisfaction, provider, experience, delivery, improve"],"frex":["custom, service, satisfaction, provider, delivery, experience, encounter"],"lift":["custom, service, satisfaction, provider, delivery, encounter, satisfy"],"score":["custom, service, satisfaction, provider, delivery, experience, encounter"],"proportion":["0.01"]}],"topic_no":[1,47]},{"name":["agent, behavior, simulation, environment, dynamic, action, intelligent"],"children":[{"name":["dynamic, behavior, situation, game, complex, play, behaviour"],"size":[1800],"topic_no":[27],"thought_1":["The paper explores the implications of melioration learning-an empirically significant variant of reinforcement learning-for game theory. We show that in games with invariable pay-offs melioration learning converges to Nash equilibria in a way similar to the replicator dynamics. Since melioration learning is known to deviate from optimizing behavior when an action's rewards decrease with increasing relative frequency of that action, we also investigate an example of a game with frequency-dependent pay-offs. Interactive melioration learning is then still appropriately described by the replicator dynamics, but it indeed deviates from rational choice behavior in such a game. "],"thought_2":["This paper is an extension of the article [13] presented at IWCG of TAAI 2010. It proposes two dynamic randomization techniques for Monte-Carlo Tree Search (MCTS) in Go. First, during the in-tree phase of a simulation game, the parameters are randomized in selected ranges before each simulation move. Second, during the play-out phase, the priority orders of the simulation move-generators are hierarchically randomized before each play-out move. Essential domain knowledge used in MCTS for Go is discussed. Both dynamic randomization techniques increase diversity while keeping the sanity of the simulation games. Experimental testing has been completely re-conducted more extensively with the latest version of GoIntellect (GI) on all three Go categories of 19 × 19, 13 × 13, and 9 × 9 boards. The results show that dynamic randomization increases the playing strength of GI significantly with 128K simulations per move, the improvement is about seven percentage points in the winning rate against GnuGo on 19 × 19 Go over the version of GI without dynamic randomization, about three percentage points on 13 × 13 Go, and four percentage points on 9 × 9 Go. "],"prob":["dynamic, behavior, complex, simulation, situation, play, role"],"frex":["dynamic, behavior, situation, game, complex, play, behaviour"],"lift":["game, dynamic, behaviour, behavior, situation, choice, dynamics"],"score":["game, behavior, dynamic, team, dynamics, simulation, choice"],"proportion":["0.01"]},{"name":["agent, action, intelligent, paradigm, environment, communication, interface"],"size":[1800],"topic_no":[54],"thought_1":["The main aim of this paper is to discuss the design issues and implications that relate to the use of software agents in Training Systems. We have designed and implemented a Radiotherapy Treatment Planning Learning Environment (called RATAPLAN), which incorporates an interface intelligent agent to support training of Simulation Technologists and Radiation Physicists in this knowledge domain. The learning environment consists of an interactive simulation. The interface agent (called Consulta) acts both as demonstrator and assistant to the users. The paper describes in depth the agent's architecture and illustrates the agent-user interaction and communication. With Consulta we promote the collaboration between novice and expert practitioners in Radiotherapy Treatment Planning. "],"thought_2":["The impractical metaphor that boldly proclaims information technology is the “information highway” is unsustainable, since information technology is evolving faster than intraplanetary transportation technology. While many of our generation embrace this metaphor and believe that the future is embedded in a network paradigm (web, neural, or other), many of us yearn for the paradigm just beyond it This paper attempts to predict and define what future paradigms are in store for artificial intelligence (Al). Although we mortals have very few discernible (and replicable) methodologies for predicting the future, we tend to develop our hopes for the future around an identifiable hero. Stories surrounding this hero make up a mythology, so consequently I chose mythology as my methodology. In particular I depend on Wagner's opera tetralogy referred to Der Ring Des Nibelungen (The Ring of the Nibelung), which tells the past and prologue of a world full of gods, mortals, giants, dwarves, and (finally) introduces a genetically- evolved hero called Siegfried. In this paper, I make the case that Siegfried is the autonomous agent we refer to in the world of artificial intelligence. Arguably, by observing Siegfried's actions and understanding the symbolism and metaphors in the third opera in The Ring, we can comprehend the future characteristics of autonomous agents. The final opera, Götterdäm- merung, reveals how the autonomous agent will undergo change, develop, and act The “evolutionary agents,” (which I hope to demonstrate is a more appropriate name than “autonomous agents”) will: 1) be allowed (and encouraged) to act without representations; 2) be capable of building theories and creating a world of their own; 3) be able to assume any virtual identity they wish; 4) possess free will; and 5) be capable of developing a moral code and a value system of their own. However, the future prospects for evolutionary agents are not very bright, because emotion and love cannot be programmed for it The final opera of The Ring is called Götterdämmerung which translates to The Twilight of the Gods or, in our world of digital computers. The Twilight of the Computer Gurus. "],"prob":["agent, environment, intelligent, action, interaction, communication, architecture"],"frex":["agent, action, intelligent, paradigm, environment, communication, interface"],"lift":["agent, paradigm, action, virtual, interface, intelligent, interactive"],"score":["agent, intelligent, action, communication, environment, interaction, virtual"],"proportion":["0.02"]}],"topic_no":[27,54]},{"name":["data, resource, process, mobile, compute, application, activity"],"children":[{"name":["mobile, activity, device, access, security, internet, usage"],"size":[1800],"topic_no":[37],"thought_1":["The new communications technologies and the convergence of the internet and the mobile phone create possibilities as never seen before in the world. Mobile phones have now become an indispensable tool for man. Attacks on mobile networks and devices have grown in number and sophistication. The need for highly secure identification and personal verification technologies is becoming apparent. Biometric authentication is gaining significance, with these systems offering several advantages over traditional authentication methods. This paper proposes the incorporation of a 'low quality image' based face recognition method to prevent the loss of mobile phones. The camera in the mobile phone captures the image of the user and verifies it with the authentication list. The recognised users are allowed to access the mobile. Any mismatch can facilitate the automatic blocking of the SIM card and the permanent locking of the mobile phone to prevent the access to confidential data. Copyright "],"thought_2":["The deployment of Internet based applications calls for adequate users management procedures, being online registration a critical element. In this respect, Email Based Identification and Authentication (EBIA) is an outstanding technique due to its usability. However, it does not handle properly some major issues which make it unsuitable for systems where security is of concern. In this work we modify EBIA to propose a protocol for users registration. Moreover, we assess the security properties of the protocol using the automatic protocol verifier ProVerif. Finally, we show that the modifications applied to EBIA are necessary to ensure security since, if they are removed, attacks on the protocol are enabled. Our proposal keeps the high usability features of EBIA, while reaching a reasonable security level for many applications. Additionally, it only requires minor modifications to current Internet infrastructures. "],"prob":["activity, mobile, internet, security, access, device, usage"],"frex":["mobile, activity, device, access, security, internet, usage"],"lift":["mobile, device, usage, access, activity, security, internet"],"score":["mobile, activity, internet, device, security, access, usage"],"proportion":["0.01"]},{"name":["compute, resource, cloud, process, distribute, execution, massive"],"size":[1800],"topic_no":[45],"thought_1":["With the current amount of data nowadays, the need for processing power has vastly grown. By relying on CPU processing power, current processing power is depending on the frequency and parallelism of the current CPU device. This means this method will lead to increased power consumption. Current research has shown that by utilize the power of GPU processing power to help CPU to do data processing can compete with parallel CPU processing design but in a more energy-efficient way. The usage of GPU to help CPU on doing general-purpose processing has stimulated the appearance of GPU databases. GPU databases have gained its popularity due to its capabilities to process huge amount of data in seconds. In this paper we have explored the open issues on GPU database and introduce a machine learning model to enhance the GPU memory usage on the system by eliminating unnecessary data processing on GPU as on certain queries, CPU processing still outperforms the GPU processing speed. To achieve this, we develop and implement the proposed approach machine learning algorithm using python 3 languages and OmniSci 4.7 for the database system. The applications are running on Ubuntu Linux environment as the GPU environment and Docker as the CPU environment and the results we find that KNN algorithm performs well for this setup with 0.93 F1-Score value. "],"thought_2":["Cloud computing has now been introduced in organisations all around the globe. With the developing prevalence of grid and distributed computing, it has become incredibly important to maintain security and trust. Researchers have now begun concentrating on mining information in cloud computing and have begun distinguishing the basic factor of moral trust. Moral angles in the cloud rely upon the application and the present conditions. Data mining is a procedure for distinguishing the most significant data from a lot of irregular information. In this paper, a three phased methodology is adopted, involving machine learning techniques to discover the most important parameter on which trust is based in the cloud environment. The methodology was then implemented on data sets, proving privacy is the most important factor to calculate ethical trust in cloud computing. The results can be employed in real cloud environments to establish trust as service providers can now consider privacy as the main issue in this relatively new distributed computing environment. Copyright "],"prob":["process, resource, compute, data, cloud, application, distribute"],"frex":["compute, resource, cloud, process, distribute, execution, massive"],"lift":["cloud, compute, execution, resource, massive, distribute, data-driven"],"score":["cloud, process, compute, resource, data, distribute, execution"],"proportion":["0.01"]}],"topic_no":[37,45]},{"name":["knowledge, expert, acquisition, domain, knowledge_based, describe, acquire"],"size":[1800],"topic_no":[64],"thought_1":["Much of today's organizational knowledge still exists outside of formal information repositories and often only in people's heads. While organizations are eager to capture this knowledge, existing acquisition methods are not up to the task. Neither traditional artificial intelligence-based approaches nor more recent, less-structured knowledge management techniques have overcome the knowledge acquisition challenges. This article investigates knowledge acquisition bottlenecks and proposes the use of collaborative, conversational knowledge management to remove them. The article demonstrates the opportunity for more effective knowledge acquisition through the application of the principles of Bazaar style, open-source development. The article introduces wikis as software that enables this type of knowledge acquisition. It empirically analyzes the Wikipedia to produce evidence for the feasibility and effectiveness of the proposed approach. Copyright "],"thought_2":["The issue of expert and experiential knowledge has received increasing attention in conjunction with the contemporary surge in interest in post-positivist approaches in planning and policy-making. So far, few concrete methods have been put forward on how to evaluate these two types of knowledge in specific policy-making situations and thereby achieve their balanced use. This paper presents two approaches that can be used to derive expert and experiential knowledge. It describes their application to two case studies, one in Sweden and one in Italy, where expert and experiential knowledge were generated and used in urban development plans. It also proposes a method for evaluating and integrating the two types of knowledge. "],"prob":["knowledge, expert, domain, process, describe, acquisition, knowledge_based"],"frex":["knowledge, expert, acquisition, domain, knowledge_based, describe, acquire"],"lift":["acquisition, knowledge_based, knowledge, expert, acquire, domain, generic"],"score":["acquisition, knowledge, expert, knowledge_based, domain, process, reason"],"proportion":["0.02"]},{"name":["work, task, job, worker, machine, time, fit"],"size":[1800],"topic_no":[39],"thought_1":["The capability of AI is currently expanding beyond mechanical and repetitive to analytical and thinking. A “Feeling Economy” is emerging, in which AI performs many of the analytical and thinking tasks, and human workers gravitate more toward interpersonal and empathetic tasks. Although these people-focused tasks have always been important to jobs, they are now becoming more important to an unprecedented degree. To manage more effectively in the Feeling Economy, managers must adapt the nature of jobs to compensate for the fact that many of the analytical and thinking tasks are increasingly being performed by AI, and, thus, human workers must place increased emphasis on the empathetic and emotional dimensions of their work. "],"thought_2":["Much Workload Control research has focussed on the order release stage but failed to address practical considerations that impact practical application. Order release mechanisms have been developed through simulations that neglect job size variation effects while empirical evidence suggests groups of small/large jobs are often found in practice. When job sizes vary, it is difficult to release all jobs effectivelysmall jobs favour a short period between releases and a tight workload bounding while large jobs require a longer period between releases and a slacker workload bounding. This paper represents a return from a case study setting to theory building. Through simulation, the impact of job sizes on overall performance is explored using all three aggregate load approaches. Options tested include: using distinct load capacities for small/large jobs and prioritising based on job size or routing length. Results suggest the best solution is assigning priority based on routing length; this improved performance, especially for large jobs, and allowed a short release period to be applied, as favoured by small jobs. These ideas have also been applied to a second practical problem: how to handle rush orders. Again, prioritisation, given to rush orders, leads to the best overall shop performance. "],"prob":["work, task, time, machine, job, worker, numb"],"frex":["work, task, job, worker, machine, time, fit"],"lift":["job, worker, work, task, fit, time, machine"],"score":["job, work, task, worker, machine, time, shop"],"proportion":["0.01"]},{"name":["question, answer, community, response, raise, primary, reason"],"size":[1800],"topic_no":[33],"thought_1":["The problem of pollsters is addressed which is to forecast accurately the final answers of the undecided respondents to the primary question in a public opinion poll. The task is viewed as a pattern-recognition problem of correlating the answers of the respondents to the peripheral questions in the survey with their primary answers. The underlying pattern is determined with a supervised artificial neural network that is trained using the peripheral answers of the decided respondents whose primary answers are also known. With peripheral answers as inputs, the trained network outputs the most probable primary response of an undecided respondent. For a poll conducted to determine the approval rating of the (former) Philippine president, J. E. Estrada in December 1999 and March 2000, the trained network predicted with a 95% success rate the direct responses of a test population that consists of 24.57% of the decided population who were excluded in the network training set. For the undecided population (22.67% of December respondents; 23.67% of March respondents), the network predicted a final response distribution that is consistent with the approval/disapproval ratio of the decided population. Copyright "],"thought_2":["The question answering system can answer questions from various fields and forms with deep neural networks, but it still lacks effective ways when facing multiple evidences. We introduce a new model called SRQA, which means Synthetic Reader for Factoid Question Answering. This model enhances the question answering system in the multi-document scenario from three aspects: model structure, optimization goal, and training method, corresponding to Multilayer Attention (MA), Cross Evidence (CE), and Adversarial Training (AT) respectively. First, we propose a multilayer attention network to obtain a better representation of the evidences. The multilayer attention mechanism conducts interaction between the question and the passage within each layer, making the token representation of evidences in each layer takes the requirement of the question into account. Second, we design a cross evidence strategy to choose the answer span within more evidences. We improve the optimization goal, considering all the answers’ locations in multiple evidences as training targets, which leads the model to reason among multiple evidences. Third, adversarial training is employed to high-level variables besides the word embedding in our model. A new normalization method is also proposed for adversarial perturbations so that we can jointly add perturbations to several target variables. As an effective regularization method, adversarial training enhances the model's ability to process noisy data. Combining these three strategies, we enhance the contextual representation and locating ability of our model, which could synthetically extract the answer span from several evidences. We perform SRQA on the WebQA dataset, and experiments show that our model outperforms the state-of-the-art models (the best fuzzy score of our model is up to 78.56%, with an improvement of about 2%). "],"prob":["response, question, community, answer, reason, primary, raise"],"frex":["question, answer, community, response, raise, primary, reason"],"lift":["answer, question, raise, primary, community, response, reason"],"score":["answer, question, community, response, primary, reason, raise"],"proportion":["0.01"]},{"name":["intelligence, public, potential, robot, standard, automate, human"],"children":[{"name":["human, robot, intelligence, robotic, automation, shape, future"],"size":[1800],"topic_no":[4],"thought_1":["Many ethicists writing about automated systems (e.g. self-driving cars and autonomous weapons systems) attribute agency to these systems. Not only that; they seemingly attribute an autonomous or independent form of agency to these machines. This leads some ethicists to worry about responsibility-gaps and retribution-gaps in cases where automated systems harm or kill human beings. In this paper, I consider what sorts of agency it makes sense to attribute to most current forms of automated systems, in particular automated cars and military robots. I argue that whereas it indeed makes sense to attribute different forms of fairly sophisticated agency to these machines, we ought not to regard them as acting on their own, independently of any human beings. Rather, the right way to understand the agency exercised by these machines is in terms of human–robot collaborations, where the humans involved initiate, supervise, and manage the agency of their robotic collaborators. This means, I argue, that there is much less room for justified worries about responsibility-gaps and retribution-gaps than many ethicists think. "],"thought_2":["We need an account of corporate agency that is temporally robust—one that will help future people to cope with challenges posed by corporate groups in a range of credible futures. In particular, we need to bequeath moral resources that enable future people to avoid futures dominated by corporate groups that have no regard for human beings. This paper asks how future philosophers living in broken or digital futures might re-imagine contemporary debates about corporate agency. It argues that the only temporally robust account is moralised extreme collectivism, where full moral personhood is accorded (only) to those corporate groups that are reliably disposed to respond appropriately to moral reasons. "],"prob":["human, robot, intelligence, future, robotic, automation, shape"],"frex":["human, robot, intelligence, robotic, automation, shape, future"],"lift":["robot, human, robotic, intelligence, automation, shape, replace"],"score":["robot, human, robotic, intelligence, automation, shape, future"],"proportion":["0.01"]},{"name":["automate, public, principle, potential, explain, standard, law"],"size":[1800],"topic_no":[42],"thought_1":["The aim of this paper is to analyse the very recently approved national Member States’ laws that have implemented the GDPR in the field of automated decision-making (prohibition, exceptions, safeguards): all national legislations have been analysed and in particular 9 Member States Law address the case of automated decision making providing specific exemptions and relevant safeguards, as requested by Article 22(2)(b) of the GDPR (Belgium, The Netherlands, France, Germany, Hungary, Slovenia, Austria, the United Kingdom, Ireland). The approaches are very diverse: the scope of the provision can be narrow (just automated decisions producing legal or similarly detrimental effects) or wide (any decision with a significant impact) and even specific safeguards proposed are very diverse. After this overview, this article will also address the following questions: are Member States free to broaden the scope of automated decision-making regulation? Are ‘positive decisions’ allowed under Article 22, GDPR, as some Member States seem to affirm? Which safeguards can better guarantee rights and freedoms of the data subject? In particular, while most Member States refers just to the three safeguards mentioned at Article 22(3) (i.e. subject's right to express one's point of view; right to obtain human intervention; right to contest the decision), three approaches seem very innovative: a) some States guarantee a right to legibility/explanation about the algorithmic decisions (France and Hungary); b) other States (Ireland and United Kingdom) regulate human intervention on algorithmic decisions through an effective accountability mechanism (e.g. notification, explanation of why such contestation has not been accepted, etc.); c) another State (Slovenia) require an innovative form of human rights impact assessments on automated decision-making. "],"thought_2":["The deployment of artificial intelligence on automated platforms needs to go hand in hand with the development of a legal framework safeguarding socio-ethical values as well as fundamental rights, particularly the self-determination and the non-discrimination principle. A trust-based approach focused on human values can mitigate a potential clash between a solely market- and technology-oriented use of artificial intelligence and a more inclusive multistakeholder approach. The regulatory tools are to be designed in a manner that leads to a symbiotic relationship between ethics and law. "],"prob":["potential, automate, public, standard, principle, challenge, explain"],"frex":["automate, public, principle, potential, explain, standard, law"],"lift":["law, principle, automate, public, explain, argue, agency"],"score":["law, automate, public, principle, potential, argue, explain"],"proportion":["0.02"]}],"topic_no":[4,42]}],"topic_no":[55,24,51,68,15,66,1,47,27,54,37,45,64,39,33,4,42]},{"name":["risk, decision, market, quality, product, factor, develop"],"children":[{"name":["factor, project, market, risk, product, decision, strategy"],"children":[{"name":["select, evaluation, process, decision_support, selection, decision, decision_make"],"children":[{"name":["decision_support, decision, decision_support_system, decision_make, support, maker, aid"],"size":[1800],"topic_no":[9],"thought_1":["Understanding the culture of decisions in an organizational context is imperative for the successful design and delivery of decision support systems. The cultural paradigm is especially important in the context of strategic decisions. This paper develops a framework for emphasizing the cultural distinctions around decision processes that semantically articulates those distinctions through four culturally-embedded decision modes: making a decision, taking a decision, baking a decision, and faking a decision. The paper then articulates the multidimensionality of decisions around the types of actors, the reach and range of the decision, and the decision's central metaphor. Using those dimensions, the paper shows the implications of those four decision modes for the design and delivery of decision support systems. This is illustrated by an example of an enterprise using DSS/EISfor strategic decision making. The paper concludes with the observation that the critical leverage points for the successful design and delivery of DSS may be different depending on the dominant decision culture. "],"thought_2":["Decision problems at the strategic level tend to have multiple criteria and outcomes that are uncertain. Many of the current decision-making tools are too simplistic to incorporate the important features. This paper considers a multicriteria decision-making scenario in which the outcomes of the decisions, evaluated on different criteria, are uncertain. The main contribution of this paper is the presentation of a tool that enables decision makers to visualize the expected payoff and likelihood that the payoff of a decision does not fall short of a preset target value. Furthermore, it presents decision makers with a tool that shows the tradeoff between expected payoff and downside risk. A variety of solution techniques are suggested that build upon this visualization. "],"prob":["decision, decision_support, decision_make, decision_support_system, process, support, information"],"frex":["decision_support, decision, decision_support_system, decision_make, support, maker, aid"],"lift":["decision_support_system, decision_support, decision, decision_make, maker, aid, assist"],"score":["decision_support_system, decision, decision_support, decision_make, maker, aid, process"],"proportion":["0.02"]},{"name":["selection, evaluation, criterion, supplier, select, choose, quantitative"],"size":[1800],"topic_no":[22],"thought_1":["Multiple Criteria Decision Making (MCDM) methods generally require the decision maker to evaluate alternatives with respect to decision criteria and also to assign importance weightings to the criteria. Then, based on the assigned weightings, the best alternative can be selected. However, after a decision is made it often happens that the decision maker becomes doubtful whether the right weightings have been assigned to the criteria given that a variety of eventualities may occur in the near future. The main aim of this paper is to address this concern and improve the application of MCDM methods by addressing possible fluctuations in the criteria weightings. The recently proposed concept of stratification (CST) is used in conjunction with MCDM methods to stratify the decision environment. The method is then applied to a supplier selection problem. The stratified MCDM (SMCDM) approach is in its early stages only and requires further research to reach its maturity. "],"thought_2":["This paper proposes a multi-attribute comprehensive evaluation method of individual research output (IRO). It highlights the fact that a single index can never give more than a rough approximation to IRO, and the evaluation of IRO is a multi-attribute complex problem. Firstly, an evaluation index system is established by determining evaluation attributes and choosing the appropriate bibliometric indicators. To address the multiple authorship problem, this paper develops an improved number-of-papers-published indicator. Following this, TOPSIS method is used to conduct a comprehensive IRO evaluation. Then this paper uses a case study to test the feasibility of the methodology. Finally, this paper discusses the effectiveness of the proposed method. Compared with traditional single-indicator evaluation approaches, the proposed multi-attribute evaluation takes more aspects into consideration, therefore it is able to effectively overcome the one-sidedness of a single indicator. The proposed method also has significant advantages compared with other comprehensive IRO evaluation methods. "],"prob":["evaluation, selection, select, criterion, evaluate, supplier, choose"],"frex":["selection, evaluation, criterion, supplier, select, choose, quantitative"],"lift":["supplier, criterion, selection, evaluation, hierarchy, select, quantitative"],"score":["supplier, selection, criterion, evaluation, select, hierarchy, qualitative"],"proportion":["0.01"]}],"topic_no":[9,22]},{"name":["set, rule, measure, fuzzy, alternative, attribute, rank"],"children":[{"name":["attribute, preference, alternative, measure, conflict, rank, association"],"size":[1800],"topic_no":[40],"thought_1":["Measuring consistency of preferences is very important in decision-making. This paper addresses this key issue for interval-valued reciprocal preference relations. Existing studies implement one of two different measures: the “classical” consistency measure, and the “boundary” consistency measure. The classical consistency degree of an interval-valued reciprocal preference relation is determined by its associated reciprocal preference relation with highest consistency degree, while the boundary consistency degree is determined by its two associated boundary reciprocal preference relations. However, the consistency index of an interval-valued reciprocal preference relation should be determined by taking into account all its associated reciprocal preference relations. Motivated by this, a new consistency measure for interval-valued reciprocal preference relations, the average-case consistency measure, is suggested and introduced. The new average-case consistency measure of an interval-valued reciprocal preference relation is determined as the average consistency degree of all reciprocal preference relations associated to the interval-valued reciprocal preference relation. Furthermore, the analysis and comparison of the different consistency measure internal mechanisms is used to justify the validity of the average-case consistency measure. Finally, an average-case consistency improving method which aims to obtain a modified interval-valued reciprocal preference relation with a required average consistency degree is developed. "],"thought_2":["Purpose - This paper explores the relationships between body type and fit preferences with body cathexis, clothing benefits sought by consumers, and demographic profiles of consumers. Design/methodology/approach - The survey instrument consisted of a questionnaire with scales assessing fit preference, body type, body cathexis, clothing benefits sought and consumer demographics. Findings - Significant associations were found between body cathexis (satisfaction with head/upper body, lower body, height, weight and torso) and body shape. The degree of satisfaction with different body parts depended on the body type of the individual. The level of satisfaction with head/upper body, height and torso did not vary by body type. No significant differences were found between fit preferences and body type for lower body garments. Research limitations/implications - The majority of respondents were between the ages 18 and 28, affluent Caucasian Americans, with an hourglass body type, who had a family income of $85,000 or more and shopped in department or boutique/specialty stores. Originality/value - Understanding the fit preferences of female consumers could help apparel companies to produce and meet demands for comfortable and well fitting clothes for women. The results of this research may be used as a first step to develop an expert system to correlate body shape and fit preferences of consumers. "],"prob":["measure, attribute, alternative, rank, preference, set, conflict"],"frex":["attribute, preference, alternative, measure, conflict, rank, association"],"lift":["preference, attribute, conflict, alternative, association, measure, rank"],"score":["preference, measure, attribute, rank, alternative, conflict, association"],"proportion":["0.01"]},{"name":["fuzzy, rule, inference, fuzzy_logic, uncertainty, rule-based, set"],"size":[1800],"topic_no":[52],"thought_1":["A new approach to the rule-base evidential reasoning based on the synthesis of fuzzy logic, Atannasov's intuitionistic fuzzy sets theory and the Dempster-Shafer theory of evidence is proposed. It is shown that the use of intuitionistic fuzzy values and the classical operations on them directly may provide counter-intuitive results. Therefore, an interpretation of intuitionistic fuzzy values in the framework of Dempster-Shafer theory is proposed and used in the evidential reasoning. The merits of the proposed approach are illustrated with the use of developed expert systems for diagnostics of type 2 diabetes. Using the real-world examples, it is shown that such an approach provides reasonable and intuitively obvious results when the classical method of rule-base evidential reasoning cannot produce any reasonable results. "],"thought_2":["Hesitant fuzzy sets are very useful to deal with group decision making problems when experts have a hesitation among several possible memberships for an element to a set. During the evaluating process in practice, however, these possible memberships may be not only crisp values in [0, 1], but also interval values. In this study, we extend hesitant fuzzy sets by intuitionistic fuzzy sets and refer to them as generalized hesitant fuzzy sets. Zadeh's fuzzy sets, intuitionistic fuzzy sets and hesitant fuzzy sets are special cases of the new fuzzy sets. We redefine some basic operations of generalized hesitant fuzzy sets, which are consistent with those of hesitant fuzzy sets. Some arithmetic operations and relationships among them are discussed as well. We further introduce the comparison law to distinguish two generalized hesitant fuzzy sets according to score function and consistency function. Besides, the proposed extension principle enables decision makers to employ aggregation operators of intuitionistic fuzzy sets to aggregate a set of generalized hesitant fuzzy sets for decision making. The rationality of applying the proposed techniques is clarified by a practical example. At last, the proposed techniques are devoted to a decision support system. "],"prob":["fuzzy, rule, set, uncertainty, inference, fuzzy_logic, rule-based"],"frex":["fuzzy, rule, inference, fuzzy_logic, uncertainty, rule-based, set"],"lift":["fuzzy, rule, fuzzy_logic, rule-based, inference, uncertainty, uncertain"],"score":["fuzzy, rule, inference, uncertainty, fuzzy_logic, rule-based, set"],"proportion":["0.01"]}],"topic_no":[40,52]},{"name":["construction, project, develop, risk, assessment, cost, safety"],"children":[{"name":["project, construction, success, cost, stage, stakeholder, phase"],"size":[1800],"topic_no":[29],"thought_1":["The accurate prediction of the duration of a construction project represents a critical factor for the feasibility study of this project. Employers are in an urgent need for reliable information about the construction duration in this early stage of the project. Such information can materially help project managers create a cash and material flow plan in a pre-set time. This paper aims to develop an artificial neural network (ANN) model for predicting the expected construction duration of building projects in its early stage, where no detailed planning is available. The MATLAB program was used as a suitable environment for developing the proposed model. The required field data was collected from 130 building projects in Egypt, which fall within the appropriate sample size. Testing the validity of the model clearly showed that it has a good prediction capability with a maximum error of 14%. Copyright "],"thought_2":["Cost estimate at pre-Tender stage of subsurface drainage projects could determine whether a project is dropped or continued. Forty one factors that are expected to greatly influence the cost estimate of subsurface drainage at pre-Tender stage were identified through a comprehensive literature review. A questionnaire survey was agreed out through 68 qualified subsurface engineers in Egypt, to get the most important cost factors. Through this survey, 12 factors were only considered as the most important cost factors. The neural power program was found appropriate for the development of the suggested model. The desired field information grouped from 61 subsurface drainage projects in Egypt. The best building of the suggested model was identified with RMS = 0.0405 and max absolute error = 4.01%. Testing the validity of the model clearly showed that it has a good prediction capability with a maximum error of 3.51%. "],"prob":["project, construction, cost, success, develop, build, stage"],"frex":["project, construction, success, cost, stage, stakeholder, phase"],"lift":["project, construction, success, stakeholder, progress, site, questionnaire"],"score":["project, construction, cost, success, stakeholder, site, factor"],"proportion":["0.01"]},{"name":["risk, assessment, safety, assess, mitigate, bayesian, event"],"size":[1800],"topic_no":[44],"thought_1":["Purpose - Globally expanding supply chains (SCs) have grown in complexity increasing the nature and magnitude of risks companies are exposed to. Effective methods to identify, model and analyze these risks are needed. Risk events often influence each other and rarely act independently. The SC risk management practices currently used are mostly qualitative in nature and are unable to fully capture this interdependent influence of risks. The purpose of this paper is to present a methodology and tool developed for multi-tier SC risk modeling and analysis. Design/methodology/approach - SC risk taxonomy is developed to identify and document all potential risks in SCs and a risk network map that captures the interdependencies between risks is presented. A Bayesian Theory-based approach, that is capable of analyzing the conditional relationships between events, is used to develop the methodology to assess the influence of risks on SC performance Findings - Application of the methodology to an industry case study for validation reveals the usefulness of the Bayesian Theory-based approach and the tool developed. Back propagation to identify root causes and sensitivity of risk events in multi-tier SCs is discussed. Practical implications - SC risk management has grown in significance over the past decade. However, the methods used to model and analyze these risks by practitioners is still limited to basic qualitative approaches that cannot account for the interdependent effect of risk events. The method presented in this paper and the tool developed demonstrates the potential of using Bayesian Belief Networks to comprehensively model and study the effects or SC risks. The taxonomy presented will also be very useful for managers as a reference guide to begin risk identification. Originality/value - The taxonomy developed presents a comprehensive compilation of SC risks at organizational, industry, and external levels. A generic, customizable software tool developed to apply the Bayesian approach permits capturing risks and the influence of their interdependence to quantitatively model and analyze SC risks, which is lacking. Copyright "],"thought_2":["Reliable and efficient risk assessments are essential to deal effectively with potential risks in international construction projects. However, most conventional risk modeling methods are based on the hypothesis that risk factors are independent, which does not account adequately for the causal relationships among risk factors. In this study, a risk assessment model for international construction projects was developed to improve the efficacy of risk management by integrating fault tree analysis and fuzzy set theory with a Bayesian belief network. The risk rating of each risk factor, expressed as the product of risk occurrence probability and impact, was incorporated into the risk assessment model to evaluate degrees of risk. Therefore, risk factors were categorized into different risk levels taking into account their inherent causal relationships, which allowed the identification of critical risk factors. The applicability of the fuzzy Bayesian belief network-based risk assessment model was verified using a case study through a comparative analysis with the results from a fuzzy synthetic evaluation method. The comparison shows that the proposed risk assessment model is able to provide guidelines for an effective risk management process and ultimately to increase project performance in a complex environment such as international construction projects. "],"prob":["risk, assessment, safety, assess, develop, identify, level"],"frex":["risk, assessment, safety, assess, mitigate, bayesian, event"],"lift":["risk, safety, assessment, mitigate, assess, prevent, quantify"],"score":["risk, assessment, safety, assess, bayesian, event, probability"],"proportion":["0.01"]}],"topic_no":[29,44]},{"name":["account, estimation, develop, procedure, individual, estimate, productivity"],"children":[{"name":["account, estimate, estimation, claim, individual, procedure, stochastic"],"size":[1800],"topic_no":[11],"thought_1":["We present an actuarial claims reserving technique that takes into account both claim counts and claim amounts. Separate (overdispersed) Poisson models for the claim counts and the claim amounts are combined by a joint embedding into a neural network architecture. As starting point of the neural network calibration, we use exactly these two separate (overdispersed) Poisson models. Such a nested model can be interpreted as a boosting machine. It allows us for joint modeling and mutual learning of claim counts and claim amounts beyond the two individual (overdispersed) Poisson models. "],"thought_2":["The aim of this project is to develop a stochastic simulation machine that generates individual claims histories of non-life insurance claims. This simulation machine is based on neural networks to incorporate individual claims feature information. We provide a fully calibrated stochastic scenario generator that is based on real non-life insurance data. This stochastic simulation machine allows everyone to simulate their own synthetic insurance portfolio of individual claims histories and back-test thier preferred claims reserving method. "],"prob":["estimate, account, individual, estimation, procedure, claim, real"],"frex":["account, estimate, estimation, claim, individual, procedure, stochastic"],"lift":["claim, account, estimation, estimate, individual, stochastic, procedure"],"score":["claim, estimate, estimation, account, individual, stochastic, procedure"],"proportion":["0.01"]},{"name":["productivity, collect, current, develop, historical, lack, level"],"size":[1800],"topic_no":[71],"thought_1":["Labor productivity is a fundamental piece of information for estimating and scheduling a construction project. The current practice of labor productivity estimation relies primarily on either published productivity data or an individual's experience. There is a lack of a systematic approach to measuring and estimating labor productivity. Although historical project data hold important predictive productivity information, the lack of a consistent productivity measurement system and the low quality of historical data may prevent a meaningful analysis of labor productivity. In response to these problems, this paper presents an approach to measuring productivity, collecting historical data, and developing productivity models using historical data. This methodology is applied to model steel drafting and fabrication productivities. First, a consistent labor productivity measurement system was defined for steel drafting and shop fabrication activities. Second, a data acquisition system was developed to collect labor productivity data from past and current projects. Finally, the collected productivity data were used to develop labor productivity models using such techniques as artificial neural network and discrete-event simulation. These productivity models were developed and validated using actual data collected from a steel fabrication company. "],"thought_2":["Purpose: Currently, there is a dearth of research studies regarding macro analysis of the workforce productivity of the US construction industry. The purpose of this paper is to calculate the workforce productivity changes of the US construction industry from 2006 to 2016, with the number of laborers as input and value of construction industry as output. Design/methodology/approach: The present study introduced the data envelopment analysis (DEA) based Malmquist productivity index model to measure the workforce productivity of the US construction industry from 2006 to 2016. Findings: The results indicated that the workforce productivity of the US construction industry experienced a continuous decline, except for the increases from 2011 to 2013 and from 2014 to 2015. It was also shown that there were gaps in the workforce productivity development level among all states and nine regions in the US construction industry. Besides, the relationship between workforce productivity and four aspects, including real estate price, workforce, climate distribution and economic factors, was analyzed. Research limitations/implications: The calculation of the productivity of the US construction industry is based on the premise that the external environment is fixed and unchanged from 2006 to 2016, but the multi-level DEA model for further calculation is required for obtaining more effective conclusions. Social implications: This paper measures the workforce productivity of the US construction industry over the past 11 years, which added latest analysis and knowledge into the construction industry, providing decision-makers with advice and data support to formulate policies to improve workforce productivity. Originality/value: This study provided both government decision-makers and industrial practitioners with important macro background environment information, which will facilitate the improvement of workforce productivity in the construction industry in different regions of the US. "],"prob":["productivity, develop, data, collect, current, level, important"],"frex":["productivity, collect, current, develop, historical, lack, level"],"lift":["productivity, historical, collect, lack, consistent, current, rely"],"score":["productivity, data, collect, develop, historical, current, level"],"proportion":["0.01"]}],"topic_no":[11,71]},{"name":["city, scale, spatial, area, water, location, map"],"children":[{"name":["water, area, plant, site, storage, region, locate"],"size":[1800],"topic_no":[18],"thought_1":["The objective of this research study was to evaluate the consequences of climate change on shifts in distributions of plant species and the vulnerability of the species in Peninsular Thailand. A sub-scene of the predicted climate in the year 2100, under the B2a scenario of the Hadley Centre Coupled Model, version 3 (HadCM3), was extracted and calibrated with topographic variables. A machine learning algorithm based on the maximum entropy theory (Maxent) was employed to generate ecological niche models of 66 forest plant species from 22 families. The results of the study showed that altitude was a significant factor for calibrating all 19 bioclimatic variables. According to the global climate data, the temperature in Peninsular Thailand will increase from 26.6 °C in 2008 to 28.7 °C in 2100, while the annual precipitation will decrease from 2253 mm to 2075 mm during the same period. Currently, nine species have suitable distribution ranges in more than 15% of the region, 20 species have suitable ecological niches in less than 10% while the ecological niches of many Dipterocarpus species cover less than 1% of the region. The number of trees gaining or losing climatically suitable areas is quite similar. However, 10 species have a turnover rate greater than 30% of the current distribution range and the status of several species will in 2100 be listed as threatened. Species hotspots are mainly located in large, intact protected forest complexes. However, several landscape indices indicated that the integrity of species hotspots in 2100 will deteriorate significantly due to the predicted climate change. "],"thought_2":["Huangshui River Valley (HRV) is a typical valley area located in the northeast of the Qinghai-Tibetan plateau. Its long and narrow space, large vertical gradient and diverse land use types bring great challenges to the land use zoning in this region. The paper aims to propose a gridding-self-organizing feature maps (SOFM) coupled method based on grid cell, which will be more reliable and refined, if compared to the method of commonly used administrative regions as the basic analytic units. Firstly the focused study area is divided into 500 m × 500 m grid cells. Six indexes, including grassland, cultivated land, forest land, construction land, population density and per capita income are both spatialized and quantified in every grid. Meanwhile, that SOFM neural network model is at the usage of classifying the land use into six zoning areas can be certain. And urban area, urban-rural transition area, agriculture-forestry transition area, typical agricultural area, agriculture-pastoral transition area, and typical pastoral area are sure to belong to them. Its results can vividly indicate the actual situation where land use pattern is in constant changes in Huangshui River Valley (HRV). The research will provide a specific method for the substantial utilization of land resources. "],"prob":["area, water, plant, site, high, storage, region"],"frex":["water, area, plant, site, storage, region, locate"],"lift":["water, plant, area, site, storage, locate, region"],"score":["water, area, plant, site, storage, region, temperature"],"proportion":["0.01"]},{"name":["map, location, road, scale, spatial, city, urban"],"size":[1800],"topic_no":[65],"thought_1":["Background: The rapid and often uncontrolled rural-urban migration in Sub-Saharan Africa is transforming urban landscapes expected to provide shelter for more than 50% of Africa's population by 2030. Consequently, the burden of malaria is increasingly affecting the urban population, while socio-economic inequalities within the urban settings are intensified. Few studies, relying mostly on moderate to high resolution datasets and standard predictive variables such as building and vegetation density, have tackled the topic of modeling intra-urban malaria at the city extent. In this research, we investigate the contribution of very-high-resolution satellite-derived land-use, land-cover and population information for modeling the spatial distribution of urban malaria prevalence across large spatial extents. As case studies, we apply our methods to two Sub-Saharan African cities, Kampala and Dar es Salaam. Methods: Openly accessible land-cover, land-use, population and OpenStreetMap data were employed to spatially model Plasmodium falciparum parasite rate standardized to the age group 2-10 years (PfPR2-10) in the two cities through the use of a Random Forest (RF) regressor. The RF models integrated physical and socio-economic information to predict PfPR2-10 across the urban landscape. Intra-urban population distribution maps were used to adjust the estimates according to the underlying population. Results: The results suggest that the spatial distribution of PfPR2-10 in both cities is diverse and highly variable across the urban fabric. Dense informal settlements exhibit a positive relationship with PfPR2-10 and hotspots of malaria prevalence were found near suitable vector breeding sites such as wetlands, marshes and riparian vegetation. In both cities, there is a clear separation of higher risk in informal settlements and lower risk in the more affluent neighborhoods. Additionally, areas associated with urban agriculture exhibit higher malaria prevalence values. Conclusions: The outcome of this research highlights that populations living in informal settlements show higher malaria prevalence compared to those in planned residential neighborhoods. This is due to (i) increased human exposure to vectors, (ii) increased vector density and (iii) a reduced capacity to cope with malaria burden. Since informal settlements are rapidly expanding every year and often house large parts of the urban population, this emphasizes the need for systematic and consistent malaria surveys in such areas. Finally, this study demonstrates the importance of remote sensing as an epidemiological tool for mapping urban malaria variations at large spatial extents, and for promoting evidence-based policy making and control efforts. "],"thought_2":["Many studies have explored the relationship between population density and obesity, but there is no consensus, particularly in dense Chinese cities. This study applied gradient boosting decision trees to 2014 national survey data to examine the non-linear or threshold effects of population density at both local and regional levels on waist-hip ratio (WHR), controlling for other built environment elements and socio-demographics. Built environment elements collectively have a stronger predictive power than socio-demographics (56.6% vs. 43.4%). Within a certain range, regional population density is negatively associated with WHR, but its marginal effect diminishes beyond the upper threshold. Local population density has a U-shaped relationship with WHR. These results suggest that urban planners can alleviate the risk of obesity through population densification, but over-densification tends to be inefficient, and sometimes counterproductive. "],"prob":["map, spatial, scale, location, city, urban, road"],"frex":["map, location, road, scale, spatial, city, urban"],"lift":["road, map, location, urban, city, scale, spatial"],"score":["road, map, spatial, urban, city, location, scale"],"proportion":["0.01"]}],"topic_no":[18,65]},{"name":["country, tourism, sector, government, tourist, economy, travel"],"size":[1800],"topic_no":[17],"thought_1":["The aim of this paper is to analyse the state of the art of the Spanish rural tourism sector, as well as performing forecasts for this strategically important sector of Spanish economy. Section 1 of the paper describes rural tourism in Spain, while in Section 2 three time series belonging to this sector are analysed, and then forecasts are calculated by applying Box-Jenkins and Artificial Neural Nets methodologies. Finally, the paper summarises major conclusions and implications for policy makers and managers involved in rural tourism in Spain. "],"thought_2":["As an industry, tourism tends to be extremely responsive and vulnerable to political instabilities. Recently, a political conflict occurred in Spain, a leader in international tourism. In October 2017, the regional parliament of Catalonia asserted its independence from Spain, engendering a negative impact on the tourism sector of Catalonia. The main goal of our study is to assess the economic impact of the Catalan separatist challenge on the region’s tourism sector during the last quarter of 2017. To this end, we conducted a counterfactual analysis, based on forecasts generated by a seasonal autoregressive moving average model and an artificial neural network. The forecasts allowed us to calculate the projected number of international and domestic tourist visitors that would have travelled to Catalonia, had the separatist challenge not occurred. According to our results, the Catalan tourist sector effectively forfeited close to €200 million in revenue from the international tourism market, and around €27 million in revenue from the domestic market. These amounts differ from the economic gains attained by the other Spanish Mediterranean regions that compete with Catalonia to attract tourists. "],"prob":["country, sector, tourism, government, develop, economy, tourist"],"frex":["country, tourism, sector, government, tourist, economy, travel"],"lift":["tourism, tourist, country, government, sector, economy, national"],"score":["tourism, country, tourist, sector, government, travel, economy"],"proportion":["0.01"]},{"name":["economic, indicator, impact, environmental, factor, development, enterprise"],"children":[{"name":["growth, emission, factor, china, consumption, affect, percentage"],"size":[1800],"topic_no":[21],"thought_1":["Carbon emissions in China have attracted increasing world attention with rapid urbanization of this country. It is critical for the government to identify the key factors causing these emissions and take controlling measures. Consistent results have not been achieved yet although some research has been conducted on the factors leading to emissions. Meanwhile, there is still considerable room to improve the methods of previous research. Index decomposition analysis (IDA) is the main method for quantifying the impact of different factors on carbon emissions. At present, the widely used forms of IDA are primarily the Laspeyres and the Divisia index methods. Compared with the Laspeyres and the majority of the Divisia index methods, the generalized Fisher index (GFI) decomposition method can eliminate the residuals and has better factor decomposition characteristics. This paper chooses Beijing as a typical example and analyzes the factors causing carbon emissions. Based on the extended Kaya identity, we built a multivariate generalized Fisher index decomposition model to measure the impacts of economic growth, population size, energy intensity and energy structure on energy-related carbon emissions from 1995 to 2012 in Beijing. The results show that the sustained growth of economic output in Beijing was the leading factor in carbon emissions. Population size had a stimulating effect on the growth of carbon emissions during this period; the pulling effect increased after 2003 and then decreased slightly after 2011 with a cumulative effect of 165.4%. Energy intensity was the primary factor restraining carbon emissions, and the inhibition effect increased yearly. The continuous optimization of the energy structure had no obvious inhibitory effect on carbon emissions. To control carbon emissions, Beijing should continue to adjust the mode of economic development and appropriately control the population size while improving energy efficiency. "],"thought_2":["With the development of China's economy, the use of fossil energy has become more and more, resulting in increasing carbon emissions. CO2 emissions have caused global warming, threatening humans and creatures on Earth. In order to effectively suppress the growth of carbon emissions, it is necessary to analyze the influencing factors of carbon emissions and apply them to predict carbon emissions. This paper presents sixteen potential influencing factors and uses grey relational analysis to identify the factors that have a strong correlation with carbon emissions. The principal component analysis (PCA) is used to extract the four principal components, which reduce the redundancy of the input data. The long short-term memory (LSTM) method is established to predict carbon emissions in China. We use back propagation neural network (BPNN) and Gaussian process regression (GPR) to compare LSTM method. The simulation results show that the prediction accuracy of carbon emissions based on LSTM is better than that of BPNN and GPR, indicating the effectiveness of PCA and LSTM in prediction of carbon emissions. Finally, this paper provides the theoretical basis for China to reduce carbon emissions by studying prediction of carbon emissions. "],"prob":["factor, growth, emission, consumption, china, increase, affect"],"frex":["growth, emission, factor, china, consumption, affect, percentage"],"lift":["emission, growth, china, factor, consumption, percentage, gas"],"score":["emission, factor, growth, consumption, china, economic, policy"],"proportion":["0.01"]},{"name":["environmental, enterprise, sustainability, indicator, economic, life, cycle"],"size":[1800],"topic_no":[48],"thought_1":["Life Cycle Assessment (LCA) methodology was used to estimate the environmental impacts and identify the most critical stages (hotspots) of cultivation of three cereal crops typically used for animal feed purposes – barley, rye and sorghum – in the Lombardy region, the most productive crop and livestock area in Northern Italy. The crop variety (out of 3 and 4 varieties of barley and rye, respectively) and cultivation regime (single vs. double cropping of sorghum) with the lowest impacts per kg of crude protein (mass-based functional unit) were identified. Environmental impact categories reported by ReCiPe method were used. According to the results, both Reni and Dank Nowe were the varieties with the lowest environmental impacts for barley and rye varieties, respectively; single cropping of sorghum had lower impacts than double cropping. Impact hotspots included field emissions, agricultural activities and agrochemical (fertilisers and herbicides) production regardless the cropping system considered. Moreover, among the cereals studies, rye was identified as the best environmental alternative. Use of land-based and economic functional units did not change the ranking of systems according to their impacts. "],"thought_2":["Sustainability of food consumption requires the understanding of multi-dimensional environmental, economic and social impacts using a holistic and integrated sustainability assessment and modeling framework. This article presents a novel method on the assessment and modeling of sustainability impacts of food consumption. First, sustainability impacts of food consumption categories are quantified using high sector resolution input-output tables of U.S. economy. Later, an integrated sustainability modeling framework based on two supervised machine-learning techniques such as k-means clustering and logistics regression is presented. The proposed framework involves five steps: (1) economic input-output life cycle sustainability assessment, (2) non-dimensional normalization, (3) sustainability performance evaluation, (4) centroid-based clustering analysis, and (5) sustainability impact modeling. The findings show that the supply chains of food production sectors are accounted for major environmental impacts with higher than 80% of portions for total carbon footprints. Animal slaughtering, rendering, and processing is found as the most dominant sector in most of the environmental impact categories. The logistic model results revealed an overall model accuracy equal to 91.67%. Furthermore, among all the environmental sustainability indicators, it has found that CO and SO2 are the most significant contributors. The results also show that 13.7% of the food and beverage sectors are clustered as high, in which the bread and bakery product manufacturing is the central sector. The large value of the variance (5.24) is attributed to the large total weighted impact value of the animal (except poultry) slaughtering, rendering, and processing cluster. "],"prob":["environmental, economic, indicator, impact, enterprise, life, sustainability"],"frex":["environmental, enterprise, sustainability, indicator, economic, life, cycle"],"lift":["sustainability, cycle, enterprise, environmental, sustainable, life, indicator"],"score":["sustainability, environmental, enterprise, economic, indicator, sustainable, cycle"],"proportion":["0.01"]}],"topic_no":[21,48]},{"name":["volume, online, configuration, consumer, retail, product, sale"],"children":[{"name":["sale, retail, configuration, volume, store, daily, variant"],"size":[1800],"topic_no":[30],"thought_1":["Despite being of increasing strategic importance for firms employing B2B salespeople, the concept of sales enablement, and the initiatives it inspires, has heretofore been unexplored by academic sales researchers. This omission is all the more surprising when considering that recent statistics suggest that 61% of firms employing B2B salespeople engage in sales enablement initiatives. The purpose of this agenda setting piece is to introduce the concept of sales enablement to a broader academic audience, and to outline a research agenda for sales researchers. Based on semi-structured, qualitative interviews with practitioners, we propose a framework that suggests that sales enablement can be best understood as a firm-wide strategic initiative that incorporates the 3 Ps of People, Process, and Performance- to deliver value to seller firms and customers alike. Then, we conclude the manuscript by proposing a set of research questions to encourage further academic research into sales enablement. "],"thought_2":["Purpose - Product configurator is a sales and production-planning tool that helps to transform customer requirements into bills-of-materials, lists of features and cost estimations. The purpose of this paper is to introduce a method of how to analyse sales configuration models by using a design structure matrix (DSM) tool. By applying the DSM techniques, the sales configuration managers may sequence the product configuration questions and organize the connection to production. Design/methodology/approach - First, the paper explains a sales configuration system structure from published academic and non-academic works. These sources employ both theoretical and practical views on the topic of computer-based sales expert systems. Second, the paper demonstrates an application of using DSM for configuration modelling. Findings - The current sales configuration approaches include constraint-based, rules-based, and object-oriented approaches. Product description methods vary, but the general problem remains the same: the configuration process should be designed in such a way that customer selections do not affect the previous selections. From the user point of view, answering the questions should be smooth and fast. In turn this will lead to the growing importance of building more effective product configuration models. DSM offers a systematic way to organise customer interface in sales configuration systems. Research limitations/implications - This paper analyses how DSM could help in planning product configuration modelling. Comparison of different sequences is presented. The examples used are hypothetical, but illustrate the suitability of DSM analysis. Companies are trying to establish easily configured product models, which are fast, flexible and cost-effective for adjustments and modifications. Use of DSM may help in the roll-out of sales configuration projects. DSM may also be used as a quick view to represent the complexity of product configurability. The future needs for configuration tools will be focused towards product model management from the technical limitations of different data storage approaches. Practical implications - Configurator software creates product variants, which are logical descriptions of physical products. Variants have parameters which describe the customer-made selections. The parameter selections may have interconnections between the choices. Some selections may affect further selections and some combinations may not be allowed for incompatibility, cost or safety reasons. There are several commercial software packages available for creating product configurations. Product description methods vary, but the general problem remains the same: the configuration process should be designed in such a way that customer selections do not affect the previous selections. Answering the questions should be smooth and fast. Configuration of complex products, for instance, airplanes, may include several sub-systems and have various loops within the quotation process. The use of DSM may help in the roll-out of sales configuration projects. DSM may also be used as a quick view to represent the complexity of product configurability. Originality/value - The paper helps both researchers and practitioners to obtain a clearer view on the development of sales configuration systems and the potential of systematic DSM-based product model analysis. "],"prob":["sale, retail, volume, configuration, store, data, information"],"frex":["sale, retail, configuration, volume, store, daily, variant"],"lift":["sale, configuration, retail, volume, store, variant, daily"],"score":["sale, retail, volume, configuration, store, data, demand"],"proportion":["0.01"]},{"name":["consumer, product, online, purchase, platform, shop, category"],"size":[1800],"topic_no":[57],"thought_1":["Understanding the process of consumer decision making is important for many decision support systems. Consumers evaluate different alternatives and then come to a decision. Prior research suggests that consumer evaluations leading to choice are comparative in nature and can be affected by other alternatives or reference products. This study proposes a mixtures-of-experts model framework to examine the role of different reference products in consumer choice of multi-attribute products. While multiple external and internal reference points have been proposed, previous studies have very rarely investigated more than one reference point in the same model. Using data from a choice-based conjoint experiment, our empirical model enables us to identify which product consumers tend to use as the reference product by incorporating four different reference products and includes consumer characteristics to examine how consumers differ in their utilization of different reference products. The results show that our model outperforms other reference-dependent models in prior literature. In our empirical context of smartphone choices, the most commonly used reference product is the most preferred product in the choice set, while the least preferred product and the average product are rarely used. We also examine the role of consumer characteristics such as gender, product familiarity, and product interest in utilizing reference products. This paper provides insights into the unobserved comparison process in consumer choice, which can be applied to decision support systems such as recommendation engines. "],"thought_2":["This paper aims to study the role product category plays as a moderating factor in online reviews, by introducing a novel method for product category classification using natural language processing (NLP). The study includes a wide variety of categories, based on a high number of products and number of reviews. The data-set presented includes 1.1 million unique reviews from 4,600 products in 30 different product categories. We find evidence for reviews having an effect on sales, and that this effect interacts with other factors, most notably the product category as well as product popularity. We find that subjectively evaluated products, as well as less popular products see the largest relative effect of WOM. This paper also reveals some evidence of rating biases as 60% of the 1.1 million reviews in our data-set show signs of bimodality. Based on the results we present “the review impact continuum”, a model mapping degree of subjectivity and product popularity enabling managers to assess the expected impact of online consumer reviews for their products. "],"prob":["product, online, consumer, purchase, platform, shop, characteristic"],"frex":["consumer, product, online, purchase, platform, shop, category"],"lift":["consumer, purchase, product, online, shop, platform, distinct"],"score":["consumer, product, online, purchase, shop, platform, experience"],"proportion":["0.01"]}],"topic_no":[30,57]},{"name":["investment, portfolio, asset, allocation, return, capital, investor"],"size":[1800],"topic_no":[19],"thought_1":["This paper presents an incentive scheme to encourage investment in the improvement and expansion of the transmission in the competitive electricity market environment. To create these incentives, a decentralized transmission asset investment model is proposed, where the new assets are built by the investors. The incentives are based on the value added to the social welfare through each asset investment. By viewing each potential investor as a player in a cooperative game the Shapley value is used to reward investors according to the added value that they create. The proposed methodology is applied to the Garver 6-bus system and the IEEE 24-bus Reliability Test System to illustrate the capability and flexibility of the decision support system presented. "],"thought_2":["Investors’ perception of past portfolio returns predicts their investment behavior, but does this relationship mediate by overconfidence? Taking into account different aspects of overconfidence, this paper examines whether overconfidence manifested as illusion of control, miscalibration and better-than-average mediates the association between perception of past portfolio returns and investment behavior. In a survey study with individual and institutional investors from Malaysia, the results indicate that perception of higher past portfolio returns increases investors’ trading, percentage of risky share investment and the number of financial asset holding, through the mediating channel of better-than-average effect. While individual investors are influenced by this overconfidence mechanism, institutional investors are not sensitive. This finding has theoretical implication for overconfidence model, house money effect and naïve reinforcement learning. Practically, the results imply that individual investors should be careful about underlying overconfidence biases as it can lead to inefficient decisions. © 2019, "],"prob":["investment, return, portfolio, allocation, asset, capital, investor"],"frex":["investment, portfolio, asset, allocation, return, capital, investor"],"lift":["portfolio, asset, investment, capital, allocation, return, investor"],"score":["portfolio, investment, return, asset, investor, capital, allocation"],"proportion":["0.01"]},{"name":["index, stock, price, exchange, market, strategy, trade"],"children":[{"name":["stock, exchange, index, indices, movement, investor, price"],"size":[1800],"topic_no":[32],"thought_1":["Predicting intraday stock jumps is a significant but challenging problem in finance. Due to the instantaneity and imperceptibility characteristics of intraday stock jumps, relevant studies on their predictability remain limited. This paper proposes a data-driven approach to predict intraday stock jumps using the information embedded in liquidity measures and technical indicators. Specifically, a trading day is divided into a series of 5-min intervals, and at the end of each interval, the candidate attributes defined by liquidity measures and technical indicators are input into machine learning algorithms to predict the arrival of a stock jump as well as its direction in the following 5-min interval. An empirical study is conducted using level-2 high-frequency data of 1271 stocks on the Shenzhen Stock Exchange of China to validate our approach. The results provide initial evidence of the predictability of jump arrivals and jump directions using level-2 stock data as well as the effectiveness of using a combination of liquidity measures and technical indicators for such prediction. We also reveal the superiority of using random forest compared with other machine learning algorithms in building prediction models. Importantly, our study provides a portable data-driven approach that exploits liquidity and technical information from level-2 stock data to predict intraday price jumps of individual stocks. "],"thought_2":["Investors' previous experiences with a stock affect their willingness to repurchase that stock. Using detailed trade data from two brokers, the authors document that investors are reluctant to repurchase stocks previously sold for a loss and stocks that have risen in price subsequent to a prior sale. The authors propose that this behavior reflects investors' emotional reactions to trading and their attempts to distance themselves from negative emotions (e.g., disappointment, regret). Investors are disappointed when they sell a stock for a loss and regret having ever purchased the stock; these negative emotions deter investors from later repurchasing stocks they sold for a loss. Having sold a stock, investors are disappointed if the stock continues to rise and regret having sold the stock in the first place; these negative emotions deter investors from repurchasing stocks that go up since being sold. Thus, investors engage in reinforcement learning by repurchasing stocks whose previous purchase resulted in positive emotions and avoiding stocks whose previous purchase resulted in negative emotions. "],"prob":["stock, index, exchange, indices, market, price, movement"],"frex":["stock, exchange, index, indices, movement, investor, price"],"lift":["stock, exchange, index, indices, movement, investor, daily"],"score":["stock, index, exchange, indices, price, investor, market"],"proportion":["0.01"]},{"name":["strategy, market, trade, price, option, profit, transaction"],"size":[1800],"topic_no":[56],"thought_1":["Automated traders operate market shares without human intervention. We propose a Trading Team based on atomic traders with opportunity detectors and simple effectors. The detectors signalize trading opportunities. For each trading signal, the effectors follow deterministic rules on when and what to trade in the market. The detectors are based on Partial Least Squares. We perform some trading experiments with twelve BM&FBovespa stocks. The empirical findings indicate that the proposed trading strategy reaches a 77.26 % annualized profit, outperforming by 380.07 % the chosen baseline strategy with a 16.07 % profit. We also investigate Multistock Resolution Strategy (MSR) performance subject to brokerage commissions and income tax. Whenever the initial investment is at least US$ 50, 000, the MSR strategy provides a profit of at least 38.63 %. "],"thought_2":["Futures markets have seen a phenomenal success since their inception both in developed and developing countries during the last four decades. This success is attributable to the tremendous leverage the futures provide to market participants. This study contributes to the literature by analyzing a trading strategy which benefits from this leverage by using the Capital Asset Pricing Model (CAPM) and cost-of-carry relationship. We apply the technical trading rules developed from spot market prices, on futures market prices using a CAPM based hedge ratio. Historical daily prices of twenty stocks from each of the ten markets (five developed markets and five emerging markets) are used for the analysis. Popular technical indicators, along with artificial intelligence techniques like Neural Networks and Genetic Algorithms, are used to generate buy and sell signals for each stock and for portfolios of stocks. The performance of the trading strategies is then calculated and compared. The results show that, although equal amounts invested in both spot and futures markets, the profit from the strategies applied on futures is considerably higher than that from the spot market in both developed and emerging markets. Moreover, the overall performance of the artificial intelligence strategies is far better than the traditional ones. "],"prob":["market, strategy, price, trade, option, profit, change"],"frex":["strategy, market, trade, price, option, profit, transaction"],"lift":["trade, strategy, market, price, option, profit, compete"],"score":["trade, market, price, strategy, profit, option, transaction"],"proportion":["0.01"]}],"topic_no":[32,56]}],"topic_no":[9,22,40,52,29,44,11,71,18,65,17,21,48,30,57,19,32,56]},{"name":["pattern, text, character, language, quality, software, machine_learn"],"children":[{"name":["text, natural, word, document, concept, language, web"],"children":[{"name":["document, text, word, corpus, retrieval, textual, mine"],"size":[1800],"topic_no":[36],"thought_1":["Stemming is used in many information retrieval (IR) systems to reduce variant word forms to common roots. It is one of the simplest applications of natural-language processing to IR and is one of the most effective in terms of user acceptance and consistency, though small retrieval improvements. Current stemming techniques do not, however, reflect the language use in specific corpora, and this can lead to occasional serious retrieval failures. We propose a technique for using corpus-based word variant cooccurrence statistics to modify or create a stemmer. The experimental results generated using English newspaper and legal text and Spanish text demonstrate the viability of this technique and its advantages relative to conventional approaches that only employ morphological rules. "],"thought_2":["This paper addresses a content management problem in situations where we have a collection of spoken documents in audio stream format in one language and a collection of related text documents in another. In our case, we have a huge digital archive of audio broadcast news in Taiwanese, but its transcriptions are unavailable. Meanwhile, we have a collection of related text-based news stories, but they are written in Chinese characters. Due to the lack of a standard written form for Taiwanese, manual transcription of spoken documents is prohibitively expensive, and automatic transcription by speech recognition is infeasible because of its poor performance for Taiwanese spontaneous speech. We present an approximate solution by aligning Taiwanese spoken documents with related text documents in Mandarin. The idea is to take advantage of the abundance of Mandarin text documents available in our application to compensate for the limitations of speech recognition systems. Experimental results show that even though our speech recognizer for spontaneous Taiwanese performs poorly, our approach still achieve a high (82.5%) alignment accuracy for sufficient for content management. "],"prob":["text, word, document, information, mine, retrieval, textual"],"frex":["document, text, word, corpus, retrieval, textual, mine"],"lift":["document, corpus, text, word, unstructured, textual, retrieval"],"score":["document, text, word, retrieval, corpus, textual, semantic"],"proportion":["0.01"]},{"name":["web, natural, concept, description, language, database, discovery"],"size":[1800],"topic_no":[70],"thought_1":["Three-way concept analysis provides a new model to make three-way decisions. Its basic structure can be shown by the three-way concept lattices. Thus, how to construct three-way concept lattices is an important issue in the three-way concept analysis. This paper proposes approaches to create the three-way concept lattices of a given formal context. First, we can transform the given formal context and its complementary context into new formal contexts which are isomorphic to the given formal context and its complementary context respectively. And then, Type I-combinatorial context and Type II-combinatorial context are defined, which are apposition and subposition of these new formal contexts, respectively. Second, we prove that the concept lattice of Type I-combinatorial context is isomorphic to object-induced three-way concept lattice and the concept lattice of Type II-combinatorial context is isomorphic to attribute-induced three-way concept lattice of the given formal context. And then, the approaches of creating the three-way concept lattices are proposed based on the concept lattices of Type I-combinatorial context and Type I-combinatorial context. Finally, we give the corresponding algorithms of constructing three-way concept lattices based on the above approaches and conduct several experiments to illustrate the efficient of proposed algorithms. "],"thought_2":["Boas (2009) has established that the recent trends in computational lexicography encompass computational methods and tools designed to assist in various lexicographical tasks, including the preparation of lexicographical evidence from many sources, the recording in database form of the relevant linguistic information, the editing of lexicographical entries and the dissemination of lexicographical products. In line with this development, Intan Safinaz's research (2011) on frame-based analysis on Malay corpus has produced essential semantic information compiled in a reference database that can assist lexicographers in editing dictionary entries for Malay-English bilingual dictionaries. This paper aims to discuss the frame- based analysis that incorporated Frame Semantics methodology and the benefits of compiling the database as a reference database. The entry 'memberi' is analysed and the findings contribute to the semantic content of the reference database. This reference database is able to facilitate the task of editing entries of the Malay monolingual dictionaries, Malay-English bilingual dictionaries, as well as the Malay language reference books. This paper also discusses the possibility of building a Malay FrameNet based on the architecture of the English FrameNet which was set up at the International Computer Science Institute in Berkeley. The proposed methods of building the Malay FrameNet is also inspired from the FrameNets of other languages for example the German, French and Japanese FrameNets. The setting up of the Malay FrameNet can be seen as an effort towards processing the Malay language and strengthening the Malay computational lexicography as well as encouraging future linguistic and computational lexicography research and cooperative efforts in natural language processing of the Malay language. "],"prob":["concept, language, natural, web, database, description, context"],"frex":["web, natural, concept, description, language, database, discovery"],"lift":["web, description, natural, discovery, concept, language, database"],"score":["web, language, natural, concept, semantic, description, database"],"proportion":["0.01"]}],"topic_no":[36,70]},{"name":["topic, event, content, literature, sentiment, social_media, machine_learn"],"children":[{"name":["sentiment, social_media, content, opinion, negative, post, positive"],"size":[1800],"topic_no":[5],"thought_1":["We present a novel approach for analysing the qualitative content of annual reports. Using natural language processing techniques we determine if sentiment expressed in the text matters in fraud detection. We focus on the Management Discussion and Analysis (MD&A) section of annual reports because of the nonfactual content present in this section, unlike other components of the annual reports. We measure the sentiment expressed in the text on the dimensions of polarity, subjectivity, and intensity and investigate in depth whether truthful and fraudulent MD&As differ in terms of sentiment polarity, sentiment subjectivity and sentiment intensity. Our results show that fraudulent MD&As on average contain three times more positive sentiment and four times more negative sentiment compared with truthful MD&As. This suggests that use of both positive and negative sentiment is more pronounced in fraudulent MD&As. We further find that, compared with truthful MD&As, fraudulent MD&As contain a greater proportion of subjective content than objective content. This suggests that the use of subjectivity clues such as presence of too many adjectives and adverbs could be an indicator of fraud. Clear cases of fraud show a higher intensity of sentiment exhibited by more use of adverbs in the “adverb modifying adjective” pattern. Based on the results of this study, frequent use of intensifiers, particularly in this pattern, could be another indicator of fraud. Moreover, the dimensions of subjectivity and intensity help in accurately classifying borderline examples of MD&As (that are equal in sentiment polarity) into fraudulent and truthful categories. When taken together, these findings suggest that fraudulent MD&As in contrast to truthful MD&As contain higher sentiment content. Copyright © 2016 John Wiley & Sons, Ltd. Copyright "],"thought_2":["Due to the huge popularity of microblogging services, microblogs have become important sources of customer opinions. Sentiment analysis systems can provide useful knowledge to decision support systems and decision makers by aggregating and summarizing the opinions in massive microblogs automatically. The most important component of sentiment analysis systems is sentiment lexicon. However, the performance of traditional sentiment lexicons on microblog sentiment analysis is far from satisfactory, especially for Chinese. In this paper, we propose a data-driven approach to build a high-quality microblog-specific sentiment lexicon for Chinese microblog sentiment analysis system. The core of our method is a unified framework that incorporates three kinds of sentiment knowledge for sentiment lexicon construction, i.e., the word-sentiment knowledge extracted from microblogs with emoticons, the sentiment similarity knowledge extracted from words’ associations among all the messages, and the prior sentiment knowledge extracted from existing sentiment lexicons. In addition, in order to improve the coverage of our sentiment lexicon, we propose an effective method to detect popular new words in microblogs, which considers not only words’ distributions over texts, but also their distributions over users.The detected new words with strong sentiment are incorporated in our sentiment lexicon.We built a microblog-specific Chinese sentiment lexicon on a large microblog dataset with more than 17 million messages. Experimental results on two microblog sentiment datasets show that our microblog-specific sentiment lexicon can significantly improve the performance of microblog sentiment analysis. "],"prob":["sentiment, content, social_media, negative, opinion, positive, post"],"frex":["sentiment, social_media, content, opinion, negative, post, positive"],"lift":["sentiment, social_media, post, opinion, content, negative, positive"],"score":["sentiment, social_media, content, opinion, post, negative, positive"],"proportion":["0.01"]},{"name":["topic, machine_learn, event, science, interest, literature, decade"],"size":[1800],"topic_no":[58],"thought_1":["Purpose: This paper aims to identify the intellectual structure of four leading hospitality journals over 40 years by applying mixed-method approach, using both machine learning and traditional statistical analyses. Design/methodology/approach: Abstracts from all 4,139 articles published in four top hospitality journals were analyzed using the structured topic modeling and inferential statistics. Topic correlation and community detection were applied to identify strengths of correlations and sub-groups of topics. Trend visualization and regression analysis were used to quantify the effects of the metadata (i.e. year of publication and journal) on topic proportions. Findings: The authors found 50 topics and eight subgroups in the hospitality journals. Different evolutionary patterns in topic popularity were demonstrated, thereby providing the insights for popular research topics over time. The significant differences in topical proportions were found across the four leading hospitality journals, suggesting different foci in research topics in each journal. Research limitations/implications: Combining machine learning techniques with traditional statistics demonstrated potential for discovering valuable insights from big text data in hospitality and tourism research contexts. The findings of this study may serve as a guide to understand the trends in the research field as well as the progress of specific areas or subfields. Originality/value: It is the first attempt to apply topic modeling to academic publications and explore the effects of article metadata with the hospitality literature. "],"thought_2":["Behavioral science and machine learning have rapidly progressed in recent years. As there is growing interest among behavioral scholars to leverage machine learning, we present strategies for how these methods that can be of value to behavioral scientists using examples centered on behavioral research. "],"prob":["machine_learn, topic, literature, event, application, interest, year"],"frex":["topic, machine_learn, event, science, interest, literature, decade"],"lift":["topic, decade, science, overview, scientific, academic, event"],"score":["topic, machine_learn, event, science, trend, literature, academic"],"proportion":["0.02"]}],"topic_no":[5,58]},{"name":["character, pattern, recognition, face, recognize, difficulty, correct"],"size":[1800],"topic_no":[41],"thought_1":["character(0)"],"thought_2":["character(0)"],"prob":["character, pattern, recognition, face, recognize, variation, difficulty"],"frex":["character, pattern, recognition, face, recognize, difficulty, correct"],"lift":["character, recognition, pattern, face, recognize, difficulty, correct"],"score":["character, recognition, pattern, face, recognize, variation, correct"],"proportion":["0.05"]},{"name":["signal, diagnosis, medical, identification, fault, patient, health"],"children":[{"name":["signal, fault, diagnosis, identification, frequency, normal, condition"],"size":[1800],"topic_no":[6],"thought_1":["Rolling element bearings faults are one of the main causes of breakdown of rotating machines. Aside from this, due to variation of operating condition, domain shift phenomenon results in important detection performance deterioration. Therefore, cross-domain intelligent fault detection and diagnosis of bearings is very critical for the reliable operation. In this paper, a new intelligent fault diagnosis approach based on tensor-aligned invariant subspace learning and two-dimensional convolutional neural networks (TAISL–2DCNN) is proposed for cross-domain intelligent fault diagnosis of bearings. The vibration signals of bearings fault are first formulated as a third-order tensor via trial, condition and channel. For adapting the source domain and the target domain tensor representations directly, without vectorization, the domain adaptation (DA) approach named TAISL is first proposed for tensor representation in bearing intelligent fault diagnosis field. Then the 2DCNN is utilized to recognize different faults. The performance of the presented algorithm has been thoroughly evaluated through extensive cross-domain fault diagnosis experiments. The verification results confirm that the developed approach is able to reliably and accurately identify different fault categories and severities of bearings when testing and training data are drawn from different distribution. "],"thought_2":["Purpose: The purpose of this article is to present a new application of pursuit-based analysis for diagnosing rolling element bearing faults. Design/methodology/approach: Intelligent diagnosis of rolling element bearing faults in rotating machinery involves the procedure of feature extraction using modern signal processing techniques and artificial intelligence technique-based fault detection and identification. This paper presents a comparative study of both the basis and matching pursuits when applied to fault diagnosis of rolling element bearings using vibration analysis. Findings: Fault features were extracted from vibration acceleration signals and subsequently fed to a feed forward neural network (FFNN) for classification. The classification rate and mean square error (MSE) were calculated to evaluate the performance of the intelligent diagnostic procedure. Results from the basis pursuit fault diagnosis procedure were compared with the classification result of a matching pursuit feature-based diagnostic procedure. The comparison clearly illustrates that basis pursuit feature-based fault diagnosis is significantly more accurate than matching pursuit feature-based fault diagnosis in detecting these faults. Practical implications: Intelligent diagnosis can reduce the reliance on experienced personnel to make expert judgements on the state of the integrity of machines. The proposed method has the potential to be extensively applied in various industrial scenarios, although this application concerned rolling element bearings only. The principles of the application are directly translatable to other parts of complex machinery. Originality/value: This work presents a novel intelligent diagnosis strategy using pursuit features and feed forward neural networks. The value of the work is to ease the burden of making decisions on the integrity of plant through a manual program in condition monitoring and diagnostics particularly of complex pieces of plant. "],"prob":["signal, identification, fault, diagnosis, condition, frequency, monitor"],"frex":["signal, fault, diagnosis, identification, frequency, normal, condition"],"lift":["fault, signal, diagnosis, normal, identification, frequency, transform"],"score":["fault, signal, diagnosis, identification, frequency, normal, monitor"],"proportion":["0.01"]},{"name":["patient, health, disease, medical, care, record, treatment"],"size":[1800],"topic_no":[23],"thought_1":["Cancer is a worldwide health problem with extremely high morbidity and mortality. Pancreatic cancer specifically is the fourth leading cause of death by cancer in the United States and is a leading cause of cancer deaths worldwide. The optimal treatment for pancreatic cancer is resection surgery, but even with surgery many patients suffer high morbidity and mortality, leading to regret in physicians over whether or not the optimal course of treatment with regard to the patient's quality of life was made. Patients also suffer regret concerning the morbidity associated with treatment. An artificial neural network is developed to predict 7-month survival of pancreatic cancer patients that achieves over a 91% sensitivity and an overall accuracy above 70%. The artificial neural network outcome predictions may be used as an additional source of information to assist physicians and patients in selecting the treatment that provides the best quality of life for the patient and reduces treatment decision regret. "],"thought_2":["Patients at risk for hepatocellular carcinoma or liver cancer should undergo semiannual screening tests to facilitate early detection, effective treatment options at lower cost, better recovery prognosis, and higher life expectancy. Health care institutions invest in direct-to-patient outreach marketing to encourage regular screening. They ask the following questions: (1) Does the effectiveness of outreach vary among patients and over time?; (2) What is the return on outreach?; and (3) Can patient-level targeted outreach increase the return? The authors use a multiperiod, randomized field experiment involving 1,800 patients. Overall, relative to the usual-care condition, outreach alone (outreach with patient navigation) increases screening completion rates by 10–20 (13–24) percentage points. Causal forests demonstrate that patient-level treatment effects vary substantially across periods and by patients’ demographics, health status, visit history, health system accessibility, and neighborhood socioeconomic status, thereby facilitating the implementation of the targeted outreach program. A simulation shows that the targeted outreach program improves the return on the randomized outreach program by 74%–96% or $1.6 million to $2 million. Thus, outreach marketing provides a substantial positive payoff to the health care system. "],"prob":["health, patient, medical, disease, record, treatment, care"],"frex":["patient, health, disease, medical, care, record, treatment"],"lift":["care, patient, health, disease, medical, record, treatment"],"score":["care, patient, health, disease, medical, treatment, record"],"proportion":["0.01"]}],"topic_no":[6,23]},{"name":["maintenance, engineer, development, software, quality, requirement, failure"],"children":[{"name":["air, quality, measurement, monitor, structural, improvement, direct"],"size":[1800],"topic_no":[12],"thought_1":["Air transportation direct share is the ratio of direct passengers to total passengers on a directional origin and destination (O&D) pair. Direct share is an essential factor of passenger flow distribution and shows passengers' general preference for direct flight services on a certain O&D. A better understanding and a more accurate forecast of direct share can benefit air transportation planners, airlines, and airports in multiple ways. In most of the previous research and applications, it is commonly assumed that direct share is a fixed ratio, which contradicts the air transportation practice. In the Federal Aviation Administration (FAA) Terminal Area Forecast (TAF), the O&D direct share is forecasted as a constant based on the latest observation of direct share on the O&D. To find factors which have significant impacts on O&D direct share and to build an accurate model for O&D direct share forecasting, both parametric and nonparametric machine learning models are investigated in this research. We propose a novel category-based learning method which can provide better forecasting performance compared to employing the single modeling method for O&D direct share forecasting. Based on the comparison, the developed category-based learning model is a promising replacement for the model used for O&D direct share forecasting by the FAA TAF. "],"thought_2":["Air pollution impact assessment is a major objective for various community councils in large cities, which have lately redirected their attention towards using more low-cost sensing units supported by citizen involvement. However, there is a lack of research studies investigating real-time mobile air-quality measurement through smart sensing units and even more of any data-driven modelling techniques that could be deployed to predict air quality accurately from the generated data-sets. This paper addresses these challenges by: a) proposing a comparative and detailed investigation of various air quality monitoring devices (both fixed and mobile), tested through field measurements and citizen sensing in an eco-neighbourhood from Lorraine, France, and by b) proposing a machine learning approach to evaluate the accuracy and potential of such mobile generated data for air quality prediction. The air quality evaluation consists of three experimenting protocols: a) first, we installed fixed passive tubes for monitoring the nitrogen dioxide concentrations placed in strategic locations highly affected by traffic circulation in an eco-neighbourhood, b) second, we monitored the nitrogen dioxide registered by citizens using smart and mobile pollution units carried at breathing level; results revealed that mobile-captured concentrations were 3–5 times higher than the ones registered by passive-static monitoring tubes and c) third, we compared different mobile pollution stations working simultaneously, which revealed noticeable differences in terms of result variability and sensitivity. Finally, we applied a machine learning modelling by using decision trees and neural networks on the mobile-generated data and show that humidity and noise are the most important factors influencing the prediction of nitrogen dioxide concentrations of mobile stations. "],"prob":["quality, air, monitor, measurement, structural, improvement, high"],"frex":["air, quality, measurement, monitor, structural, improvement, direct"],"lift":["air, quality, measurement, monitor, structural, direct, poor"],"score":["air, quality, monitor, measurement, structural, improvement, relationship"],"proportion":["0.01"]},{"name":["software, maintenance, requirement, failure, reliability, engineer, equipment"],"size":[1800],"topic_no":[46],"thought_1":["Purpose: Opportunistic maintenance (OM) policy is a prospective maintenance approach that instigates for a more effective and optimized system. The purpose of this paper is to provide the steps and methods used in model development processes for the application of the OM policy. Design/methodology/approach: Dubbed as opportunistic principle toward optimal maintenance system (OPTOMS) for OM policy toward optimal maintenance system, the model is devised as a decision support system model and contains five phases. The motivation and focus of the model resolve around the need for a practical framework or model of maintenance policy for the application in an industry. In this paper, the OPTOMS model was verified and validated to ensure that the model is applicable in the industry and robust as a support system in decision making for the optimal maintenance system. Findings: From the verification steps conducted in a case study company, it was found that the developed model incorporated simple but practical tools like check sheet, failure mode and effect analysis (FMEA), control chart that has been commonly used in the industry. Practical implications: This paper provides the general explanations of the developed model and tools used for each phase in implementing OM to achieve an optimal maintenance system. Based on a case study conducted in a semiconductor company, the OPTOMS model can align and prepare the company in increasing machine reliability by reducing machine downtime. Originality/value: The novelty of this paper is based on the in-depth discussion of all phases and steps in the model that emphasize on how the model will become practical theories in conducting an OM policy in a company. The proposed methods and tools for data collection and analysis are practical and commonly used in the industry. The framework is designed for practical application in the industry. The users would be from the Maintenance and Production Department. "],"thought_2":["Software requirements engineering is a critical discipline in the software development life cycle. The major problem in software development is the selection and prioritization of the requirements in order to develop a system of high quality. This research analyzes the issues associated with existing software requirement prioritization techniques. One of the major issues in software requirement prioritization is that the existing techniques handle only toy projects or software projects with very few requirements. The current techniques are not suitable for the prioritization of a large number of requirements in projects where requirements may grow to the hundreds or even thousands. The research paper proposes an expert system, called the Priority Handler (PHandler), for requirement prioritization. PHandler is based on the value-based intelligent requirement prioritization technique, neural network and analytical hierarchical process in order to make the requirement prioritization process scalable. The back-propagation neural network is used to predict the value of a requirement in order to reduce the extent of expert biases and make the PHandler efficient. Moreover, the analytical hierarchy process is applied on prioritized groups of requirements in order to enhance the scalability of the requirement prioritization process. "],"prob":["software, development, requirement, maintenance, failure, engineer, reliability"],"frex":["software, maintenance, requirement, failure, reliability, engineer, equipment"],"lift":["maintenance, equipment, software, reliability, failure, requirement, engineer"],"score":["maintenance, software, failure, requirement, reliability, equipment, engineer"],"proportion":["0.01"]}],"topic_no":[12,46]}],"topic_no":[36,70,5,58,41,6,23,12,46]}],"topic_no":[9,22,40,52,29,44,11,71,18,65,17,21,48,30,57,19,32,56,36,70,5,58,41,6,23,12,46]}],"topic_no":[55,24,51,68,15,66,1,47,27,54,37,45,64,39,33,4,42,9,22,40,52,29,44,11,71,18,65,17,21,48,30,57,19,32,56,36,70,5,58,41,6,23,12,46]},{"name":["forecast, data, neural_network, problem, learn, algorithm, feature"],"children":[{"name":["information, user, data, classification, network, feature, learn"],"children":[{"name":["object, target, track, module, filter, robustness, robust"],"size":[1800],"topic_no":[69],"thought_1":["Robustness and efficiency are the two main goals of existing trackers. Most robust trackers are implemented with combined features or models accompanied with a high computational cost. To achieve a robust and efficient tracking performance, we propose a multi-view correlation tracker to do tracking. On one hand, the robustness of the tracker is enhanced by the multi-view model, which fuses several features and selects the more discriminative features to do tracking. On the other hand, the correlation filter framework provides a fast training and efficient target locating. The multiple features are well fused on the model level of correlation filer, which are effective and efficient. In addition, we raise a simple but effective scale-variation detection mechanism, which strengthens the stability of scale variation tracking. We evaluate our tracker on online tracking benchmark (OTB) and two visual object tracking benchmarks (VOT2014, VOT2015). These three datasets contains more than 100 video sequences in total. On all the three datasets, the proposed approach achieves promising performance. "],"thought_2":["In this paper, we propose a novel multi-pattern correlation tracker (MPCT) which deeply models the appearance of the target object for robust tracking. Specifically, multiple correlation filters are learned to capture different appearance patterns of the target object during the tracking process and each filter represents one specific appearance pattern. With the proposed reliable and matching score, a two stage selection algorithm is developed to select a suitable correlation filter to localize the target object. To effectively obtain different filters, we design an online evaluation algorithm to generate filters for different appearance patterns. By taking advantage of multiple filters to model different appearance patterns, the proposed MPCT tracker can not only capture dynamic appearance changes under complex scenes but also deal with severe occlusion and model drift problems to achieve better tracking performance. Extensive experimental results prove that the proposed tracking algorithm performs superiorly against several state-of-the-art tracking methods on challenging tracking benchmarks. "],"prob":["target, object, module, filter, track, robust, robustness"],"frex":["object, target, track, module, filter, robustness, robust"],"lift":["track, object, robustness, target, module, filter, robust"],"score":["track, object, target, filter, module, robustness, region"],"proportion":["0.01"]},{"name":["neural_network, detection, detect, deep, deep_learn, image, computer"],"children":[{"name":["detection, code, video, sensor, detect, computer, vision"],"size":[1800],"topic_no":[8],"thought_1":["The rapid expansion of Asian hornets poses a high threat for the honey bee survival, as these invaders pray on them. Furthermore, they also pose a threat to people who are allergic, whose sting can lead to death. This study proposes a Decision Support System that uses Computer Vision techniques to automatically detect signs of Vespa velutina through images from GPS equipped camera. The goal of the system is to provide timely information about the presence of these invaders, allowing park managers and beekeepers to act quickly in removing the Vespidae. The proposed methodology obtained an 85% accuracy in the detection of V. velutina using the Mask RCNN architecture, enabling the system to perform detection at 3 FPS. "],"thought_2":["Watching online videos is a major leisure activity among Internet users. The largest video website, YouTube, stores billions of videos on its servers. Thus, previous studies have applied automatic video categorization methods to enable users to find videos corresponding to their needs; however, emotion has not been a factor considered in these classification methods. Therefore, this study classified YouTube videos into six emotion categories (i.e., happiness, anger, disgust, fear, sadness, and surprise). Through unsupervised and supervised learning methods, this study first categorized videos according to emotion. An ensemble model was subsequently applied to integrate the classification results of both methods. The experimental results confirm that the proposed method effectively facilitates the classification of YouTube videos into suitable emotion categories. "],"prob":["detection, detect, computer, sensor, code, video, vision"],"frex":["detection, code, video, sensor, detect, computer, vision"],"lift":["video, code, detection, sensor, vision, detect, computer"],"score":["video, detection, detect, computer, vision, sensor, code"],"proportion":["0.01"]},{"name":["image, deep_learn, deep, convolutional, convolutional_neural_network, visual, neural_network"],"size":[1800],"topic_no":[13],"thought_1":["We present findings on classifying the class of executable code using convolutional, recurrent neural networks by creating images from only the.text section of executables and dividing them into standard-size windows, using minimal preprocessing. We achieve up to 98.24% testing accuracy on classifying 9 types of malware, and 99.50% testing accuracy on classifying malicious vs. benign code. Then, we find that a recurrent network may not entirely be necessary, opening the door for future neural network architectures. "],"thought_2":["Visibility restoration of color rainy images is inevitable task for the researchers in many vision based applications. Rain produces a visual impact on image, so that the intensity and visibility of image is low. Therefore, there is a need to develop a robust visibility restoration algorithm for the rainy images. In this paper we proposed a robust visibility restoration framework for the images captured in rainy weather. The framework is the combined form of convolution neural network for rain removal and low light image enhancement for low contrast. The output results of the proposed framework and other latest de-rainy algorithms are estimated in terms of PSNR, SSIM and UIQI on rainy image from different databases. The quantitative and qualitative results of the proposed framework are better than other de-rainy algorithms. Finally, the obtained visualization result also shows the efficiency of the proposed framework. "],"prob":["image, deep, deep_learn, neural_network, convolutional, visual, convolutional_neural_network"],"frex":["image, deep_learn, deep, convolutional, convolutional_neural_network, visual, neural_network"],"lift":["image, convolutional_neural_network, deep_learn, convolutional, visual, deep, pose"],"score":["image, convolutional, convolutional_neural_network, deep_learn, deep, visual, neural_network"],"proportion":["0.01"]}],"topic_no":[8,13]},{"name":["cluster, distance, data_set, matrix, unsupervised, center, point"],"size":[1800],"topic_no":[20],"thought_1":["Clustering by identifying cluster centers is important for detecting patterns in a data set. However, many center-based clustering algorithms cannot process data sets containing non-spherical clusters. In this paper, we propose a novel clustering algorithm called NaNLORE based on natural neighbor and local representatives. Natural neighbor is a new neighbor concept and introduced to compute local density and find local representatives which are points with local maximum density. We first find local representatives and then select cluster centers from the local representatives. The density-adaptive distance is introduced to measure the distance between local representatives, which helps to solve the problem of clustering data sets with complex manifold structure. Cluster centers are characterized by higher density than their neighbors and a relatively large density-adaptive distance from any local representatives with higher density. In experiments, we compare the proposed algorithm NaNLORE with existing algorithms on synthetic and real data sets. Results show that NaNLORE performs better than existing algorithm, especially on clustering non-spherical data and manifold data. "],"thought_2":["Clustering is one of the well-known unsupervised learning methods that groups data into homogeneous clusters, and has been successfully used in various applications. Fuzzy C-Means(FCM) is one of the representative methods in fuzzy clustering. In FCM, however, cluster centers tend leaning to high density area because the sum of Euclidean distances in FCM forces high density clusters to make more contribution to clustering result. In this paper, proposed is an enhanced clustering method that modified the FCM objective function with additional terms, which reduce clustering errors due to density difference among clusters. Introduced are two terms, one of which keeps the cluster centers as far away as possible and the other makes cluster centers to be located in high density regions. The proposed method converges more to real centers than FCM, which can be verified with experimental results. "],"prob":["cluster, data_set, distance, matrix, algorithm, unsupervised, point"],"frex":["cluster, distance, data_set, matrix, unsupervised, center, point"],"lift":["cluster, unsupervised, distance, matrix, center, data_set, segment"],"score":["cluster, unsupervised, distance, data_set, matrix, segment, algorithm"],"proportion":["0.01"]},{"name":["feature, feature_selection, extract, subset, extraction, reduction, select"],"size":[1800],"topic_no":[28],"thought_1":["Feature selection aims at finding a feature subset that has the most discriminative information from the original feature set. In this paper, we firstly present a new scheme for feature relevance, interdependence and redundancy analysis using information theoretic criteria. Then, a dynamic weighting-based feature selection algorithm is proposed, which not only selects the most relevant features and eliminates redundant features, but also tries to retain useful intrinsic groups of interdependent features. The primary characteristic of the method is that the feature is weighted according to its interaction with the selected features. And the weight of features will be dynamically updated after each candidate feature has been selected. To verify the effectiveness of our method, experimental comparisons on six UCI data sets and four gene microarray datasets are carried out using three typical classifiers. The results indicate that our proposed method achieves promising improvement on feature selection and classification accuracy. "],"thought_2":["Despite advances in Machine Learning (ML) algorithms, the clinical viability of ML-based decision support systems (DSS) to predict the prognosis of hepatitis remains limited. However, an appropriate feature selection could improve its reliability. Differently from conventional feature reduction methods, we hypothesised that applying feature reduction first and then augmenting the reduced feature space could improve classification performance further. Thus, a novel two-stage Genetic Algorithm (GA)-based feature transformation method, which involves both feature reduction and augmentation (2-Tra-GA), was developed, tested and validated. This 2-Tra-GA was later coupled to ML-based classifiers for a prognostic prediction. Clinical data with nineteen (N = 19) features on 320 patients with hepatitis obtained from the University of California-Irvine ML repository were utilised. When tested on these data, the GA-based feature reduction resulted in a reduced set with fifteen (N=15) features that led to the highest classification accuracy and reliability. Augmenting the reduced set by adding transformed features via an interpolation method (N=32 features in total, 15 reduced and 17 transformed) further improved the classification performance. Additionally, the performance of this novel hybrid algorithm was evaluated against classifiers alike and published studies. Applying feature reduction, then augmenting only such relevant features improved classification performance and computational efficiency, also over conventional wrapper-based feature selection methods. Thus, a novel hybrid DSS to improve the reliability of the prediction of prognosis for hepatitis is proposed. Findings also support the application of the proposed hybrid method to improve clinical decision making. "],"prob":["feature, extract, feature_selection, extraction, subset, reduction, select"],"frex":["feature, feature_selection, extract, subset, extraction, reduction, select"],"lift":["feature_selection, subset, feature, extraction, extract, reduction, dimension"],"score":["feature_selection, feature, subset, extraction, extract, reduction, datasets"],"proportion":["0.01"]},{"name":["classifier, classification, train, sample, accuracy, vector, data"],"children":[{"name":["classifier, classification, class, vector, support_vector_machine, imbalance, accuracy"],"size":[1800],"topic_no":[10],"thought_1":["Contrast pattern-based classifiers are an important family of both understandable and accurate classifiers. Nevertheless, these classifiers do not achieve good performance on class imbalance problems. In this paper, we introduce a new contrast pattern-based classifier for class imbalance problems. Our proposal for solving the class imbalance problem combines the support of the patterns with the class imbalance level at the classification stage of the classifier. From our experimental results, using highly imbalanced databases, we can conclude that our proposed classifier significantly outperforms the current contrast pattern-based classifiers designed for class imbalance problems. Additionally, we show that our classifier significantly outperforms other state-of-the-art classifiers not directly based on contrast patterns, which are also designed to deal with class imbalance problems. "],"thought_2":["In Machine Learning, a data set is imbalanced when the class proportions are highly skewed. Imbalanced data sets arise routinely in many application domains and pose a challenge to traditional classifiers. We propose a new approach to building ensembles of classifiers for two-class imbalanced data sets, called Random Balance. Each member of the Random Balance ensemble is trained with data sampled from the training set and augmented by artificial instances obtained using SMOTE. The novelty in the approach is that the proportions of the classes for each ensemble member are chosen randomly. The intuition behind the method is that the proposed diversity heuristic will ensure that the ensemble contains classifiers that are specialized for different operating points on the ROC space, thereby leading to larger AUC compared to other ensembles of classifiers. Experiments have been carried out to test the Random Balance approach by itself, and also in combination with standard ensemble methods. As a result, we propose a new ensemble creation method called RB-Boost which combines Random Balance with AdaBoost.M2. This combination involves enforcing random class proportions in addition to instance re-weighting. Experiments with 86 imbalanced data sets from two well known repositories demonstrate the advantage of the Random Balance approach. "],"prob":["classification, classifier, accuracy, vector, class, support, performance"],"frex":["classifier, classification, class, vector, support_vector_machine, imbalance, accuracy"],"lift":["imbalance, class, support_vector_machine, classifier, classification, binary, kernel"],"score":["imbalance, classification, classifier, class, support_vector_machine, vector, ensemble"],"proportion":["0.03"]},{"name":["sample, label, train, data, noise, datasets, source"],"size":[1800],"topic_no":[50],"thought_1":["Semi-supervised self-labeled methods apply unlabeled data to improve the performance of classifiers which are trained by labeled data alone. Nevertheless, applying unlabeled data may deteriorate the prediction accuracy. One of the causes is that there are insufficient labeled data for training an initial classifier in self-labeled methods. However, existing solutions for this problem of lacking sufficient initial labeled data still have technical defects. For example, they fail to deal with non-spherical data and improve insufficient initial labeled data effectively, when initial labeled data are extremely scarce. In this paper, we propose an effective semi-supervised self-labeled framework based on local cores, aiming to solve the problem of lacking adequate initial labeled data in self-labeled methods and overcome existing technical defects above. Main ideas of our framework include two sides: (a) inadequate initial labeled data are improved by adding predicted local cores to them, where local cores are predicted by active labeling or co-labeling; (b) we use any semi-supervised self-labeled method to train a given classifier on improved labeled data and updated unlabeled data. In our framework, local cores roughly reveal the data distribution, which helps the proposed framework work on spherical or non-spherical data sets. In addition, local cores also help our framework improve insufficient initial labeled data effectively, even when initial labeled data are extremely scarce. Experiments show that the proposed framework is compatible with tested self-labeled methods, and can help self-labeled methods train a k nearest neighbor or support vector machine, when initial labeled data are insufficient. "],"thought_2":["Self-training method is one of the relatively successful methodologies of semi-supervised classification. It can exploit both labeled data and unlabeled data to train a satisfactory supervised classifier. Mislabeling is one of the largest challenges in the self-training method and the most common technique for removing mislabeled samples is the local noise filter. However, existing local noise filters used in self-training methods confront following technical defects: parameter dependence and using only labeled data to remove mislabeled samples. To address these shortcomings, this paper proposes a novel self-training method based on density peaks and an extended parameter-free local noise filter (STDPNF). In STDPNF, the self-training method based on density peaks is redesigned to be more suitable for combination with local noise filters. Moreover, a new local noise filter based on natural neighbors is proposed to filter out mislabeled instances. Compared with existing local noise filters used in self-training methods, the one in STDPNF is parameter-free and can remove mislabeled samples by exploiting the information of both labeled data and unlabeled data. We focus on k nearest neighbor as a base classifier. In experiments, we verify the efficiency of STDPNF in improving the performance of the base classifier of k nearest neighbor and the advantage of STDPNF in having the ability to remove mislabeled instances efficiently even when labeled data are not sufficient. "],"prob":["data, train, sample, label, domain, source, datasets"],"frex":["sample, label, train, data, noise, datasets, source"],"lift":["label, sample, noise, train, data, adaptation, instance"],"score":["label, data, train, sample, noise, datasets, domain"],"proportion":["0.02"]}],"topic_no":[10,50]},{"name":["distribution, part, space, case, function, structure, order"],"children":[{"name":["part, order, structure, operator, space, weight, metric"],"size":[1800],"topic_no":[59],"thought_1":["The aim of this paper is to investigate an ordered multiplicative modular geometric operator and its relevant properties. The ordered multiplicative modular geometric operator is a generalized form of the ordered weighted geometric operator which has been designed incorporating the advantages of the geometric mean to deal with ratio judgments and the advantages of the ordered weighted averaging (OWA) operator to represent the concept of fuzzy majority in the process of information aggregation. Besides, the ordered multiplicative modular geometric operator can be seen as a symmetrized multiplicative modular aggregation function, characterized by comonotone multiplicative modularity. It is worth pointing that lots of the existing operators (such as the ordered weighted geometric operator, the weighted geometric operator, the ordered weighted maximum, and the Max and Min operators) can be regarded as the special cases of the ordered multiplicative modular geometric operator, which is of value in developing the theory of geometric operators. "],"thought_2":["The focus of this paper are distributivity equations involving the binary aggregation operators on the unit interval [0, 1] with either absorbing or neutral element from the open interval (0, 1), and the Mayor's aggregation operators from [28]. In the second part of this paper, problem is extended to aggregation operators that have neither neutral nor absorbing element. "],"prob":["order, structure, part, case, weight, space, main"],"frex":["part, order, structure, operator, space, weight, metric"],"lift":["operator, part, functional, order, element, structure, metric"],"score":["operator, structure, order, part, weight, space, element"],"proportion":["0.02"]},{"name":["function, utility, probability, distribution, bayesian, mine, approximate"],"size":[1800],"topic_no":[60],"thought_1":["High-utility itemset mining is used to obtain high utility itemsets by taking into account both the quantity as well as the utility of each item, which have not been considered in frequent itemset mining. Many algorithms compute high utility itemsets by setting a minimum utility threshold in advance. However, determining the minimum utility threshold is not easy. Too high or too low a threshold may result in incorrect high utility itemsets. In this paper, we propose a method based on binary particle swarm optimization to optimize the search for high utility itemsets without setting the minimum utility threshold beforehand. Instead, the application of the minimum utility threshold is performed as a post-processing step. Experiments on five datasets indicate that the proposed method is better than existing methods in finding high utility itemsets, and the time to obtain those itemsets is faster than that with setting the minimum utility threshold first. "],"thought_2":["Top-k high utility itemset mining is the process of discovering the k itemsets having the highest utilities in a transactional database. In recent years, several algorithms have been proposed for this task. However, it remains very expensive both in terms of runtime and memory consumption. The reason is that current algorithms often generate a huge amount of candidate itemsets and are unable to prune the search space effectively. In this paper, we address this issue by proposing a novel algorithm named kHMC to discover the top-k high utility itemsets more efficiently. Unlike several algorithms for top-k high utility itemset mining, kHMC discovers high utility itemsets using a single phase. Furthermore, it employs three strategies named RIU, CUD, and COV to raise its internal minimum utility threshold effectively, and thus reduce the search space. The COV strategy introduces a novel concept of coverage. The concept of coverage can be employed to prune the search space in high utility itemset mining, or to raise the threshold in top-k high utility itemset mining, as proposed in this paper. Furthermore, kHMC relies on a novel co-occurrence pruning technique named EUCPT to avoid performing costly join operations for calculating the utilities of itemsets. Moreover, a novel pruning strategy named TEP is proposed for reducing the search space. To evaluate the performance of the proposed algorithm, extensive experiments have been conducted on six datasets having various characteristics. Results show that the proposed algorithm outperforms the state-of-the-art TKO and REPT algorithms for top-k high utility itemset mining both in terms of memory consumption and runtime. "],"prob":["function, distribution, probability, utility, mine, bayesian, high"],"frex":["function, utility, probability, distribution, bayesian, mine, approximate"],"lift":["utility, function, probability, approximate, distribution, bayesian, mine"],"score":["utility, function, distribution, probability, mine, bayesian, approximate"],"proportion":["0.01"]}],"topic_no":[59,60]},{"name":["item, match, user, engine, recommendation, query, search"],"children":[{"name":["query, search, engine, match, relevance, retrieval, rank"],"size":[1800],"topic_no":[16],"thought_1":["In response to a query, a search engine returns a ranked list of documents. If the query is about a popular topic (i.e., it matches many documents), then the returned list is usually too long to view fully. Studies show that users usually look at only the top 10 to 20 results. However, we can exploit the fact that the best targets for popular topics are usually linked to by enthusiasts in the same domain. In this paper, we propose a novel ranking scheme for popular topics that places the most authoritative pages on the query topic at the top of the ranking. Our algorithm operates on a special index of \"expert documents.\" These are a subset of the pages on the WWW identified as directories of links to non-affiliated sources on specific topics. Results are ranked based on the match between the query and relevant descriptive text for hyperlinks on expert pages pointing to a given result page. We present a prototype search engine that implements our ranking scheme and discuss its performance. With a relatively small (2.5 million page) expert index, our algorithm was able to perform comparably on popular queries with the best of the mainstream search engines. "],"thought_2":["Baidu, the most popular Chinese search engine, monitors what their users are currently searching and provides top 50 search terms, called trending search terms, in descending order of popularity ranking. The paper focused on predicting the popularity ranking trends of this top trending search terms in Baidu. Based on the data analysis, two issues were identified that could affect accuracy of using the ranking data for predicting the popularity of trending searched terms. Firstly, all trending terms are disappeared from the top 50 terms list when the popularity is getting lower. However, there are several trending terms that reappear to the top 50 terms list after they disappeared. New distinct search terms can be differentiated from reappearances of old terms so we proposed the term distinction model by using the related news articles of a trending search term provided by Baidu. Secondly, it is necessary to handle the missing value when the term is out of the trending term list. To achieve the goal of this paper, we collected top 50 trending search terms from Baidu engine and its related news articles hourly for 6 months (from 1st March 2013 to 31th August 2013). Based on the proposed model, we found that the optimal disappearing interval can be 9 h, and using rank 51 for the missing values was the most successful. We conducted evaluations by using 3 months data (from 1st September 2013 to 30th November 2013), and four machine learning techniques where compared to evaluate the most accurate for predicting the popularity rank of trending search terms. Feed Forward Neural Network was achieved 78.81 % the most highest prediction accuracy, and achieved 85.55 % accuracy in ±3 error range. "],"prob":["search, match, query, engine, relevance, term, rank"],"frex":["query, search, engine, match, relevance, retrieval, rank"],"lift":["query, engine, search, match, relevance, retrieval, candidate"],"score":["query, search, engine, match, relevance, retrieval, rank"],"proportion":["0.01"]},{"name":["recommendation, item, user, feedback, recommend, profile, list"],"size":[1800],"topic_no":[49],"thought_1":["As one of the collaborative filtering (CF) techniques, memory-based CF technique which recommends items to users based on rating information of like-minded users (called neighbors) has been widely used and has also proven to be useful in many practices in the age of information overload. However, there is still considerable room for improving the quality of recommendation. Shortly, similarity functions in traditional CF compute a similarity between a target user and the other user without considering a target item. More specifically, they give an equal weight to each of the co-rated items rated by both users. Neighbors of a target user, therefore, are identical for all target items. However, a reasonable assumption is that the similarity between a target item and each of the co-rated items should be considered when finding neighbors of a target user. Additionally, a different set of neighbors should be selected for each different target item. Thus, the objective of this paper is to propose a new similarity function in order to select different neighbors for each different target item. In the new similarity function, the rating of a user on an item is weighted by the item similarity between the item and the target item. Experimental results from MovieLens dataset and Netflix dataset provide evidence that our recommender model considerably outperforms the traditional CF-based recommender model. "],"thought_2":["Recommending user generated lists (e.g., playlists) has become an emerging task in many online systems. Many existing list recommendation methods predict user preferences on lists by aggregating their preferences on individual items, which neglects the list-level information (e.g., list attributes) and thus results in suboptimal performance. This paper proposes a neural network-based solution for user generated list recommendation, which can leverage both item-level information and list-level information to improve performance. Firstly, a representation learning network with attention and gate mechanism is proposed to learn the user embeddings, item embeddings and list embeddings simultaneously. Then, an interaction network is proposed to learn user–item interactions and user–list interactions, in which the two kinds of interactions can share the convolution layers to further improve performance. Experimental studies on two real-world datasets demonstrate that (1) the proposed representation learning network can learn more representative user/item/list embedding than existing methods and (2) the proposed solution can outperform state-of-the-art methods in both item recommendation and list recommendation in terms of accuracy. "],"prob":["user, recommendation, item, feedback, profile, recommend, list"],"frex":["recommendation, item, user, feedback, recommend, profile, list"],"lift":["item, recommendation, recommend, feedback, user, profile, list"],"score":["item, user, recommendation, feedback, profile, recommend, collaborative"],"proportion":["0.01"]}],"topic_no":[16,49]},{"name":["network, node, lie, hide, lay, neuron, link"],"size":[1800],"topic_no":[34],"thought_1":["The purpose of this paper is to design an efficient recurrent neural network (RNN)-based speech recognition system using software with long short-term memory (LSTM). The design process involves speech acquisition, pre-processing, feature extraction, training and pattern recognition tasks for a spoken sentence recognition system using LSTM-RNN. There are five layers namely, an input layer, a fully connected layer, a hidden LSTM layer, SoftMax layer and a sequential output layer. A vocabulary of 80 words which constitute 20 sentences is used. The depth of the layer is chosen as 20, 42 and 60 and the accuracy of each system is determined. The results reveal that the maximum accuracy of 89% is achieved when the depth of the hidden layer is 42. Since the depth of the hidden layer is fixed for a task, increased performance can be achieved by increasing the number of hidden layers. Copyright "],"thought_2":["Pulse coupled neural networks (PCNN, for short) are models abstracting the synchronization behavior observed experimentally for the cortical neurons in the visual cortex of a cat's brain, and the intersecting cortical model is a simplified version of the PCNN model. Membrane computing (MC) is a kind computation paradigm abstracted from the structure and functioning of biological cells that provide models working in cell-like mode, neural-like mode and tissue-like mode. Inspired from intersecting cortical model, this paper proposes a new kind of neural-like P systems, called dynamic threshold neural P systems (for short, DTNP systems). DTNP systems can be represented as a directed graph, where nodes are dynamic threshold neurons while arcs denote synaptic connections of these neurons. DTNP systems provide a kind of parallel computing models, they have two data units (feeding input unit and dynamic threshold unit) and the neuron firing mechanism is implemented by using a dynamic threshold mechanism. The Turing universality of DTNP systems as number accepting/generating devices is established. In addition, an universal DTNP system having 109 neurons for computing functions is constructed. "],"prob":["network, lie, node, hide, lay, neuron, link"],"frex":["network, node, lie, hide, lay, neuron, link"],"lift":["node, network, hide, lie, neuron, lay, connection"],"score":["node, network, lie, hide, neuron, lay, architecture"],"proportion":["0.01"]},{"name":["representation, embed, information, state-of-the-art, relation, graph, learn"],"children":[{"name":["learn, reinforcement, transfer, learn_algorithm, extreme, active, generalization"],"size":[1800],"topic_no":[14],"thought_1":["Active learning is an effective methodology to relieve the tedious and expensive work of manual annotation for many supervised learning applications. The active learning framework with good performance usually contains powerful learning models and delicate active learning strategies. Gaussian process (GP)-based active learning was proposed to be one of the most effective methods. However, the single GP suffers from the limitation of not modeling multimodal data well enough, and thus existing active learning strategies based on GPs only make use of limited information from data. In this paper, we propose three novel active learning methods, in which the existing mixture of GP model (MGP) is adjusted as the learning model and three active learning strategies are designed based on the adjusted MGP. Through experiments on multiple data sets, we analyze the performance and characteristics of the three proposed active learning methods, and further compare with popular GP-based methods and some other state-of-the-art methods. "],"thought_2":["Transfer learning aims to provide a framework to utilize previously-acquired knowledge to solve new but similar problems much more quickly and effectively. In contrast to classical machine learning methods, transfer learning methods exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modeling consisting of different data patterns in the current domain. To improve the performance of existing transfer learning methods and handle the knowledge transfer process in real-world systems, computational intelligence has recently been applied in transfer learning. This paper systematically examines computational intelligence-based transfer learning techniques and clusters related technique developments into four main categories: (a) neural network-based transfer learning; (b) Bayes-based transfer learning; (c) fuzzy transfer learning, and (d) applications of computational intelligence-based transfer learning. By providing state-of-the-art knowledge, this survey will directly support researchers and practice-based professionals to understand the developments in computational intelligence-based transfer learning research and applications. "],"prob":["learn, transfer, reinforcement, learn_algorithm, extreme, active, experiment"],"frex":["learn, reinforcement, transfer, learn_algorithm, extreme, active, generalization"],"lift":["reinforcement, learn, learn_algorithm, extreme, transfer, generalization, active"],"score":["reinforcement, learn, learn_algorithm, transfer, extreme, generalization, active"],"proportion":["0.01"]},{"name":["representation, graph, entity, relation, embed, state-of-the-art, information"],"size":[1800],"topic_no":[53],"thought_1":["Entity Disambiguation (ED) aims to automatically resolve mentions of entities in a document to corresponding entries in a given knowledge base. State-of-the-art ED methods typically utilize local contextual information for obtaining mention embeddings which will be compared to candidate entity embeddings and then apply Conditional Random Field (CRF) for collective ED, considering global coherence. An inherent drawback of these methods is that, the global semantic relationships among the candidate entities in the same document are not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the global coherence effect. In this paper, to address the issue, we propose a novel end-to-end graph neural entity disambiguation model which fully exploits the global semantic information. In particular, a heterogeneous entity-word graph is first constructed for each document to model the global semantic relationships among candidate entities in a same document. Then graph convolutional network (GCN) is applied on the entity-word graph to generate enhanced entity embeddings encoding global semantics, which are fed to a CRF for collective ED. Extensive experiments have demonstrated the efficiency and effectiveness of our method over a few state-of-the-art ED methods. "],"thought_2":["Relation classification is an important semantic processing task in the field of Natural Language Processing (NLP). The past works mainly focused on binary relations in a single sentence. Recently, cross-sentence N-ary relation classification, which detects relations among n entities across multiple sentences, has been arousing people's interests. The dependency tree based methods and some Graph Neural Network (GNN) based methods have been carried out to convey rich structural information. However, it is challenging for researchers to fully use the relevant information while ignore the irrelevant information from the dependency trees. In this paper, we propose a Graph Attention-based LSTM (GA LSTM) network to make full use of the relevant graph structure information. The dependency tree of multiple sentences is divided into many subtrees whose root node is a word in the sentence and the leaf nodes are regarded as the neighborhood. A graph attention mechanism is used to aggregate the local information in the neighborhood. Using this network, we identify the relevant information from the dependency tree. On the other hand, because the GNNs highly depend on the graph structure of the sentence and lack context sequence structural information, their effectiveness to the task is limited. To tackle this problem, we propose an N-gram Graph LSTM (NGG LSTM) network, which updates the hidden states by aggregating graph neighbor node information and the inherent sequence structural information of sentence. The experimental results show that our methods outperform most of the existing methods. "],"prob":["information, representation, relation, state-of-the-art, graph, embed, entity"],"frex":["representation, graph, entity, relation, embed, state-of-the-art, information"],"lift":["entity, graph, representation, embed, dependency, relation, state-of-the-art"],"score":["entity, representation, graph, semantic, information, relation, state-of-the-art"],"proportion":["0.02"]}],"topic_no":[14,53]}],"topic_no":[69,8,13,20,28,10,50,59,60,16,49,34,14,53]},{"name":["algorithm, neural_network, technique, prediction, forecast, predict, problem"],"children":[{"name":["algorithm, production, solve, optimization, plan, problem, solution"],"children":[{"name":["problem, solve, solution, algorithm, optimization, heuristic, genetic_algorithm"],"size":[1800],"topic_no":[3],"thought_1":["In this work, an ant colony optimisation-based heuristic is developed to solve the location-routing problem. The proposed heuristic solves the allocation problem and the routing problem simultaneously. This simultaneous approach is the novelty of the solution methodology. The proposed heuristic is tested on a set of well-known benchmark problem instances and the results obtained are compared with that using the other heuristics reported in the literature. For larger size problem instances, the proposed heuristic provides better solutions. "],"thought_2":["Ant colony optimization (ACO) algorithms have been used successfully to solve a wide variety of combinatorial optimization problems. In the recent past many modifications have been proposed in ACO algorithms to solve continuous optimization problems. However, most of the ACO variants to solve continuous optimization problems lack ability of efficient exploration of the search space and suffer from the problem of premature convergence. In this work a new ACO algorithm (ACO–LD) is proposed that incorporates Laplace distribution based interaction scheme among the ants. Also, in order to avoid the problem of stagnation, an additional diversification mechanism is introduced. The proposed ACO–LD is tested on benchmark test functions taken from Congress on Evolutionary Computation 2014 (CEC2014) and the results are compared with four state-of-the-art algorithms reported in CEC2014. ACO–LD is also applied to solve six real life problems and the results are compared with the results of six other algorithms reported in the literature. The analysis of the results shows that the overall performance of ACO–LD is found to be better than the other algorithms included in the present study. "],"prob":["problem, algorithm, solution, optimization, solve, optimal, genetic_algorithm"],"frex":["problem, solve, solution, algorithm, optimization, heuristic, genetic_algorithm"],"lift":["aco, ant_colony_optimization, swarm, heuristic, solve, particle, solution"],"score":["aco, algorithm, problem, optimization, solve, solution, genetic_algorithm"],"proportion":["0.04"]},{"name":["production, schedule, manufacture, line, plan, programme, constraint"],"size":[1800],"topic_no":[61],"thought_1":["In order to overcome the problems such as low efficiency of flexible production line and delay of production, this paper proposes a scheduling method of green flexible production line based on task priority. The mathematical model of green flexible production line scheduling is established with the constraints of non-interruptibility and machine uniqueness. The dynamic algorithm of task priority based on the number value of remaining operations is applied to the model to determine the priority distribution order. Through ant colony optimisation algorithm to determine the processing path, through the expression of pheromone and the establishment of ant colony solution to achieve green flexible production line scheduling. The experimental results show that the research method can effectively schedule the green flexible production line, and the machine utilisation rate is more than 90% of the processing level, which effectively reduces the production delay of the workpiece. "],"thought_2":["We describe a real world case study that involves the monthly planning and scheduling of the sand-casting department in a metal foundry. The problem can be characterised as a single-level multi-item capacitated lot-sizing model with a variety of additional process-specific constraints. The main objective is to smooth production. We present a hierarchical approach, in which we use a combination of mixed integer linear programming, shortest path algorithms, and iterative local improvement. The quality of the production schedules obtained in this way is by far superior to the quality of the schedules constructed by a very expert production planner with no other tool than a plan board. Furthermore, the planning effort is significantly reduced: the manual method requires about 2-3 days, whereas a typical planning session with a prototype decision support system takes no more than half an hour. "],"prob":["production, plan, manufacture, schedule, line, operation, constraint"],"frex":["production, schedule, manufacture, line, plan, programme, constraint"],"lift":["schedule, line, production, manufacture, plan, programme, flexible"],"score":["schedule, production, manufacture, plan, line, constraint, sequence"],"proportion":["0.02"]}],"topic_no":[3,61]},{"name":["supply_chain, vehicle, level, demand, drive, control, cost"],"children":[{"name":["demand, supply_chain, cost, inventory, capacity, supply, total"],"size":[1800],"topic_no":[25],"thought_1":["This paper first proposes the use of metaheuristic, to combine with exponential smoothing methods, in forecasting future demands and in determining the optimal inventory policy values for each node in a supply chain network based on historical demand or order streams without the need of any prior knowledge about the demand distribution or distribution fitting. The effects of five demand forecasting methods, two inventory policies, and three lead times on the total inventory cost of a 3-echelon serial supply chain system are then investigated. The effect of sharing the demand information for planning the inventories is also compared with that of no sharing. For testing, 15 quarterly and 15 monthly time series were taken from the M3 Competition and are considered as the multi-item demand streams to be fulfilled in the supply chain. The results indicate that: (1) the damped Pegel forecasting method is the best in terms of prediction errors because it outperforms others in three of five measures, followed by the simple exponential smoothing that wins one of the remaining two and ties the damped Pegel in one; (2) the supply chain inventory cost increases with increasing lead time and echelon level of the supply chain when the (s, S) policy is used, but not the (r, Q) policy; (3) the (r, Q) inventory policy generally incurs lower supply chain inventory cost than the (s, S) policy; (4) sharing demand information reduces inventory cost and the reduction is higher for (s, S) than for (r, Q); (5) the best demand forecasting method for minimizing inventory cost varies with the inventory policy used and lead time; and (6) the correlation between forecasting errors and inventory costs is either negligible or minimal. "],"thought_2":["An axiomatic approach to universal integrals based on level dependent capacities is presented. Based on a given semicopula, two corresponding extremal universal integrals based on level dependent capacities are given. If the underlying semicopula is even a copula, another class of universal integrals based on level dependent capacities is studied, generalizing, among others, both the Choquet and the Sugeno integral based on level dependent capacities which were introduced recently. "],"prob":["cost, demand, supply_chain, level, inventory, reduce, total"],"frex":["demand, supply_chain, cost, inventory, capacity, supply, total"],"lift":["inventory, supply_chain, demand, cost, supply, capacity, manufacturer"],"score":["inventory, cost, supply_chain, demand, supply, policy, capacity"],"proportion":["0.02"]},{"name":["vehicle, logistic, drive, control, driver, transportation, route"],"size":[1800],"topic_no":[43],"thought_1":["Considering that emergency logistics distribution has the timeliness, weak economy, and the traffic road condition and so on, based on reasonable assumptions, we effectively scheduled vehicles to maximise the demand for emergency logistics within the requested time. This paper proposes an emergency logistics vehicle scheduling model based on internet of things. Firstly, we built the model of emergency logistics distribution vehicle scheduling problem. Our research built a model of emergency logistics distribution vehicle scheduling by using the model of division time, and gave the interrelation of constraint condition. On this basis, this paper combined genetic algorithm and ant colony algorithm for the vehicle scheduling model design and parameter selection, which improved the efficiency of emergency logistics distribution. Simulation results show that the proposed model can effectively increase the transport efficiency and reduce transportation costs, and has strong practical value. It is suited for post-disaster emergency logistics distribution vehicle scheduling. Copyright "],"thought_2":["The modeling of car-following behavior is an attractive research topic in traffic simulation and intelligent transportation. The driver plays an important role in car following but is ignored by most car-following models. This paper presents a novel car-following driver model, which can retain aspects of human driving styles. First, simulated car-following data are generated by using the speed control driver model and the real-world driving behavior data if the real-world car-following data are not available. Then, the car-following driver model is established by imitating human driving maneuver during real-world car following. This is accomplished by using a neural network-based learning control paradigm and car-following data. Finally, the FTP-72 driving cycle is borrowed as the speed profile of the leading vehicle for the model test. The driving style is quantitatively analyzed by AESD. The results show that the proposed car-following driver model is capable of retaining the naturalistic driving styles while well accomplishing the car-following task with the error of relative distance mostly less than 5 meters for every driving styles. "],"prob":["control, vehicle, drive, logistic, transportation, driver, speed"],"frex":["vehicle, logistic, drive, control, driver, transportation, route"],"lift":["vehicle, logistic, driver, route, drive, transportation, control"],"score":["vehicle, control, logistic, transportation, drive, route, driver"],"proportion":["0.01"]}],"topic_no":[25,43]},{"name":["parameter, test, neural_network, artificial, predict, process, artificial_neural_network"],"children":[{"name":["parameter, surface, material, optimize, force, minimum, machine"],"size":[1800],"topic_no":[35],"thought_1":["The study aims at design and development of an integrated system to model and optimize the cutting parameters for identifying and controlling the parameters so as to achieve high level performance and quality in the 2.5 D end milling process. Taguchi’s method is used for experimental design to find out the critical parameters in the 2.5 D end milling process. An optimized artificial neural network (ANN) based on feed-forward back propagation was used to establish the model between 2.5 D end milling parameters and responses. Genetic algorithm (GA) was utilized to find the best combination of cutting parameters providing lower temperature rise in the work piece (Al 6061 T6). As fitness function of GA the output of ANN is used in the study. Cutting parameters include speed, feed, depth of cut and step over. Parameters found out by GA were able to lowers the minimum temperature rise from 19.7 to 17.2 °C with almost 13% decrease. Both the modeling and optimization process was found satisfactory. Also the GA has emerged as an effective tool to optimize 2.5 D end milling process parameters. "],"thought_2":["Free-form surfaces are widely used in many applications in today's industry. This paper presents a new approach to identify and compensate process-related errors in machining of free-form surfaces. The process-related errors are identified online by a newly developed in-process inspection technique. In this technique, the surface is first machined through an intermediate semi-finishing process that is specifically designed to machine different geometric shapes on the surface with different process parameters. An inspection method is developed to identify the process-related errors in the selected regions on the semi-finished surface. The relationship between the machining/surface parameters and process-related error is then achieved using a neural network. This relationship is used to predict the process-related errors in the finishing process. The process-related errors, together with the machine tool geometric errors identified using a method developed in our previous work, are compensated in the finishing tool paths through tool path re-planning. Experiment has been conducted to machine a part with a free-form surface to show the improvements in the machining accuracy. "],"prob":["parameter, process, machine, material, surface, optimize, determine"],"frex":["parameter, surface, material, optimize, force, minimum, machine"],"lift":["surface, parameter, force, minimum, material, fee, optimum"],"score":["surface, parameter, process, material, machine, optimize, temperature"],"proportion":["0.02"]},{"name":["artificial, artificial_neural_network, neural_network, property, strength, test, backpropagation"],"size":[1800],"topic_no":[67],"thought_1":["An artificial neural network (ANN) model was developed to predict the drape coefficient (DC). Hanging weight, Sample diameter and the bending rigidities in warp, weft and skew directions are selected as inputs of the ANN model. The ANN developed is a multilayer perceptron using a back-propagation algorithm with one hidden layer. The drape coefficient is measured by a Cusick drape meter. Bending rigidities in different directions were calculated according to the Cantilever method. The DC obtained results show a good correlation between the experimental and the estimated ANN values. The results prove a significant relationship between the ANN inputs and the drape coefficient. The algorithm developed can easily predict the drape coefficient of fabrics at different diameters. "],"thought_2":["A study on predicting the stiffness of woven fabric using an artificial neural network was conducted. A neural network system trained with a back-propagation algorithm performed functional mapping between the fabric surface and mechanical properties and the evaluated hand stiffness values. The correlation coefficient was applied to confirm the effectiveness of the model that had been developed. It was established that a hand characteristic value of stiffness can be predicted from the mechanical and surface properties of fabric. "],"prob":["neural_network, artificial, artificial_neural_network, test, predict, property, train"],"frex":["artificial, artificial_neural_network, neural_network, property, strength, test, backpropagation"],"lift":["strength, artificial_neural_network, artificial, property, backpropagation, multilayer, neural_network"],"score":["strength, artificial_neural_network, neural_network, artificial, property, train, predict"],"proportion":["0.03"]}],"topic_no":[35,67]},{"name":["prediction, traffic, flow, predict, prediction_accuracy, temporal, delay"],"size":[1800],"topic_no":[26],"thought_1":["As an important part of a smart city, intelligent transport can effectively reduce energy consumption and environmental pollution. Traffic flow forecasting provides a reliable traffic dispatch basis for intelligent transport, and most of the existing prediction methods only predict a single saturation or speed and do not use the saturation and speed in a unified way. This paper proposes a new traffic flow prediction method based on RNN-GCN and BRB. First, the belief rule base (BRB) is used for data fusion to obtain new traffic flow data, then the recurrent neural network (RNN) and graph convolution neural network (GCN) model is used to obtain the time correlation of the traffic data, and finally, the traffic flow is predicted by the topology graph. The experimental results show that the method has a better performance than ARIMA, LSTM, and GCN. "],"thought_2":["Travel time of traffic flow is the basis of traffic guidance. To improve the estimation accuracy, a travel time estimation model based on Random Forests is proposed. 7 influence variables are viewed as candidates in this paper. Data obtained from VISSIM simulation are used to verify the model. Different from other machine learning algorithm as black boxes, Random Forests can provide interpretable results through variable importance. The result of variable importance shows that mean travel time of floating car t-f, traffic state parameter X, density of vehicle Kall, and median travel time of floating car tmenf are important variables affecting travel time of traffic flow; meanwhile other variables also have a certain influence on travel time. Compared with the BP (Back Propagation) neural network model and the quadratic polynomial regression model, the proposed Random Forests model is more accurate, and the variables contained in the model are more abundant. "],"prob":["prediction, predict, traffic, time, flow, data, prediction_accuracy"],"frex":["prediction, traffic, flow, predict, prediction_accuracy, temporal, delay"],"lift":["traffic, flow, prediction, prediction_accuracy, temporal, delay, recurrent"],"score":["traffic, prediction, flow, predict, prediction_accuracy, temporal, delay"],"proportion":["0.01"]},{"name":["efficiency, build, input, output, generation, power, energy"],"children":[{"name":["power, load, generation, day, operate, source, period"],"size":[1800],"topic_no":[38],"thought_1":["Purpose: Load forecasting is important to any electrical grid, but for the developing and third-world countries with power shortages, load forecasting is essential. When planed load shedding programs are implemented to face power shortage, a noticeable distortion to the load curves will happen, and this will make the load forecasting more difficult. Design/methodology/approach: In this paper, a new load forecasting model is developed that can detect the effect of planned load shedding on the power consumption and estimate the load curve behavior without the shedding and with different shedding programs. A neuro-Fuzzy technique is used for the model, which is trained and tested with real data taken from one of the 11 KV feeders in Najaf city in Iraq to forecast the load for two days ahead for the four seasons. Load, temperature, time of the day and load shedding schedule for one month before are the input parameters for the training, and the load forecasting data for two days are estimated by the model. Findings: To verify the model, the load is forecasted without shedding by the proposed model and compared to real data without shedding and the difference is acceptable. Originality/value: The proposed model provides acceptable forecasting with the load shedding effect available and better than other models. The proposed model provides expected behavior of load with different shedding programs an issue helps to select the appropriate shedding program. The proposed model is useful to estimate the real demands by assuming load shedding hours to be zero and forecast the load. This is important in places suffer from grid problems and cannot supply full loads to calculate the peak demands as the case in Iraq. "],"thought_2":["Biological hydrogen sulphide (H2S) removal from a biogas mimic (pH = ∼7.0) was tested for 189 days in an anoxic biotrickling filter (BTF) inoculated with a pure culture of Paracoccus versutus strain MAL 1HM19. The BTF was packed with polyurethane foam cubes and operated in both fed-batch and continuous modes. The H2S inlet concentration to the BTF was varied between ∼100 and ∼500 ppmv during steady-state tests, and thereafter to ∼1000, ∼2000, ∼3000 and ∼4000 ppmv during shock-load (i.e. transient state) tests. The H2S removal efficiency (RE) ranged between 17 and 100% depending on the operational mode of the BTF and the presence of acetate as a carbon source. The maximum elimination capacity (ECmax) of the BTF reached 113.5 (±6.4) g S/m3 h with 97% RE during H2S shock-load experiments at ∼4000 ppmv which showed the robustness and resilient capacity of BTF for the large fluctuations of H2S concentrations. The results from polymerase chain reaction denaturing gradient gel electrophoresis (PCR–DGGE) revealed that P. versutus remained dominant throughout the 189 days of BTF operation which can imply the crucial role of this bacterium to remove H2S and upgrade to clean biogas. The analysis using artificial neural networks (ANNs) predicted the H2S and NO3 −-N REs and SO4 2− production in the anoxic BTF. Consequently, this study revealed that a BTF can be used to treat H2S contamination of biogas under anoxic conditions. "],"prob":["power, generation, load, day, operate, compare, operation"],"frex":["power, load, generation, day, operate, source, period"],"lift":["load, power, generation, day, operate, curve, variation"],"score":["load, power, generation, day, energy, consumption, curve"],"proportion":["0.01"]},{"name":["efficiency, energy, output, energy_consumption, input, coefficient, build"],"size":[1800],"topic_no":[62],"thought_1":["The purpose of this study was to apply artificial neural networks (ANNs) for forecasting and sensitivity analysis of energy inputs and GHG emissions of three groups of kiwifruit orchards of different sizes in Guilan Province, Iran. The initial data were collected from 80 kiwifruit producers in Langroud City, Guilan Province. The total energy input and output were estimated at 37.32 GJ ha−1 and 43.44 GJ ha−1, respectively. The ANOVA (analysis of variance) results showed significant variance among the different orchard sizes from an energy input point of view. The results revealed that the highest share of energy input was that of nitrogen fertilizer use in kiwifruit production. The main reason for the overuse of nitrogen fertilizer is government subsidies provided for chemical fertilizers, followed by high levels of nitrogen leaching due to high rainfall. The average values of some energy indices, such as energy use efficiency, energy productivity, net energy and energy intensiveness, were calculated as 1.16, 0.61 × 10−3 kg GJ−1, 6.12 GJ ha−1 and 3.27 × 10−3 GJ $−1, respectively. The average total GHG emissions were calculated as 1310 kg CO2eq. ha−1. Nitrogen fertilizer had the highest share in GHG emissions for kiwifruit production, with 26.17% of total emissions. The 12-9-9-2 structure ANN model was the best topology for predicting yield and GHG (greenhouse gas) emissions of kiwifruit production in the studied area. The coefficients of determination (R2) of the best topology calculated were 0.987 and 0.992 for yield and greenhouse gas emissions, respectively, indicating the high correlation in the model. The results of model sensitivity analysis indicated that diesel fuel and nitrogen fertilizer were the most sensitive inputs for kiwifruit yield and greenhouse gas emissions, reflecting the important role of nitrogen fertilizer in the excess energy consumption and greenhouse gas emissions of kiwifruit orchards. According to the current study, it is suggested for new policies to be adopted to reduce nitrogen fertilizer consumption. "],"thought_2":["Buildings must be energy efficient and sustainable because buildings have contributed significantly to world energy consumption and greenhouse gas emission. Predicting energy consumption patterns in buildings is beneficial to utility companies, users, and facility managers because it can help to improve energy efficiency. This work proposed a Random Forests (RF) – based prediction model to predict the short-term energy consumption in the hourly resolution in multiple buildings. Five one-year datasets of hourly building energy consumption were used to examine the effectiveness of the RF model throughout the training and test phases. The evaluation results presented that the RF model exhibited a good prediction accuracy in the prediction. In four evaluation scenarios, the mean absolute error (MAE) values ranged from 0.430 to 0.501 kWh for the 1-step-ahead prediction, from 0.612 to 0.940 kWh for the 12-steps-ahead prediction, and from 0.626 to 0.868 kWh for the 24-steps-ahead prediction. The RF model was superior to the M5P and Random Tree (RT) models. The RF was better about 49.21%, 46.93% in the MAE and mean absolute percentage error (MAPE) than the RT model in forecasting 1-step-ahead building energy consumption. The RF model approved the outstanding performance with the improvement of 49.95% and 29.29% in MAE compared to the M5P model in the 12-steps-ahead, and 24-steps-ahead energy use, respectively. Thus, the proposed RF model was an effective prediction model among the investigated machine learning (ML) models. This study contributes to (i) the state of the knowledge by examining the generalization and effectiveness of ML models in predicting building energy consumption patterns; and (ii) the state of practice by proposing an effective tool to help the building owners and facility managers in understanding building energy performance for enhancing the energy efficiency in buildings. "],"prob":["efficiency, energy, input, output, build, coefficient, energy_consumption"],"frex":["efficiency, energy, output, energy_consumption, input, coefficient, build"],"lift":["energy_consumption, efficiency, energy, output, coefficient, input, determination"],"score":["energy_consumption, energy, efficiency, output, input, coefficient, build"],"proportion":["0.01"]}],"topic_no":[38,62]},{"name":["forecast, neural_network, error, time_series, linear, regression, hybrid"],"children":[{"name":["linear, error, time_series, nonlinear, hybrid, regression, average"],"size":[1800],"topic_no":[2],"thought_1":["This paper proposes a hybrid forecasting model, which combines the seasonal time series ARIMA (SARIMA) and the neural network back propagation (BP) models, known as SARIMABP. This model was used to forecast two seasonal time series data of total production value for Taiwan machinery industry and the soft drink time series. The forecasting performance was compared among four models, i.e., the SARIMABP and SARIMA models and the two neural network models with differenced and deseasonalized data, respectively. Among these methods, the mean square error (MSE), the mean absolute error (MAE), and the mean absolute percentage error (MAPE) of the SARIMABP model were the lowest. The SARIMABP model was also able to forecast certain significant turning points of the test time series. "],"thought_2":["The development of accurate forecasting systems can be challenging in real-world applications. The modeling of real-world time series is a particularly difficult task because they are generally composed of linear and nonlinear patterns that are combined in some form. Several hybrid systems that combine linear and nonlinear techniques have obtained relevant results in terms of accuracy in comparison with single models. However, the best combination function of the forecasting of the linear and nonlinear patterns is unknown, which makes this modeling an open question. This work proposes a hybrid system that searches for a suitable function to combine the forecasts of linear and nonlinear models. Thus, the proposed system performs: (i) linear modeling of the time series; (ii) nonlinear modeling of the error series; and (iii) a data-driven combination that searches for: (iii.a) the most suitable function, between linear and nonlinear formalisms, and (iii.b) the number of forecasts of models (i) and (ii) that maximizes the performance of the combination. Two versions of the hybrid system are evaluated. In both versions, the ARIMA model is used in step (i) and two nonlinear intelligent models – Multi-Layer Perceptron (MLP) and Support Vector Regression (SVR) – are used in steps (ii) and (iii), alternately. Experimental simulations with six real-world complex time series that are well-known in the literature are evaluated using a set of popular performance metrics. Our results show that the proposed hybrid system attains superior performance when compared with single and hybrid models in the literature. "],"prob":["error, hybrid, regression, linear, neural_network, time_series, average"],"frex":["linear, error, time_series, nonlinear, hybrid, regression, average"],"lift":["autoregressive, nonlinear, linear, decomposition, non-linear, time_series, square"],"score":["autoregressive, time_series, error, regression, nonlinear, linear, hybrid"],"proportion":["0.02"]},{"name":["forecast, competition, horizon, long-term, short-term, combination, accuracy"],"size":[1800],"topic_no":[63],"thought_1":["Forecasting competitions are now so widespread that it is often forgotten how controversial they were when first held, and how influential they have been over the years. I briefly review the history of forecasting competitions, and discuss what we have learned about their design and implementation, and what they can tell us about forecasting. I also provide a few suggestions for potential future competitions, and for research about forecasting based on competitions. "],"thought_2":["The ability to forecast the volatility of the markets is critical to analysts. Among the large array of approaches available for forecasting volatility, neural networks are gaining in popularity. We present a primer for using neural networks for financial forecasting. We compare volatility forecasts from neural networks with implied volatility from S&P 500 Index futures options using the Barone-Adesi and Whaley (BAW) American futures options pricing model. Forecasts from neural networks outperform implied volatility forecasts and are not found to be significantly different from realized volatility. Implied volatility forecasts are found to be significantly different from realized volatility in two of three forecast horizons. "],"prob":["forecast, accuracy, future, competition, combination, long-term, horizon"],"frex":["forecast, competition, horizon, long-term, short-term, combination, accuracy"],"lift":["competition, horizon, forecast, long-term, short-term, probabilistic, series"],"score":["competition, forecast, horizon, time_series, short-term, accuracy, long-term"],"proportion":["0.01"]}],"topic_no":[2,63]},{"name":["predictive, technique, variable, score, machine_learn, financial, rate"],"children":[{"name":["variable, rate, bank, financial, ratio, predictor, finance"],"size":[1800],"topic_no":[7],"thought_1":["This study determines whether it is possible to distinguish between conventional and Islamic banks in the Gulf Cooperation Council (GCC) region on the basis of financial characteristics alone. Islamic banks operate under different principles, such as risk sharing and the prohibition of interest, yet both types of banks face similar competitive conditions. The combination of effects makes it unclear whether financial ratios will differ significantly between the two categories of banks. We input 26 financial ratios into logit, neural network, and k-means nearest neighbor classification models to determine whether researchers or regulators could use these ratios to distinguish between the two types of banks. Although the means of several ratios are similar between the two categories of banks, non-linear classification techniques (k-means nearest neighbors and neural networks) are able to correctly distinguish Islamic from conventional banks in out-of-sample tests at about a 92% success rate. "],"thought_2":["The banking sector plays a special role in the economy and has critical functions which are essential for economic stability. Hence, systemic banking crises disrupt financial markets and hinder global economic growth. In this study, Extreme Gradient Boosting, a state of the art machine learning method, is applied to identify a set of key leading indicators that may help predict and prevent bank failure in the Eurozone banking sector. The cross-sectional data used in this study consists of 25 annual financial ratio series for commercial banks in the Eurozone. The sample includes Eurozone listed failed and non-failed banks for the period 2006–2016. A number of early warning systems and leading indicator models have been developed to prevent bank failure. Yet the breadth and depth of the recent financial crisis indicates that these methods must improve if they are to serve as a useful tool for regulators and managers of financial institutions. Our goal is to build a classification model to determine which variables should be monitored to anticipate bank financial distress. A set of key variables are identified to anticipate bank defaults. Identifying leading indicators of bank failure is necessary so that regulators and financial institutions' management can take preventive and corrective measures before it is too late. "],"prob":["variable, rate, financial, bank, ratio, predictor, apply"],"frex":["variable, rate, bank, financial, ratio, predictor, finance"],"lift":["bank, variable, rate, ratio, financial, predictor, finance"],"score":["bank, variable, financial, rate, ratio, predictor, finance"],"proportion":["0.01"]},{"name":["score, predictive, credit, technique, data_mine, decision_tree, logistic_regression"],"size":[1800],"topic_no":[31],"thought_1":["Purpose: This paper aims to assess the application of seven statistical and data mining techniques to second-stage data envelopment analysis (DEA) for bank performance. Design/methodology/approach: Different statistical and data mining techniques are used to second-stage DEA for bank performance as a part of an attempt to produce a powerful model for bank performance with effective predictive ability. The projected data mining tools are classification and regression trees (CART), conditional inference trees (CIT), random forest based on CART and CIT, bagging, artificial neural networks and their statistical counterpart, logistic regression. Findings: The results showed that random forests and bagging outperform other methods in terms of predictive power. Originality/value: This is the first study to assess the impact of environmental factors on banking performance in Middle East and North Africa countries. "],"thought_2":["This paper compares the predictive performance of linear discriminant analysis, neural networks, genetic algorithms and decision trees in distinguishing between good and slow payers of bank credit card accounts. Predictive models were built using the evolutionary techniques and the results compared with those gained from the discriminant analysis model published in Crook et al. (1992), The Service Industries Journal 12 which uses the same dataset. A range of parameters under the control of the investigator was investigated. We found that the predictive performance of linear discriminant analysis was superior to that of the other three techniques. This is consistent with some studies but inconsistent with others. "],"prob":["technique, machine_learn, predictive, score, statistical, good, data_mine"],"frex":["score, predictive, credit, technique, data_mine, decision_tree, logistic_regression"],"lift":["credit, logistic_regression, decision_tree, score, predictive, data_mine, random_fore"],"score":["credit, technique, score, predictive, decision_tree, data_mine, machine_learn"],"proportion":["0.02"]}],"topic_no":[7,31]}],"topic_no":[3,61,25,43,35,67,26,38,62,2,63,7,31]}],"topic_no":[69,8,13,20,28,10,50,59,60,16,49,34,14,53,3,61,25,43,35,67,26,38,62,2,63,7,31]}],"topic_no":[55,24,51,68,15,66,1,47,27,54,37,45,64,39,33,4,42,9,22,40,52,29,44,11,71,18,65,17,21,48,30,57,19,32,56,36,70,5,58,41,6,23,12,46,69,8,13,20,28,10,50,59,60,16,49,34,14,53,3,61,25,43,35,67,26,38,62,2,63,7,31],"name":["AI STM 2021"],"this_root":[true],"summary":["A topic model with 71 topics, 10036 documents and a 1073 word dictionary."],"proportions":[0.0132,0.0213,0.0416,0.0091,0.0095,0.0093,0.0124,0.0109,0.0196,0.0262,0.0128,0.0068,0.0108,0.0135,0.0118,0.0072,0.0092,0.0071,0.0066,0.012,0.0109,0.0146,0.0084,0.015,0.0159,0.014,0.0147,0.0149,0.0109,0.0052,0.023,0.0064,0.0075,0.0126,0.0193,0.0132,0.008,0.0058,0.0134,0.0127,0.0537,0.0177,0.0095,0.0088,0.015,0.0142,0.0103,0.0103,0.0099,0.0248,0.0174,0.0127,0.0187,0.0162,0.017,0.0118,0.0104,0.0211,0.0222,0.0105,0.0169,0.011,0.0137,0.0177,0.0076,0.0147,0.0267,0.0144,0.0096,0.0126,0.0052]}
 
 root.x0=0;root.y0=0;update(root);function update(source){var nodes=tree.nodes(root);var height=Math.max(500,nodes.length*barHeight+margin.top+margin.bottom);
  d3.select("svg").transition().duration(duration).attr("height",height);d3.select(self.frameElement).transition().duration(duration).style("height",height+"px");nodes.forEach(function(n,i){n.x=i*barHeight;});
  var node=svg.selectAll("g.node").data(nodes,function(d){return d.id||(d.id=++i);});
  var nodeEnter=node.enter().append("g").attr("class",function(d){if(d.size){return"node leaf"}else{return"node noleaf"}}).attr("transform",function(d){return"translate("+source.y0+","+source.x0+")";}).style("opacity",1e-6);
  nodeEnter.append("rect").data(nodes).attr("y",-barHeight/2).attr("height",barHeight).attr("width",barWidth).style("fill",color).on("click",clickModal);nodeEnter.append("text").attr("dy",3.5).attr("dx",5.5).text(function(d){if (d.children){return d.name;}else{return d.topic_no + ": " + d.name;}});
  d3.selectAll("g.noleaf").append("svg:foreignObject").attr("width",20).attr("height",20).attr("y","-10px").attr("x",barWidth-15).append("xhtml:span").attr("class","control glyphicon glyphicon-minus").attr("width","30").on("click",function(d){click(d);});
  nodeEnter.transition().duration(duration).attr("transform",function(d){return"translate("+d.y+","+d.x+")";}).style("opacity",1);node.transition().duration(duration).attr("transform",function(d){return"translate("+d.y+","+d.x+")";}).style("opacity",1).select("rect").style("fill",color);
  node.exit().transition().duration(duration).attr("transform",function(d){return"translate("+source.y+","+source.x+")";}).style("opacity",1e-6).remove();
  var link=svg.selectAll("path.link").data(tree.links(nodes),function(d){return d.target.id;});link.enter().insert("path","g").attr("class","link").attr("d",function(d){var o={x:source.x0,y:source.y0};return diagonal({source:o,target:o});}).transition().duration(duration).attr("d",diagonal);
  link.transition().duration(duration).attr("d",diagonal);link.exit().transition().duration(duration).attr("d",function(d){var o={x:source.x,y:source.y};return diagonal({source:o,target:o});}).remove();nodes.forEach(function(d){d.x0=d.x;d.y0=d.y;});}
  function click(d){if(d.children){d._children=d.children;d.children=null;}else{d.children=d._children;d._children=null;}update(d);}
  function color(d){return d._children?"#3182bd":d.children?"#c6dbef":"#fd8d3c";}
  function clickModal(d){if(d.size){$("#doc1").text(d.thought_1);$("#doc2").text(d.thought_2);$("#high-prob").text("Highest Probability: "+d.prob);$("#topicModalLabel").text("Topic "+d.topic_no+" Information");$("#frex").text("FREX: "+d.frex);$("#lift").text("Lift: "+d.lift);
  $("#score").text("Score: "+d.score);$("#proportion").text(""+d.proportion);$("#modelBody").hide();$("#clusterBody").hide();$("#topicBody").show();$("#topicModal").modal("show");}else if(d.this_root){$("#topicModalLabel").text("Fitted Model Information");$("#mod1-text").text(d.summary);$("#topicBody").hide();
  $("#clusterBody").hide();$("#modelBody").show();$("#topicModal").modal("show");if ($("#barchartDiv").children().length == 2){proportionChart();}}else{$("#topicModalLabel").text("Cluster Information");$("#clust1-text").text("This cluster comprises topics "+d.topic_no.join(", ")+".");$("#topicBody").hide();$("#modelBody").hide();
  $("#clusterBody").show();$("#topicModal").modal("show");}} function proportionChart(){for(var t=window.innerHeight/3.5,a=window.innerWidth/2.5,r=35,e=35,n=root.proportions.length,o=[.5];o.length<n;)o.push(o[o.length]+1);var i=d3.select("#barchartDiv").append("svg").attr({width:a,height:t,style:"display: block; margin: auto;"}),
  l=d3.scale.linear().domain([0,d3.max(root.proportions)]).range([0,t-r]),s=d3.scale.linear().domain([0,n]).range([0,a-e]),d=d3.scale.linear().domain([0,d3.max(root.proportions)]).range([t-r,0]),p=d3.svg.axis().scale(d).orient("left"),c=d3.svg.axis().scale(s).orient("bottom").tickValues(d3.range(.5,n+.5,1));
  i.selectAll("rect").data(root.proportions).enter().append("rect").attr({x:function(t,r){return r*(a-e)/n+e},y:function(a){return t-l(a)-r},width:(a-e)/n-1,height:l,fill:"orange"}),
  i.append("g").attr({"class":"axis",transform:"translate("+e+","+(t-r)+")"}).call(c),i.append("g").attr({"class":"axis",transform:"translate("+e+")"}).call(p),i.append("text").attr("class","x label").attr("text-anchor","middle").attr("x",a/2).attr("y",t-6).text("Topic")}</script>
  <div class="modal fade" id="topicModal" tabindex="-1" role="dialog" aria-labelledby="topicModalLabel" aria-hidden="true">
  <div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button><h4 class="modal-title" id="topicModalLabel">Topic Information</h4></div>
  <div class="modal-body" id="topicBody"><h5>Top Words</h5><ul id="word-list"><li id="high-prob">Highest Probability: </li><li id="frex">FREX: </li><li id="lift">Lift: </li><li id="score">Score: </li></ul><hr><h5>Representative Documents</h5>
  <div id="doc1" class="modal-body scrollbox"></div>
  <br><div id="doc2" class="modal-body scrollbox"></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Close</button></div></div><div class="modal-body" id="modelBody"><h5>Summary</h5><span id="mod1-text"></span><hr><div id="barchartDiv">
  <h5>Topic Proportions in Corpus</h5><br></div><br><div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button></div></div><div class="modal-body" id="clusterBody"><h5>Summary</h5><span id="clust1-text"></span><br><br><div class="modal-footer">
  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button></div></div></div></div></body>
